{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d30de09-0983-4f32-a855-55832702ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ Two-Phase Classification Training\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: 4695Í∞ú\n",
      "Train: 3637, Val: 912, Test: 146\n",
      "Train - Mean: 8.31, Std: 4.26\n",
      "Val - Mean: 8.35, Std: 4.15\n",
      "Test - Mean: 7.78, Std: 4.20\n",
      "\n",
      "üì¶ Creating DataLoaders...\n",
      "‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\n",
      "   Train: 3637 samples, 28 batches\n",
      "   Val:   912 samples, 8 batches\n",
      "   Test:  146 samples, 2 batches\n",
      "\n",
      "======================================================================\n",
      "üìç PHASE 1: Training with Uniform Weights\n",
      "======================================================================\n",
      "============================================================\n",
      "Class Distribution Analysis\n",
      "============================================================\n",
      "\n",
      "A:\n",
      "  Class 0: 1791 ( 50.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1122 ( 31.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  457 ( 12.8%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  214 (  6.0%) ‚ñà‚ñà\n",
      "\n",
      "B:\n",
      "  Class 0:  740 ( 20.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  912 ( 25.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1159 ( 32.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  773 ( 21.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "C:\n",
      "  Class 0:  392 ( 10.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  797 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1295 ( 36.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1100 ( 30.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "D:\n",
      "  Class 0: 1791 ( 50.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1114 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  452 ( 12.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  227 (  6.3%) ‚ñà‚ñà‚ñà\n",
      "\n",
      "E:\n",
      "  Class 0:  726 ( 20.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  982 ( 27.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1109 ( 30.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  767 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "F:\n",
      "  Class 0:  409 ( 11.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  765 ( 21.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1292 ( 36.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1118 ( 31.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "‚úÖ Class weights: [0.95209146 0.96513265 0.9590858  1.12369   ]\n",
      "‚úÖ Part weights: [1. 1. 1. 1. 1. 1.]\n",
      "   Parameters: 19,371,924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.2036 acc:0.4492 off1:0.8598 mae:0.7092\n",
      "  Region Acc: [0.49  0.394 0.404 0.593 0.404 0.411]\n",
      "‚úÖ Phase 1 Best (Acc=0.4492, MAE=0.7092)\n",
      "\n",
      "[Phase1 Epoch 01/30]\n",
      "  Train - acc:0.3878 off1:0.8030 mae:0.8513\n",
      "  Val   - acc:0.4492 off1:0.8598 mae:0.7092\n",
      "  Part losses (Val): [1.228 1.262 1.242 1.015 1.245 1.231]\n",
      "  LR:1.00e-04 | 18.5s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.1401 acc:0.4960 off1:0.9077 mae:0.6043\n",
      "  Region Acc: [0.537 0.461 0.458 0.603 0.459 0.457]\n",
      "‚úÖ Phase 1 Best (Acc=0.4960, MAE=0.6043)\n",
      "\n",
      "[Phase1 Epoch 02/30]\n",
      "  Train - acc:0.4324 off1:0.8436 mae:0.7551\n",
      "  Val   - acc:0.4960 off1:0.9077 mae:0.6043\n",
      "  Part losses (Val): [1.134 1.195 1.191 0.991 1.15  1.182]\n",
      "  LR:1.00e-04 | 13.1s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.1196 acc:0.5217 off1:0.9092 mae:0.5740\n",
      "  Region Acc: [0.579 0.473 0.481 0.638 0.495 0.465]\n",
      "‚úÖ Phase 1 Best (Acc=0.5217, MAE=0.5740)\n",
      "\n",
      "[Phase1 Epoch 03/30]\n",
      "  Train - acc:0.4648 off1:0.8708 mae:0.6828\n",
      "  Val   - acc:0.5217 off1:0.9092 mae:0.5740\n",
      "  Part losses (Val): [1.111 1.165 1.172 0.969 1.135 1.166]\n",
      "  LR:1.00e-04 | 13.5s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.1032 acc:0.5256 off1:0.9174 mae:0.5610\n",
      "  Region Acc: [0.559 0.496 0.486 0.598 0.518 0.498]\n",
      "‚úÖ Phase 1 Best (Acc=0.5256, MAE=0.5610)\n",
      "\n",
      "[Phase1 Epoch 04/30]\n",
      "  Train - acc:0.4704 off1:0.8710 mae:0.6797\n",
      "  Val   - acc:0.5256 off1:0.9174 mae:0.5610\n",
      "  Part losses (Val): [1.102 1.132 1.162 0.964 1.103 1.156]\n",
      "  LR:1.00e-04 | 14.1s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0906 acc:0.5417 off1:0.9280 mae:0.5351\n",
      "  Region Acc: [0.58  0.522 0.507 0.629 0.519 0.493]\n",
      "‚úÖ Phase 1 Best (Acc=0.5417, MAE=0.5351)\n",
      "\n",
      "[Phase1 Epoch 05/30]\n",
      "  Train - acc:0.4817 off1:0.8811 mae:0.6576\n",
      "  Val   - acc:0.5417 off1:0.9280 mae:0.5351\n",
      "  Part losses (Val): [1.082 1.113 1.152 0.959 1.084 1.154]\n",
      "  LR:1.00e-04 | 13.7s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0746 acc:0.5431 off1:0.9355 mae:0.5258\n",
      "  Region Acc: [0.586 0.518 0.5   0.643 0.524 0.489]\n",
      "‚úÖ Phase 1 Best (Acc=0.5431, MAE=0.5258)\n",
      "\n",
      "[Phase1 Epoch 06/30]\n",
      "  Train - acc:0.4834 off1:0.8760 mae:0.6637\n",
      "  Val   - acc:0.5431 off1:0.9355 mae:0.5258\n",
      "  Part losses (Val): [1.042 1.089 1.125 0.936 1.082 1.175]\n",
      "  LR:1.00e-04 | 13.8s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0613 acc:0.5609 off1:0.9386 mae:0.5038\n",
      "  Region Acc: [0.595 0.524 0.524 0.673 0.541 0.508]\n",
      "‚úÖ Phase 1 Best (Acc=0.5609, MAE=0.5038)\n",
      "\n",
      "[Phase1 Epoch 07/30]\n",
      "  Train - acc:0.4972 off1:0.8841 mae:0.6400\n",
      "  Val   - acc:0.5609 off1:0.9386 mae:0.5038\n",
      "  Part losses (Val): [1.052 1.106 1.106 0.914 1.064 1.126]\n",
      "  LR:1.00e-04 | 14.1s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0670 acc:0.5598 off1:0.9360 mae:0.5095\n",
      "  Region Acc: [0.598 0.532 0.52  0.664 0.533 0.512]\n",
      "\n",
      "[Phase1 Epoch 08/30]\n",
      "  Train - acc:0.5209 off1:0.9062 mae:0.5871\n",
      "  Val   - acc:0.5598 off1:0.9360 mae:0.5095\n",
      "  Part losses (Val): [1.057 1.098 1.108 0.916 1.093 1.131]\n",
      "  LR:1.00e-04 | 13.6s | Pat:1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0437 acc:0.5694 off1:0.9516 mae:0.4828\n",
      "  Region Acc: [0.592 0.546 0.518 0.68  0.571 0.51 ]\n",
      "‚úÖ Phase 1 Best (Acc=0.5694, MAE=0.4828)\n",
      "\n",
      "[Phase1 Epoch 09/30]\n",
      "  Train - acc:0.5169 off1:0.8982 mae:0.6026\n",
      "  Val   - acc:0.5694 off1:0.9516 mae:0.4828\n",
      "  Part losses (Val): [1.018 1.064 1.107 0.903 1.048 1.122]\n",
      "  LR:1.00e-04 | 13.8s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0549 acc:0.5718 off1:0.9441 mae:0.4874\n",
      "  Region Acc: [0.602 0.544 0.519 0.677 0.579 0.511]\n",
      "‚úÖ Phase 1 Best (Acc=0.5718, MAE=0.4874)\n",
      "\n",
      "[Phase1 Epoch 10/30]\n",
      "  Train - acc:0.5225 off1:0.9066 mae:0.5855\n",
      "  Val   - acc:0.5718 off1:0.9441 mae:0.4874\n",
      "  Part losses (Val): [1.071 1.086 1.096 0.907 1.044 1.126]\n",
      "  LR:1.00e-04 | 13.8s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0444 acc:0.5694 off1:0.9503 mae:0.4832\n",
      "  Region Acc: [0.599 0.553 0.531 0.643 0.568 0.524]\n",
      "\n",
      "[Phase1 Epoch 11/30]\n",
      "  Train - acc:0.5349 off1:0.9080 mae:0.5714\n",
      "  Val   - acc:0.5694 off1:0.9503 mae:0.4832\n",
      "  Part losses (Val): [1.014 1.068 1.108 0.91  1.043 1.123]\n",
      "  LR:1.00e-04 | 13.8s | Pat:1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0523 acc:0.5696 off1:0.9384 mae:0.4976\n",
      "  Region Acc: [0.596 0.549 0.542 0.679 0.556 0.496]\n",
      "\n",
      "[Phase1 Epoch 12/30]\n",
      "  Train - acc:0.5412 off1:0.9136 mae:0.5590\n",
      "  Val   - acc:0.5696 off1:0.9384 mae:0.4976\n",
      "  Part losses (Val): [1.05  1.068 1.087 0.906 1.076 1.127]\n",
      "  LR:1.00e-04 | 13.8s | Pat:2/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0441 acc:0.5755 off1:0.9488 mae:0.4792\n",
      "  Region Acc: [0.613 0.548 0.536 0.664 0.568 0.523]\n",
      "‚úÖ Phase 1 Best (Acc=0.5755, MAE=0.4792)\n",
      "\n",
      "[Phase1 Epoch 13/30]\n",
      "  Train - acc:0.5194 off1:0.8928 mae:0.6110\n",
      "  Val   - acc:0.5755 off1:0.9488 mae:0.4792\n",
      "  Part losses (Val): [1.023 1.069 1.094 0.913 1.037 1.13 ]\n",
      "  LR:1.00e-04 | 13.7s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0639 acc:0.5603 off1:0.9412 mae:0.5020\n",
      "  Region Acc: [0.588 0.559 0.542 0.641 0.534 0.498]\n",
      "\n",
      "[Phase1 Epoch 14/30]\n",
      "  Train - acc:0.5144 off1:0.8913 mae:0.6160\n",
      "  Val   - acc:0.5603 off1:0.9412 mae:0.5020\n",
      "  Part losses (Val): [1.02  1.08  1.132 0.922 1.08  1.15 ]\n",
      "  LR:1.00e-04 | 13.3s | Pat:1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0581 acc:0.5742 off1:0.9463 mae:0.4845\n",
      "  Region Acc: [0.607 0.546 0.549 0.674 0.572 0.496]\n",
      "\n",
      "[Phase1 Epoch 15/30]\n",
      "  Train - acc:0.5000 off1:0.8802 mae:0.6452\n",
      "  Val   - acc:0.5742 off1:0.9463 mae:0.4845\n",
      "  Part losses (Val): [1.072 1.08  1.075 0.915 1.062 1.144]\n",
      "  LR:1.00e-04 | 13.0s | Pat:2/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0419 acc:0.5810 off1:0.9530 mae:0.4693\n",
      "  Region Acc: [0.598 0.561 0.546 0.666 0.594 0.521]\n",
      "‚úÖ Phase 1 Best (Acc=0.5810, MAE=0.4693)\n",
      "\n",
      "[Phase1 Epoch 16/30]\n",
      "  Train - acc:0.5437 off1:0.9072 mae:0.5673\n",
      "  Val   - acc:0.5810 off1:0.9530 mae:0.4693\n",
      "  Part losses (Val): [1.013 1.064 1.098 0.904 1.043 1.13 ]\n",
      "  LR:1.00e-04 | 12.8s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0391 acc:0.5786 off1:0.9492 mae:0.4761\n",
      "  Region Acc: [0.6   0.555 0.542 0.669 0.578 0.529]\n",
      "\n",
      "[Phase1 Epoch 17/30]\n",
      "  Train - acc:0.5112 off1:0.8800 mae:0.6331\n",
      "  Val   - acc:0.5786 off1:0.9492 mae:0.4761\n",
      "  Part losses (Val): [1.011 1.058 1.1   0.906 1.038 1.121]\n",
      "  LR:1.00e-04 | 13.3s | Pat:1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0531 acc:0.5853 off1:0.9543 mae:0.4645\n",
      "  Region Acc: [0.6   0.581 0.56  0.673 0.584 0.513]\n",
      "‚úÖ Phase 1 Best (Acc=0.5853, MAE=0.4645)\n",
      "\n",
      "[Phase1 Epoch 18/30]\n",
      "  Train - acc:0.5915 off1:0.9363 mae:0.4816\n",
      "  Val   - acc:0.5853 off1:0.9543 mae:0.4645\n",
      "  Part losses (Val): [1.043 1.058 1.091 0.938 1.046 1.143]\n",
      "  LR:1.00e-04 | 13.6s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0398 acc:0.5762 off1:0.9485 mae:0.4793\n",
      "  Region Acc: [0.593 0.568 0.548 0.662 0.576 0.51 ]\n",
      "\n",
      "[Phase1 Epoch 19/30]\n",
      "  Train - acc:0.5342 off1:0.8969 mae:0.5895\n",
      "  Val   - acc:0.5762 off1:0.9485 mae:0.4793\n",
      "  Part losses (Val): [1.013 1.05  1.084 0.916 1.048 1.128]\n",
      "  LR:1.00e-04 | 13.0s | Pat:1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0535 acc:0.5777 off1:0.9521 mae:0.4731\n",
      "  Region Acc: [0.588 0.564 0.552 0.652 0.587 0.524]\n",
      "\n",
      "[Phase1 Epoch 20/30]\n",
      "  Train - acc:0.5758 off1:0.9170 mae:0.5226\n",
      "  Val   - acc:0.5777 off1:0.9521 mae:0.4731\n",
      "  Part losses (Val): [1.036 1.066 1.091 0.935 1.055 1.139]\n",
      "  LR:1.00e-04 | 13.0s | Pat:2/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0484 acc:0.5711 off1:0.9476 mae:0.4852\n",
      "  Region Acc: [0.609 0.556 0.541 0.65  0.575 0.497]\n",
      "\n",
      "[Phase1 Epoch 21/30]\n",
      "  Train - acc:0.5525 off1:0.9025 mae:0.5646\n",
      "  Val   - acc:0.5711 off1:0.9476 mae:0.4852\n",
      "  Part losses (Val): [1.035 1.069 1.088 0.93  1.036 1.132]\n",
      "  LR:1.00e-04 | 13.2s | Pat:3/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0643 acc:0.5707 off1:0.9556 mae:0.4766\n",
      "  Region Acc: [0.589 0.567 0.555 0.649 0.57  0.495]\n",
      "\n",
      "[Phase1 Epoch 22/30]\n",
      "  Train - acc:0.5213 off1:0.8723 mae:0.6318\n",
      "  Val   - acc:0.5707 off1:0.9556 mae:0.4766\n",
      "  Part losses (Val): [1.025 1.094 1.098 0.929 1.07  1.171]\n",
      "  LR:1.00e-04 | 13.2s | Pat:4/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0811 acc:0.5689 off1:0.9565 mae:0.4783\n",
      "  Region Acc: [0.566 0.575 0.553 0.637 0.578 0.505]\n",
      "\n",
      "[Phase1 Epoch 23/30]\n",
      "  Train - acc:0.5458 off1:0.8896 mae:0.5896\n",
      "  Val   - acc:0.5689 off1:0.9565 mae:0.4783\n",
      "  Part losses (Val): [1.054 1.088 1.108 0.969 1.076 1.19 ]\n",
      "  LR:1.00e-04 | 12.9s | Pat:5/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0769 acc:0.5711 off1:0.9518 mae:0.4806\n",
      "  Region Acc: [0.61  0.565 0.541 0.641 0.558 0.512]\n",
      "\n",
      "[Phase1 Epoch 24/30]\n",
      "  Train - acc:0.5398 off1:0.8823 mae:0.6072\n",
      "  Val   - acc:0.5711 off1:0.9518 mae:0.4806\n",
      "  Part losses (Val): [1.053 1.087 1.132 0.95  1.068 1.172]\n",
      "  LR:5.00e-05 | 12.8s | Pat:6/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0737 acc:0.5711 off1:0.9485 mae:0.4841\n",
      "  Region Acc: [0.594 0.567 0.552 0.638 0.573 0.502]\n",
      "\n",
      "[Phase1 Epoch 25/30]\n",
      "  Train - acc:0.5903 off1:0.9162 mae:0.5099\n",
      "  Val   - acc:0.5711 off1:0.9485 mae:0.4841\n",
      "  Part losses (Val): [1.034 1.092 1.114 0.963 1.067 1.172]\n",
      "  LR:5.00e-05 | 12.9s | Pat:7/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0888 acc:0.5705 off1:0.9455 mae:0.4874\n",
      "  Region Acc: [0.589 0.557 0.558 0.638 0.579 0.502]\n",
      "\n",
      "[Phase1 Epoch 26/30]\n",
      "  Train - acc:0.5603 off1:0.8902 mae:0.5746\n",
      "  Val   - acc:0.5705 off1:0.9455 mae:0.4874\n",
      "  Part losses (Val): [1.064 1.112 1.12  0.985 1.075 1.177]\n",
      "  LR:5.00e-05 | 13.0s | Pat:8/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0848 acc:0.5735 off1:0.9490 mae:0.4814\n",
      "  Region Acc: [0.598 0.559 0.558 0.651 0.566 0.509]\n",
      "\n",
      "[Phase1 Epoch 27/30]\n",
      "  Train - acc:0.5799 off1:0.9055 mae:0.5341\n",
      "  Val   - acc:0.5735 off1:0.9490 mae:0.4814\n",
      "  Part losses (Val): [1.06  1.098 1.128 0.963 1.089 1.172]\n",
      "  LR:5.00e-05 | 13.3s | Pat:9/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0911 acc:0.5711 off1:0.9466 mae:0.4861\n",
      "  Region Acc: [0.596 0.56  0.545 0.656 0.56  0.509]\n",
      "\n",
      "‚èπÔ∏è Phase 1 Early stopping at epoch 28\n",
      "\n",
      "======================================================================\n",
      "üìä Phase 1 Average Part Losses:\n",
      "   A: 1.0567\n",
      "   B: 1.0975\n",
      "   C: 1.1190\n",
      "   D: 0.9391\n",
      "   E: 1.0765\n",
      "   F: 1.1522\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üìç PHASE 2: Training with Difficulty-Based Weights\n",
      "======================================================================\n",
      "\n",
      "üéØ Calculated Part Weights:\n",
      "   A: 0.992 (loss: 1.0567)\n",
      "   B: 1.011 (loss: 1.0975)\n",
      "   C: 1.021 (loss: 1.1190)\n",
      "   D: 0.935 (loss: 0.9391)\n",
      "   E: 1.001 (loss: 1.0765)\n",
      "   F: 1.036 (loss: 1.1522)\n",
      "============================================================\n",
      "Class Distribution Analysis\n",
      "============================================================\n",
      "\n",
      "A:\n",
      "  Class 0: 1791 ( 50.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1122 ( 31.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  457 ( 12.8%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  214 (  6.0%) ‚ñà‚ñà\n",
      "\n",
      "B:\n",
      "  Class 0:  740 ( 20.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  912 ( 25.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1159 ( 32.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  773 ( 21.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "C:\n",
      "  Class 0:  392 ( 10.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  797 ( 22.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1295 ( 36.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1100 ( 30.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "D:\n",
      "  Class 0: 1791 ( 50.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1114 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  452 ( 12.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  227 (  6.3%) ‚ñà‚ñà‚ñà\n",
      "\n",
      "E:\n",
      "  Class 0:  726 ( 20.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  982 ( 27.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1109 ( 30.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  767 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "F:\n",
      "  Class 0:  409 ( 11.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  765 ( 21.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1292 ( 36.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1118 ( 31.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "‚úÖ Class weights: [0.95209146 0.96513265 0.9590858  1.12369   ]\n",
      "‚úÖ Part weights: [0.99216276 1.0110927  1.020975   0.93532246 1.0014079  1.0359867 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0394 acc:0.5837 off1:0.9571 mae:0.4625\n",
      "  Region Acc: [0.598 0.583 0.562 0.649 0.593 0.516]\n",
      "‚úÖ Phase 2 Best (Acc=0.5837, MAE=0.4625)\n",
      "\n",
      "[Phase2 Epoch 01/50]\n",
      "  Train - acc:0.5283 off1:0.8868 mae:0.6114\n",
      "  Val   - acc:0.5837 off1:0.9571 mae:0.4625\n",
      "  Part losses (Val): [0.999 1.061 1.107 0.85  1.047 1.173]\n",
      "  LR:5.00e-05 | 13.3s | Pat:0/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0511 acc:0.5676 off1:0.9501 mae:0.4867\n",
      "  Region Acc: [0.592 0.537 0.544 0.663 0.56  0.509]\n",
      "\n",
      "[Phase2 Epoch 02/50]\n",
      "  Train - acc:0.5843 off1:0.9285 mae:0.5007\n",
      "  Val   - acc:0.5676 off1:0.9501 mae:0.4867\n",
      "  Part losses (Val): [1.028 1.078 1.105 0.861 1.068 1.167]\n",
      "  LR:5.00e-05 | 12.7s | Pat:1/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0558 acc:0.5819 off1:0.9554 mae:0.4662\n",
      "  Region Acc: [0.606 0.569 0.553 0.66  0.578 0.525]\n",
      "\n",
      "[Phase2 Epoch 03/50]\n",
      "  Train - acc:0.5724 off1:0.9119 mae:0.5344\n",
      "  Val   - acc:0.5819 off1:0.9554 mae:0.4662\n",
      "  Part losses (Val): [1.036 1.08  1.112 0.869 1.052 1.185]\n",
      "  LR:5.00e-05 | 13.5s | Pat:2/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0629 acc:0.5804 off1:0.9545 mae:0.4691\n",
      "  Region Acc: [0.592 0.577 0.555 0.65  0.58  0.529]\n",
      "\n",
      "[Phase2 Epoch 04/50]\n",
      "  Train - acc:0.5764 off1:0.9159 mae:0.5247\n",
      "  Val   - acc:0.5804 off1:0.9545 mae:0.4691\n",
      "  Part losses (Val): [1.039 1.084 1.119 0.883 1.06  1.191]\n",
      "  LR:5.00e-05 | 12.8s | Pat:3/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0633 acc:0.5757 off1:0.9505 mae:0.4777\n",
      "  Region Acc: [0.589 0.572 0.56  0.635 0.59  0.508]\n",
      "\n",
      "[Phase2 Epoch 05/50]\n",
      "  Train - acc:0.5916 off1:0.9206 mae:0.5029\n",
      "  Val   - acc:0.5757 off1:0.9505 mae:0.4777\n",
      "  Part losses (Val): [1.029 1.083 1.132 0.872 1.056 1.208]\n",
      "  LR:5.00e-05 | 13.0s | Pat:4/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0599 acc:0.5769 off1:0.9541 mae:0.4730\n",
      "  Region Acc: [0.586 0.578 0.569 0.636 0.571 0.522]\n",
      "\n",
      "[Phase2 Epoch 06/50]\n",
      "  Train - acc:0.5774 off1:0.9153 mae:0.5219\n",
      "  Val   - acc:0.5769 off1:0.9541 mae:0.4730\n",
      "  Part losses (Val): [1.022 1.087 1.126 0.873 1.064 1.187]\n",
      "  LR:5.00e-05 | 12.8s | Pat:5/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0627 acc:0.5819 off1:0.9525 mae:0.4693\n",
      "  Region Acc: [0.598 0.569 0.565 0.66  0.58  0.52 ]\n",
      "\n",
      "[Phase2 Epoch 07/50]\n",
      "  Train - acc:0.5836 off1:0.9172 mae:0.5154\n",
      "  Val   - acc:0.5819 off1:0.9525 mae:0.4693\n",
      "  Part losses (Val): [1.041 1.079 1.114 0.874 1.074 1.194]\n",
      "  LR:2.50e-05 | 13.2s | Pat:6/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0649 acc:0.5806 off1:0.9538 mae:0.4693\n",
      "  Region Acc: [0.589 0.569 0.561 0.652 0.588 0.524]\n",
      "\n",
      "[Phase2 Epoch 08/50]\n",
      "  Train - acc:0.5572 off1:0.9007 mae:0.5642\n",
      "  Val   - acc:0.5806 off1:0.9538 mae:0.4693\n",
      "  Part losses (Val): [1.043 1.085 1.118 0.882 1.062 1.199]\n",
      "  LR:2.50e-05 | 12.7s | Pat:7/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0759 acc:0.5821 off1:0.9545 mae:0.4667\n",
      "  Region Acc: [0.601 0.561 0.565 0.651 0.587 0.527]\n",
      "\n",
      "[Phase2 Epoch 09/50]\n",
      "  Train - acc:0.5859 off1:0.9128 mae:0.5196\n",
      "  Val   - acc:0.5821 off1:0.9545 mae:0.4667\n",
      "  Part losses (Val): [1.056 1.096 1.127 0.888 1.078 1.21 ]\n",
      "  LR:2.50e-05 | 12.9s | Pat:8/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0793 acc:0.5777 off1:0.9525 mae:0.4733\n",
      "  Region Acc: [0.598 0.567 0.561 0.649 0.58  0.511]\n",
      "\n",
      "[Phase2 Epoch 10/50]\n",
      "  Train - acc:0.6305 off1:0.9409 mae:0.4383\n",
      "  Val   - acc:0.5777 off1:0.9525 mae:0.4733\n",
      "  Part losses (Val): [1.062 1.096 1.131 0.895 1.076 1.216]\n",
      "  LR:2.50e-05 | 13.2s | Pat:9/10\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0792 acc:0.5760 off1:0.9518 mae:0.4762\n",
      "  Region Acc: [0.603 0.557 0.56  0.654 0.566 0.516]\n",
      "\n",
      "‚èπÔ∏è Phase 2 Early stopping at epoch 11\n",
      "\n",
      "======================================================================\n",
      "üéâ Training Finished!\n",
      "======================================================================\n",
      "\n",
      "üìä Test evaluation with Phase 2 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] loss:1.0002 acc:0.5936 off1:0.9658 mae:0.4418\n",
      "  Region Acc: [0.623 0.589 0.521 0.726 0.568 0.534]\n",
      "\n",
      "üèÜ Test Results:\n",
      "   Accuracy: 0.5936\n",
      "   Off-by-1: 0.9658\n",
      "   MAE: 0.4418\n",
      "   Region Acc: [0.623 0.589 0.521 0.726 0.568 0.534]\n",
      "   Part losses: [0.91  1.081 1.075 0.787 0.997 1.15 ]\n",
      "\n",
      "üíæ Phase 1 model saved: runs_severity_classification/best_efficientnet_b0_classification.pth\n",
      "üíæ Phase 2 model saved: runs_severity_classification/phase2_weighted_classification.pth\n",
      "\n",
      "üìà Summary:\n",
      "   Phase 1 Best Acc: 0.5853\n",
      "   Phase 2 Best Acc: 0.5837\n",
      "   Improvement: -0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- Í≤ΩÎ°ú ÏÑ§Ï†ï Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ---\n",
    "BASE_DIR     = f\"./data/covid19-xray-severity-scoring/\"\n",
    "CSV_PATH     = str(Path(BASE_DIR) / \"Brixia.csv\")\n",
    "IMAGE_DIR    = str(Path(BASE_DIR) / \"segmented_png\")\n",
    "\n",
    "OUT_DIR      = \"./runs_severity_classification\"\n",
    "BEST_PATH    = str(Path(OUT_DIR) / \"best_efficientnet_b0_classification.pth\")\n",
    "PHASE2_PATH  = str(Path(OUT_DIR) / \"phase2_weighted_classification.pth\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED         = 42\n",
    "IMG_SIZE     = 224\n",
    "BATCH_SIZE   = 128  # üîÑ Î∂ÑÎ•òÎäî Î∞∞Ïπò ÌÅ¨Í∏∞ Ï§ÑÏûÑ\n",
    "NUM_CLASSES  = 4   # ‚úÖ NEW: 0, 1, 2, 3\n",
    "EPOCHS_PHASE1 = 30\n",
    "EPOCHS_PHASE2 = 50\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "AMP          = True\n",
    "EARLY_STOP_ACC = 0.75  # üîÑ MAE ‚Üí Accuracy\n",
    "DROP_RATIO   = 0.3\n",
    "AUG_RATIO    = 0.5\n",
    "MIXUP_ALPHA  = 0.2\n",
    "LABEL_SMOOTHING = 0.1  # ‚úÖ NEW: Label smoothing\n",
    "\n",
    "# --- ÏãúÎìú Í≥†Ï†ï ---\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "def make_transform_with_label(train: bool, img_size: int = IMG_SIZE, aug_ratio=AUG_RATIO):\n",
    "    \"\"\"Brixia ScoreÏùò Ï¢åÏö∞ Íµ¨Ï°∞Î•º Í≥†Î†§Ìïú transform\"\"\"\n",
    "    def _tfm(img: Image.Image, label: torch.Tensor = None):\n",
    "        img = img.convert('RGB')\n",
    "        img = TF.resize(\n",
    "            img, \n",
    "            [img_size, img_size], \n",
    "            interpolation=TF.InterpolationMode.BILINEAR,\n",
    "            antialias=True\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            # 1. Horizontal Flip (ÎùºÎ≤®ÎèÑ Ìï®Íªò flip)\n",
    "            if random.random() < aug_ratio:\n",
    "                img = TF.hflip(img)\n",
    "                if label is not None:\n",
    "                    label = label[[3, 4, 5, 0, 1, 2]]\n",
    "            \n",
    "            # 2. ÏïΩÌïú ÌöåÏ†Ñ (¬±5ÎèÑ)\n",
    "            if random.random() < aug_ratio:\n",
    "                angle = float(torch.empty(1).uniform_(-5, 5))\n",
    "                img = TF.rotate(\n",
    "                    img, \n",
    "                    angle, \n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 3. ÏïΩÌïú Translation\n",
    "            if random.random() < aug_ratio:\n",
    "                max_dx = 0.05 * img_size\n",
    "                max_dy = 0.05 * img_size\n",
    "                translations = (\n",
    "                    float(torch.empty(1).uniform_(-max_dx, max_dx)),\n",
    "                    float(torch.empty(1).uniform_(-max_dy, max_dy))\n",
    "                )\n",
    "                img = TF.affine(\n",
    "                    img,\n",
    "                    angle=0,\n",
    "                    translate=translations,\n",
    "                    scale=1.0,\n",
    "                    shear=0,\n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 4. Brightness & Contrast\n",
    "            if random.random() < aug_ratio:\n",
    "                brightness_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_brightness(img, brightness_factor)\n",
    "            \n",
    "            if random.random() < aug_ratio:\n",
    "                contrast_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_contrast(img, contrast_factor)\n",
    "            \n",
    "            # 5. Gamma Correction\n",
    "            if random.random() < 0.3:\n",
    "                gamma = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_gamma(img, gamma)\n",
    "        \n",
    "        # Tensor Î≥ÄÌôò\n",
    "        img = TF.to_tensor(img)\n",
    "        \n",
    "        # Gaussian Noise (train only)\n",
    "        if train and random.random() < 0.2:\n",
    "            noise = torch.randn_like(img) * 0.01\n",
    "            img = img + noise\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôî\n",
    "        img = TF.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        if label is not None:\n",
    "            return img, label\n",
    "        return img\n",
    "    \n",
    "    return _tfm\n",
    "\n",
    "def load_and_split_brixia(csv_path, val_ratio=0.2, seed=SEED):\n",
    "    df = pd.read_csv(csv_path, dtype={'BrixiaScore': str})\n",
    "    df = df.dropna(subset=['BrixiaScore'])\n",
    "    df = df[df['BrixiaScore'] != 'nan']\n",
    "    df = df[df['BrixiaScore'].str.len() == 6].copy()\n",
    "    \n",
    "    print(f\"Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: {len(df)}Í∞ú\")\n",
    "    \n",
    "    if 'ConsensusTestset' in df.columns:\n",
    "        test_df = df[df['ConsensusTestset'] == 1].copy()\n",
    "        train_val_df = df[df['ConsensusTestset'] == 0].copy()\n",
    "    else:\n",
    "        test_df = pd.DataFrame()\n",
    "        train_val_df = df.copy()\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
    "    train_idx, val_idx = next(gss.split(\n",
    "        train_val_df, \n",
    "        groups=train_val_df['StudyId']\n",
    "    ))\n",
    "    \n",
    "    tr_df = train_val_df.iloc[train_idx].copy()\n",
    "    val_df = train_val_df.iloc[val_idx].copy()\n",
    "    \n",
    "    print(f\"Train: {len(tr_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    validate_split(tr_df, val_df, test_df)\n",
    "    \n",
    "    return tr_df, val_df, test_df\n",
    "\n",
    "def validate_split(tr_df, val_df, tt_df):\n",
    "    train_studies = set(tr_df['StudyId'])\n",
    "    val_studies = set(val_df['StudyId'])\n",
    "    test_studies = set(tt_df['StudyId']) if len(tt_df) > 0 else set()\n",
    "    \n",
    "    assert len(train_studies & val_studies) == 0, \"Train-Val Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(train_studies & test_studies) == 0, \"Train-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(val_studies & test_studies) == 0, \"Val-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    \n",
    "    for name, data in [('Train', tr_df), ('Val', val_df), ('Test', tt_df)]:\n",
    "        if len(data) > 0:\n",
    "            scores = data['BrixiaScore'].apply(lambda x: sum(int(c) for c in x))\n",
    "            print(f\"{name} - Mean: {scores.mean():.2f}, Std: {scores.std():.2f}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class BrixiaDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_col = \"Filename\"\n",
    "        self.label_col = \"BrixiaScore\"\n",
    "        self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        assert self.img_col in self.df.columns\n",
    "        assert self.label_col in self.df.columns\n",
    "        \n",
    "        invalid_scores = self.df[self.df[self.label_col].str.len() != 6]\n",
    "        if len(invalid_scores) > 0:\n",
    "            print(f\"‚ö†Ô∏è Í≤ΩÍ≥†: {len(invalid_scores)}Í∞úÏùò ÏûòÎ™ªÎêú BrixiaScore Î∞úÍ≤¨\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_name_from_csv = row[self.img_col]\n",
    "        img_name = img_name_from_csv.replace('.dcm', '.png')\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïò§Î•ò: {img_path}\")\n",
    "            raise\n",
    "        \n",
    "        scores_str = row[self.label_col]\n",
    "        scores_list = [int(c) for c in scores_str]\n",
    "        labels = torch.tensor(scores_list, dtype=torch.long)  # üîÑ longÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "        \n",
    "        if self.transform:\n",
    "            image, labels = self.transform(image, labels)\n",
    "        else:\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "def create_dataloaders(tr_df, val_df, tt_df, img_dir, \n",
    "                       batch_size=32, img_size=224, num_workers=4):\n",
    "    train_transform = make_transform_with_label(train=True, img_size=img_size)\n",
    "    val_transform = make_transform_with_label(train=False, img_size=img_size)\n",
    "    \n",
    "    tr_ds = BrixiaDataset(tr_df, img_dir, transform=train_transform)\n",
    "    val_ds = BrixiaDataset(val_df, img_dir, transform=val_transform)\n",
    "    tt_ds = BrixiaDataset(tt_df, img_dir, transform=val_transform)\n",
    "    \n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    tt_loader = DataLoader(\n",
    "        tt_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "    print(f\"   Train: {len(tr_ds)} samples, {len(tr_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_ds)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(tt_ds)} samples, {len(tt_loader)} batches\")\n",
    "    \n",
    "    return tr_loader, val_loader, tt_loader\n",
    "\n",
    "# ============================================================\n",
    "# Loss Function (Î∂ÑÎ•òÏö©ÏúºÎ°ú Î≥ÄÍ≤Ω)\n",
    "# ============================================================\n",
    "def calculate_class_weights(labels, num_classes=4, method='sqrt_inverse'):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    counts = np.bincount(labels_flat.astype(int), minlength=num_classes)\n",
    "    \n",
    "    if method == 'sqrt_inverse':\n",
    "        weights = 1.0 / (np.sqrt(counts) + 1e-6)\n",
    "    elif method == 'inverse':\n",
    "        weights = 1.0 / (counts + 1e-6)\n",
    "    else:\n",
    "        total = len(labels_flat)\n",
    "        weights = total / (num_classes * (counts + 1e-6))\n",
    "    \n",
    "    weights = weights / weights.mean()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def print_class_distribution(labels):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Class Distribution Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    region_names = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    for idx, name in enumerate(region_names):\n",
    "        region_labels = labels[:, idx]\n",
    "        counts = np.bincount(region_labels.astype(int), minlength=4)\n",
    "        total = counts.sum()\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        for cls in range(4):\n",
    "            pct = 100 * counts[cls] / total if total > 0 else 0\n",
    "            bar = '‚ñà' * int(pct / 2)\n",
    "            print(f\"  Class {cls}: {counts[cls]:4d} ({pct:5.1f}%) {bar}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "class AdaptiveClassificationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Îã§Ï§ë ÌÅ¥ÎûòÏä§ Î∂ÑÎ•òÎ•º ÏúÑÌïú ÏÜêÏã§Ìï®Ïàò\n",
    "    - CrossEntropyLoss Í∏∞Î∞ò\n",
    "    - ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "    - Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "    - Label Smoothing ÏßÄÏõê\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_labels, num_classes=4, use_class_weights=True, \n",
    "                 part_weights=None, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        if isinstance(train_labels, torch.Tensor):\n",
    "            train_labels_np = train_labels.cpu().numpy()\n",
    "        else:\n",
    "            train_labels_np = train_labels\n",
    "        \n",
    "        print_class_distribution(train_labels_np)\n",
    "        \n",
    "        # ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "        if use_class_weights:\n",
    "            class_weights = calculate_class_weights(train_labels_np, num_classes=num_classes)\n",
    "            self.register_buffer('class_weights', class_weights)\n",
    "            print(f\"‚úÖ Class weights: {class_weights.numpy()}\")\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò\n",
    "        if part_weights is None:\n",
    "            self.part_weights = torch.ones(6)\n",
    "        else:\n",
    "            self.part_weights = torch.tensor(part_weights, dtype=torch.float32)\n",
    "        self.register_buffer('part_weights_buf', self.part_weights)\n",
    "        print(f\"‚úÖ Part weights: {self.part_weights.numpy()}\")\n",
    "\n",
    "    def forward(self, pred_logits, target, use_mixup=False):\n",
    "        \"\"\"\n",
    "        pred_logits: [B, 6, 4] - Í∞Å Î∂ÄÏúÑÎ≥Ñ 4Í∞ú ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌïú logits\n",
    "        target: [B, 6] - Í∞Å Î∂ÄÏúÑÎ≥Ñ ÌÅ¥ÎûòÏä§ Î†àÏù¥Î∏î (0~3)\n",
    "        \"\"\"\n",
    "        B, num_regions, num_classes = pred_logits.shape\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        pred_logits_flat = pred_logits.view(B * num_regions, num_classes)  # [B*6, 4]\n",
    "        target_flat = target.view(B * num_regions)  # [B*6]\n",
    "        \n",
    "        # CrossEntropyLoss with label smoothing\n",
    "        if self.class_weights is not None and not use_mixup:\n",
    "            criterion = nn.CrossEntropyLoss(\n",
    "                weight=self.class_weights.to(pred_logits.device),\n",
    "                label_smoothing=self.label_smoothing,\n",
    "                reduction='none'\n",
    "            )\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(\n",
    "                label_smoothing=self.label_smoothing,\n",
    "                reduction='none'\n",
    "            )\n",
    "        \n",
    "        loss = criterion(pred_logits_flat, target_flat)  # [B*6]\n",
    "        loss = loss.view(B, num_regions)  # [B, 6]\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "        part_weights = self.part_weights_buf.to(pred_logits.device)\n",
    "        loss = loss * part_weights.unsqueeze(0)  # [B, 6]\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ ÌèâÍ∑† ÏÜêÏã§ (Î™®ÎãàÌÑ∞ÎßÅÏö©)\n",
    "        part_losses = loss.mean(dim=0)  # [6]\n",
    "        \n",
    "        return loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Mixup (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "def mixup_data_classification(x, y, alpha=MIXUP_ALPHA):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•òÏö© Mixup\n",
    "    yÎäî one-hotÏúºÎ°ú Î≥ÄÌôò ÌõÑ mixup\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    \n",
    "    # One-hot encoding\n",
    "    y_onehot = F.one_hot(y, num_classes=NUM_CLASSES).float()  # [B, 6, 4]\n",
    "    y_onehot_shuffled = y_onehot[index]\n",
    "    \n",
    "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot_shuffled  # [B, 6, 4]\n",
    "    \n",
    "    return mixed_x, mixed_y, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred_logits, y_mixed, lam):\n",
    "    \"\"\"MixupÏùÑ ÏúÑÌïú ÏÜêÏã§ Í≥ÑÏÇ∞\"\"\"\n",
    "    # y_mixedÎäî [B, 6, 4] soft labels\n",
    "    # pred_logitsÎäî [B, 6, 4]\n",
    "    \n",
    "    B, num_regions, num_classes = pred_logits.shape\n",
    "    \n",
    "    # Reshape\n",
    "    pred_flat = pred_logits.view(B * num_regions, num_classes)  # [B*6, 4]\n",
    "    target_flat = y_mixed.view(B * num_regions, num_classes)  # [B*6, 4]\n",
    "    \n",
    "    # Soft target loss\n",
    "    log_probs = F.log_softmax(pred_flat, dim=1)\n",
    "    loss = -(target_flat * log_probs).sum(dim=1)  # [B*6]\n",
    "    loss = loss.view(B, num_regions)  # [B, 6]\n",
    "    \n",
    "    # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö© (criterionÏóêÏÑú Í∞ÄÏ†∏Ïò¥)\n",
    "    if hasattr(criterion, 'part_weights_buf'):\n",
    "        part_weights = criterion.part_weights_buf.to(pred_logits.device)\n",
    "        loss = loss * part_weights.unsqueeze(0)\n",
    "    \n",
    "    part_losses = loss.mean(dim=0)\n",
    "    return loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Metrics (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def calculate_classification_metrics(pred_logits, labels):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•ò ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    pred_logits: [B, 6, 4]\n",
    "    labels: [B, 6]\n",
    "    \"\"\"\n",
    "    # ÏòàÏ∏° ÌÅ¥ÎûòÏä§\n",
    "    preds = pred_logits.argmax(dim=-1)  # [B, 6]\n",
    "    \n",
    "    # Exact match accuracy\n",
    "    exact_acc = (preds == labels).float().mean().item()\n",
    "    \n",
    "    # Off-by-1 accuracy (Ïù∏Ï†ë ÌÅ¥ÎûòÏä§ ÌóàÏö©)\n",
    "    off_by_1 = (torch.abs(preds - labels) <= 1).float().mean().item()\n",
    "    \n",
    "    # Per-region accuracy\n",
    "    region_acc = (preds == labels).float().mean(dim=0)  # [6]\n",
    "    \n",
    "    # MAE (Ï∞∏Í≥†Ïö©)\n",
    "    mae = torch.abs(preds.float() - labels.float()).mean().item()\n",
    "    \n",
    "    return exact_acc, off_by_1, mae, region_acc\n",
    "\n",
    "# ============================================================\n",
    "# Model (Î∂ÑÎ•òÏö©ÏúºÎ°ú Î≥ÄÍ≤Ω)\n",
    "# ============================================================\n",
    "class EfficientNetB0Classification(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•òÎ•º ÏúÑÌïú Î™®Îç∏\n",
    "    Í∞Å Î∂ÄÏúÑ(A~F)ÎßàÎã§ 4Í∞ú ÌÅ¥ÎûòÏä§(0~3) ÏòàÏ∏°\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True, drop=0.3, num_regions=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = efficientnet_b0(weights=weights)\n",
    "        \n",
    "        self.features = backbone.features\n",
    "        in_feat = 1280\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 49, in_feat))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Region queries\n",
    "        self.region_queries = nn.Parameter(torch.randn(num_regions, in_feat))\n",
    "        nn.init.xavier_uniform_(self.region_queries)\n",
    "        \n",
    "        # Cross attention\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_feat)\n",
    "        self.norm2 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_feat * 2, in_feat),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        \n",
    "        # üîÑ Classification heads: Í∞Å Î∂ÄÏúÑÎßàÎã§ 4Í∞ú ÌÅ¥ÎûòÏä§ ÏòàÏ∏°\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feat, 256),\n",
    "                nn.LayerNorm(256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(128, num_classes)  # üîÑ 4Í∞ú ÌÅ¥ÎûòÏä§ Ï∂úÎ†•\n",
    "            ) for _ in range(num_regions)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # Feature extraction\n",
    "        feat = self.features(x)  # [B, 1280, 7, 7]\n",
    "        feat = feat.flatten(2).transpose(1, 2)  # [B, 49, 1280]\n",
    "        feat = feat + self.pos_embed\n",
    "        \n",
    "        # Region queries\n",
    "        queries = self.region_queries.unsqueeze(0).expand(B, -1, -1)  # [B, 6, 1280]\n",
    "        \n",
    "        # Cross attention\n",
    "        attn_out, _ = self.cross_attention(\n",
    "            query=queries,\n",
    "            key=feat,\n",
    "            value=feat\n",
    "        )\n",
    "        \n",
    "        attn_out = self.norm1(attn_out + queries)\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        attn_out = self.norm2(attn_out + ffn_out)  # [B, 6, 1280]\n",
    "        \n",
    "        # üîÑ Í∞Å Î∂ÄÏúÑÎ≥Ñ classification head ÌÜµÍ≥º\n",
    "        outputs = []\n",
    "        for i in range(len(self.heads)):\n",
    "            region_feat = attn_out[:, i, :]  # [B, 1280]\n",
    "            logits = self.heads[i](region_feat)  # [B, 4]\n",
    "            outputs.append(logits)\n",
    "        \n",
    "        out = torch.stack(outputs, dim=1)  # [B, 6, 4]\n",
    "        return out\n",
    "\n",
    "# ============================================================\n",
    "# Training Functions (Î∂ÑÎ•òÏö© ÏàòÏ†ï)\n",
    "# ============================================================\n",
    "def train_epoch(model, tr_loader, criterion, optimizer, scaler, device, \n",
    "                amp=True, use_mixup=True):\n",
    "    model.train()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(tr_loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)  # [B, 6]\n",
    "        \n",
    "        # Mixup Ï†ÅÏö©\n",
    "        is_mixup = use_mixup and (random.random() < 0.5)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast(enabled=amp):\n",
    "            pred_logits = model(imgs)  # [B, 6, 4]\n",
    "            \n",
    "            if is_mixup:\n",
    "                imgs_mixed, labels_mixed, lam = mixup_data_classification(imgs, labels)\n",
    "                pred_logits = model(imgs_mixed)\n",
    "                loss, part_losses = mixup_criterion(criterion, pred_logits, labels_mixed, lam)\n",
    "            else:\n",
    "                loss, part_losses = criterion(pred_logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Metrics (ÏõêÎ≥∏ labelsÎ°ú Í≥ÑÏÇ∞)\n",
    "        exact_acc, off_by_1, mae, _ = calculate_classification_metrics(pred_logits.detach(), labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\",\n",
    "            mae=f\"{run_mae/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    \n",
    "    return run_loss/n, run_acc/n, run_off1/n, run_mae/n, part_losses_avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, split='val'):\n",
    "    model.eval()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    region_acc_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"{split.capitalize()}\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        pred_logits = model(imgs)  # [B, 6, 4]\n",
    "        loss, part_losses = criterion(pred_logits, labels)\n",
    "        \n",
    "        exact_acc, off_by_1, mae, region_acc = calculate_classification_metrics(pred_logits, labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        region_acc_sum += region_acc.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    avg_loss = run_loss/n\n",
    "    avg_acc = run_acc/n\n",
    "    avg_off1 = run_off1/n\n",
    "    avg_mae = run_mae/n\n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    region_acc_avg = region_acc_sum / n\n",
    "    \n",
    "    print(f\"[{split}] loss:{avg_loss:.4f} acc:{avg_acc:.4f} \"\n",
    "          f\"off1:{avg_off1:.4f} mae:{avg_mae:.4f}\")\n",
    "    print(f\"  Region Acc: {region_acc_avg.numpy().round(3)}\")\n",
    "    \n",
    "    return avg_loss, avg_acc, avg_off1, avg_mae, part_losses_avg, region_acc_avg\n",
    "\n",
    "def get_lrs(optimizer):\n",
    "    return [pg['lr'] for pg in optimizer.param_groups]\n",
    "\n",
    "# ============================================================\n",
    "# Main Function\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ Two-Phase Classification Training\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "    print(\"\\nüìÇ Loading data...\")\n",
    "    tr_df, val_df, tt_df = load_and_split_brixia(CSV_PATH)\n",
    "    \n",
    "    print(\"\\nüì¶ Creating DataLoaders...\")\n",
    "    tr_loader, val_loader, tt_loader = create_dataloaders(\n",
    "        tr_df, val_df, tt_df, img_dir=IMAGE_DIR, \n",
    "        batch_size=BATCH_SIZE, img_size=IMG_SIZE, num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Train labels Ï∂îÏ∂ú\n",
    "    train_labels = torch.cat([labels for _, labels in tr_loader], dim=0)\n",
    "    \n",
    "    # ========================================\n",
    "    # Phase 1: Í∑†Îì± Í∞ÄÏ§ëÏπò ÌïôÏäµ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìç PHASE 1: Training with Uniform Weights\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    criterion_phase1 = AdaptiveClassificationLoss(\n",
    "        train_labels, \n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_class_weights=True,\n",
    "        part_weights=None,\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    )\n",
    "    \n",
    "    model = EfficientNetB0Classification(\n",
    "        pretrained=True, \n",
    "        drop=DROP_RATIO,\n",
    "        num_regions=6,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    scaler = GradScaler(enabled=AMP)\n",
    "    \n",
    "    best_acc_phase1 = 0.0\n",
    "    patience_counter = 0\n",
    "    max_patience = 10\n",
    "    \n",
    "    # Phase 1 ÌïôÏäµ\n",
    "    phase1_part_losses = []\n",
    "    \n",
    "    for ep in range(1, EPOCHS_PHASE1 + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        tr_loss, tr_acc, tr_off1, tr_mae, tr_part_losses = train_epoch(\n",
    "            model, tr_loader, criterion_phase1, optimizer, scaler, DEVICE, AMP\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_off1, val_mae, val_part_losses, val_region_acc = evaluate(\n",
    "            model, val_loader, criterion_phase1, DEVICE, split='val'\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ ÏÜêÏã§ ÎàÑÏ†Å\n",
    "        phase1_part_losses.append(val_part_losses.numpy())\n",
    "        \n",
    "        # Save best\n",
    "        if val_acc > best_acc_phase1:\n",
    "            best_acc_phase1 = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': best_acc_phase1,\n",
    "                'val_mae': val_mae,\n",
    "            }, BEST_PATH)\n",
    "            print(f\"‚úÖ Phase 1 Best (Acc={best_acc_phase1:.4f}, MAE={val_mae:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\n‚èπÔ∏è Phase 1 Early stopping at epoch {ep}\")\n",
    "            break\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"\\n[Phase1 Epoch {ep:02d}/{EPOCHS_PHASE1}]\")\n",
    "        print(f\"  Train - acc:{tr_acc:.4f} off1:{tr_off1:.4f} mae:{tr_mae:.4f}\")\n",
    "        print(f\"  Val   - acc:{val_acc:.4f} off1:{val_off1:.4f} mae:{val_mae:.4f}\")\n",
    "        print(f\"  Part losses (Val): {val_part_losses.numpy().round(3)}\")\n",
    "        print(f\"  LR:{get_lrs(optimizer)[0]:.2e} | {elapsed:.1f}s | Pat:{patience_counter}/{max_patience}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # Phase 1 Î∂ÄÏúÑÎ≥Ñ ÌèâÍ∑† ÏÜêÏã§ Í≥ÑÏÇ∞\n",
    "    avg_part_losses = np.mean(phase1_part_losses, axis=0)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä Phase 1 Average Part Losses:\")\n",
    "    region_names = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    for i, name in enumerate(region_names):\n",
    "        print(f\"   {name}: {avg_part_losses[i]:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ========================================\n",
    "    # Phase 2: ÎÇúÏù¥ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìç PHASE 2: Training with Difficulty-Based Weights\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ÎÇúÏù¥ÎèÑ Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\n",
    "    normalized_losses = avg_part_losses / avg_part_losses.mean()\n",
    "    part_weights_phase2 = normalized_losses ** 0.5\n",
    "    part_weights_phase2 = part_weights_phase2.tolist()\n",
    "    \n",
    "    print(f\"\\nüéØ Calculated Part Weights:\")\n",
    "    for i, name in enumerate(region_names):\n",
    "        print(f\"   {name}: {part_weights_phase2[i]:.3f} (loss: {avg_part_losses[i]:.4f})\")\n",
    "    \n",
    "    # Phase 2 criterion\n",
    "    criterion_phase2 = AdaptiveClassificationLoss(\n",
    "        train_labels,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_class_weights=True,\n",
    "        part_weights=part_weights_phase2,\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    )\n",
    "    \n",
    "    # Phase 1 best model Î°úÎìú\n",
    "    model.load_state_dict(torch.load(BEST_PATH)['model_state_dict'])\n",
    "    \n",
    "    # ÏÉàÎ°úÏö¥ optimizer & scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR*0.5, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-7\n",
    "    )\n",
    "    scaler = GradScaler(enabled=AMP)\n",
    "    \n",
    "    best_acc_phase2 = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Phase 2 ÌïôÏäµ\n",
    "    for ep in range(1, EPOCHS_PHASE2 + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        tr_loss, tr_acc, tr_off1, tr_mae, tr_part_losses = train_epoch(\n",
    "            model, tr_loader, criterion_phase2, optimizer, scaler, DEVICE, AMP\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_off1, val_mae, val_part_losses, val_region_acc = evaluate(\n",
    "            model, val_loader, criterion_phase2, DEVICE, split='val'\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc >= EARLY_STOP_ACC:\n",
    "            print(f\"\\n‚úÖ Target Acc {EARLY_STOP_ACC} reached at epoch {ep}!\")\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'val_mae': val_mae,\n",
    "                'part_weights': part_weights_phase2\n",
    "            }, PHASE2_PATH)\n",
    "            break\n",
    "        \n",
    "        # Save best\n",
    "        if val_acc > best_acc_phase2:\n",
    "            best_acc_phase2 = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'val_acc': best_acc_phase2,\n",
    "                'val_mae': val_mae,\n",
    "                'part_weights': part_weights_phase2\n",
    "            }, PHASE2_PATH)\n",
    "            print(f\"‚úÖ Phase 2 Best (Acc={best_acc_phase2:.4f}, MAE={val_mae:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\n‚èπÔ∏è Phase 2 Early stopping at epoch {ep}\")\n",
    "            break\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"\\n[Phase2 Epoch {ep:02d}/{EPOCHS_PHASE2}]\")\n",
    "        print(f\"  Train - acc:{tr_acc:.4f} off1:{tr_off1:.4f} mae:{tr_mae:.4f}\")\n",
    "        print(f\"  Val   - acc:{val_acc:.4f} off1:{val_off1:.4f} mae:{val_mae:.4f}\")\n",
    "        print(f\"  Part losses (Val): {val_part_losses.numpy().round(3)}\")\n",
    "        print(f\"  LR:{get_lrs(optimizer)[0]:.2e} | {elapsed:.1f}s | Pat:{patience_counter}/{max_patience}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # ========================================\n",
    "    # Test Evaluation\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ Training Finished!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(tt_loader) > 0:\n",
    "        print(\"\\nüìä Test evaluation with Phase 2 model...\")\n",
    "        model.load_state_dict(torch.load(PHASE2_PATH)['model_state_dict'])\n",
    "        tt_loss, tt_acc, tt_off1, tt_mae, tt_part_losses, tt_region_acc = evaluate(\n",
    "            model, tt_loader, criterion_phase2, DEVICE, split='test'\n",
    "        )\n",
    "        print(f\"\\nüèÜ Test Results:\")\n",
    "        print(f\"   Accuracy: {tt_acc:.4f}\")\n",
    "        print(f\"   Off-by-1: {tt_off1:.4f}\")\n",
    "        print(f\"   MAE: {tt_mae:.4f}\")\n",
    "        print(f\"   Region Acc: {tt_region_acc.numpy().round(3)}\")\n",
    "        print(f\"   Part losses: {tt_part_losses.numpy().round(3)}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Phase 1 model saved: {BEST_PATH}\")\n",
    "    print(f\"üíæ Phase 2 model saved: {PHASE2_PATH}\")\n",
    "    \n",
    "    print(\"\\nüìà Summary:\")\n",
    "    print(f\"   Phase 1 Best Acc: {best_acc_phase1:.4f}\")\n",
    "    print(f\"   Phase 2 Best Acc: {best_acc_phase2:.4f}\")\n",
    "    print(f\"   Improvement: {(best_acc_phase2 - best_acc_phase1):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69a9ce3-a38a-4809-8a34-e0102903f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ Brixia COVID-19 Classification Training\n",
      "   ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ (ÏûêÎèô Í∞ÄÏ§ëÏπò)\n",
      "   ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ (Self-Attention)\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: 4695Í∞ú\n",
      "Train: 3637, Val: 912, Test: 146\n",
      "Train - Mean: 8.31, Std: 4.26\n",
      "Val - Mean: 8.35, Std: 4.15\n",
      "Test - Mean: 7.78, Std: 4.20\n",
      "\n",
      "üì¶ Creating DataLoaders...\n",
      "‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\n",
      "   Train: 3637 samples, 113 batches\n",
      "   Val:   912 samples, 29 batches\n",
      "   Test:  146 samples, 5 batches\n",
      "\n",
      "======================================================================\n",
      "üìç Training Setup\n",
      "======================================================================\n",
      "============================================================\n",
      "Class Distribution Analysis\n",
      "============================================================\n",
      "\n",
      "A:\n",
      "  Class 0: 1810 ( 50.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1126 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  446 ( 12.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  234 (  6.5%) ‚ñà‚ñà‚ñà\n",
      "\n",
      "B:\n",
      "  Class 0:  721 ( 19.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  949 ( 26.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1171 ( 32.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  775 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "C:\n",
      "  Class 0:  406 ( 11.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  783 ( 21.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1300 ( 36.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1127 ( 31.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "D:\n",
      "  Class 0: 1804 ( 49.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1126 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  476 ( 13.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  210 (  5.8%) ‚ñà‚ñà\n",
      "\n",
      "E:\n",
      "  Class 0:  762 ( 21.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  963 ( 26.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1118 ( 30.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  773 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "F:\n",
      "  Class 0:  407 ( 11.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  795 ( 22.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1306 ( 36.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1108 ( 30.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "‚úÖ Class weights: [0.9512662  0.96508205 0.9588403  1.1248114 ]\n",
      "‚úÖ Part weights: [1. 1. 1. 1. 1. 1.]\n",
      "   Parameters: 25,935,764\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training Start\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.1561 acc:0.4856 off1:0.8688 mae:0.6603\n",
      "  Region Acc: [0.542 0.44  0.405 0.617 0.467 0.443]\n",
      "‚úÖ New Best! (Acc=0.4856, MAE=0.6603)\n",
      "\n",
      "[Epoch 001/100]\n",
      "  Train - loss:1.2178 acc:0.4151 off1:0.8294 mae:0.7886\n",
      "  Val   - loss:1.1561 acc:0.4856 off1:0.8688 mae:0.6603\n",
      "  Part losses (Val): [1.141 1.203 1.239 0.984 1.166 1.203]\n",
      "  Region Acc (Val): [0.542 0.44  0.405 0.617 0.467 0.443]\n",
      "  LR:1.00e-04 | 15.4s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.1078 acc:0.5143 off1:0.9123 mae:0.5780\n",
      "  Region Acc: [0.546 0.489 0.463 0.612 0.505 0.47 ]\n",
      "‚úÖ New Best! (Acc=0.5143, MAE=0.5780)\n",
      "\n",
      "[Epoch 002/100]\n",
      "  Train - loss:1.1414 acc:0.4534 off1:0.8569 mae:0.7148\n",
      "  Val   - loss:1.1078 acc:0.5143 off1:0.9123 mae:0.5780\n",
      "  Part losses (Val): [1.073 1.14  1.175 0.969 1.119 1.171]\n",
      "  Region Acc (Val): [0.546 0.489 0.463 0.612 0.505 0.47 ]\n",
      "  LR:1.00e-04 | 14.9s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0821 acc:0.5521 off1:0.9240 mae:0.5307\n",
      "  Region Acc: [0.582 0.518 0.514 0.654 0.534 0.511]\n",
      "‚úÖ New Best! (Acc=0.5521, MAE=0.5307)\n",
      "\n",
      "[Epoch 003/100]\n",
      "  Train - loss:1.1075 acc:0.4791 off1:0.8797 mae:0.6592\n",
      "  Val   - loss:1.0821 acc:0.5521 off1:0.9240 mae:0.5307\n",
      "  Part losses (Val): [1.057 1.125 1.132 0.941 1.096 1.141]\n",
      "  Region Acc (Val): [0.582 0.518 0.514 0.654 0.534 0.511]\n",
      "  LR:1.00e-04 | 15.0s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0551 acc:0.5523 off1:0.9390 mae:0.5143\n",
      "  Region Acc: [0.557 0.541 0.534 0.621 0.545 0.516]\n",
      "‚úÖ New Best! (Acc=0.5523, MAE=0.5143)\n",
      "\n",
      "[Epoch 004/100]\n",
      "  Train - loss:1.0675 acc:0.4775 off1:0.8786 mae:0.6667\n",
      "  Val   - loss:1.0551 acc:0.5523 off1:0.9390 mae:0.5143\n",
      "  Part losses (Val): [1.04  1.072 1.093 0.944 1.059 1.122]\n",
      "  Region Acc (Val): [0.557 0.541 0.534 0.621 0.545 0.516]\n",
      "  LR:1.00e-04 | 15.4s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0389 acc:0.5698 off1:0.9527 mae:0.4790\n",
      "  Region Acc: [0.601 0.554 0.523 0.656 0.569 0.516]\n",
      "‚úÖ New Best! (Acc=0.5698, MAE=0.4790)\n",
      "\n",
      "[Epoch 005/100]\n",
      "  Train - loss:1.0458 acc:0.4950 off1:0.8903 mae:0.6343\n",
      "  Val   - loss:1.0389 acc:0.5698 off1:0.9527 mae:0.4790\n",
      "  Part losses (Val): [1.004 1.059 1.1   0.91  1.044 1.117]\n",
      "  Region Acc (Val): [0.601 0.554 0.523 0.656 0.569 0.516]\n",
      "  LR:1.00e-04 | 14.7s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0407 acc:0.5720 off1:0.9432 mae:0.4889\n",
      "  Region Acc: [0.599 0.558 0.539 0.661 0.575 0.5  ]\n",
      "‚úÖ New Best! (Acc=0.5720, MAE=0.4889)\n",
      "\n",
      "[Epoch 006/100]\n",
      "  Train - loss:1.0339 acc:0.4810 off1:0.8760 mae:0.6656\n",
      "  Val   - loss:1.0407 acc:0.5720 off1:0.9432 mae:0.4889\n",
      "  Part losses (Val): [1.03  1.044 1.077 0.898 1.063 1.132]\n",
      "  Region Acc (Val): [0.599 0.558 0.539 0.661 0.575 0.5  ]\n",
      "  LR:1.00e-04 | 15.1s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0404 acc:0.5623 off1:0.9507 mae:0.4905\n",
      "  Region Acc: [0.59  0.543 0.53  0.664 0.559 0.488]\n",
      "\n",
      "[Epoch 007/100]\n",
      "  Train - loss:1.0283 acc:0.5035 off1:0.8903 mae:0.6269\n",
      "  Val   - loss:1.0404 acc:0.5623 off1:0.9507 mae:0.4905\n",
      "  Part losses (Val): [1.001 1.062 1.099 0.896 1.053 1.132]\n",
      "  Region Acc (Val): [0.59  0.543 0.53  0.664 0.559 0.488]\n",
      "  LR:1.00e-04 | 14.2s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0244 acc:0.5788 off1:0.9512 mae:0.4739\n",
      "  Region Acc: [0.61  0.547 0.542 0.677 0.598 0.5  ]\n",
      "‚úÖ New Best! (Acc=0.5788, MAE=0.4739)\n",
      "\n",
      "[Epoch 008/100]\n",
      "  Train - loss:1.0099 acc:0.5132 off1:0.9006 mae:0.6046\n",
      "  Val   - loss:1.0244 acc:0.5788 off1:0.9512 mae:0.4739\n",
      "  Part losses (Val): [1.011 1.056 1.066 0.884 1.018 1.111]\n",
      "  Region Acc (Val): [0.61  0.547 0.542 0.677 0.598 0.5  ]\n",
      "  LR:1.00e-04 | 14.9s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0607 acc:0.5667 off1:0.9388 mae:0.4996\n",
      "  Region Acc: [0.609 0.545 0.527 0.671 0.557 0.491]\n",
      "\n",
      "[Epoch 009/100]\n",
      "  Train - loss:1.0068 acc:0.5140 off1:0.8972 mae:0.6103\n",
      "  Val   - loss:1.0607 acc:0.5667 off1:0.9388 mae:0.4996\n",
      "  Part losses (Val): [1.01  1.117 1.112 0.904 1.088 1.133]\n",
      "  Region Acc (Val): [0.609 0.545 0.527 0.671 0.557 0.491]\n",
      "  LR:1.00e-04 | 13.4s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0276 acc:0.5757 off1:0.9512 mae:0.4762\n",
      "  Region Acc: [0.605 0.539 0.566 0.651 0.566 0.526]\n",
      "\n",
      "[Epoch 010/100]\n",
      "  Train - loss:1.0012 acc:0.5422 off1:0.9163 mae:0.5549\n",
      "  Val   - loss:1.0276 acc:0.5757 off1:0.9512 mae:0.4762\n",
      "  Part losses (Val): [0.974 1.062 1.06  0.896 1.067 1.106]\n",
      "  Region Acc (Val): [0.605 0.539 0.566 0.651 0.566 0.526]\n",
      "  LR:1.00e-04 | 14.8s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0291 acc:0.5872 off1:0.9527 mae:0.4629\n",
      "  Region Acc: [0.629 0.575 0.546 0.675 0.57  0.527]\n",
      "‚úÖ New Best! (Acc=0.5872, MAE=0.4629)\n",
      "\n",
      "[Epoch 011/100]\n",
      "  Train - loss:0.9737 acc:0.5294 off1:0.9037 mae:0.5845\n",
      "  Val   - loss:1.0291 acc:0.5872 off1:0.9527 mae:0.4629\n",
      "  Part losses (Val): [0.982 1.049 1.078 0.89  1.045 1.131]\n",
      "  Region Acc (Val): [0.629 0.575 0.546 0.675 0.57  0.527]\n",
      "  LR:1.00e-04 | 14.9s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0252 acc:0.5870 off1:0.9582 mae:0.4578\n",
      "  Region Acc: [0.617 0.589 0.55  0.685 0.57  0.51 ]\n",
      "\n",
      "[Epoch 012/100]\n",
      "  Train - loss:0.9708 acc:0.5355 off1:0.9004 mae:0.5849\n",
      "  Val   - loss:1.0252 acc:0.5870 off1:0.9582 mae:0.4578\n",
      "  Part losses (Val): [0.985 1.028 1.077 0.887 1.051 1.123]\n",
      "  Region Acc (Val): [0.617 0.589 0.55  0.685 0.57  0.51 ]\n",
      "  LR:1.00e-04 | 14.3s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0349 acc:0.5755 off1:0.9631 mae:0.4635\n",
      "  Region Acc: [0.603 0.544 0.553 0.677 0.562 0.514]\n",
      "\n",
      "[Epoch 013/100]\n",
      "  Train - loss:0.9796 acc:0.5336 off1:0.9043 mae:0.5797\n",
      "  Val   - loss:1.0349 acc:0.5755 off1:0.9631 mae:0.4635\n",
      "  Part losses (Val): [1.003 1.052 1.092 0.889 1.043 1.13 ]\n",
      "  Region Acc (Val): [0.603 0.544 0.553 0.677 0.562 0.514]\n",
      "  LR:1.00e-04 | 14.6s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0449 acc:0.5810 off1:0.9518 mae:0.4702\n",
      "  Region Acc: [0.601 0.571 0.553 0.68  0.568 0.513]\n",
      "\n",
      "[Epoch 014/100]\n",
      "  Train - loss:0.9560 acc:0.5643 off1:0.9207 mae:0.5293\n",
      "  Val   - loss:1.0449 acc:0.5810 off1:0.9518 mae:0.4702\n",
      "  Part losses (Val): [1.045 1.043 1.076 0.918 1.047 1.141]\n",
      "  Region Acc (Val): [0.601 0.571 0.553 0.68  0.568 0.513]\n",
      "  LR:1.00e-04 | 14.7s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0410 acc:0.5789 off1:0.9594 mae:0.4636\n",
      "  Region Acc: [0.606 0.561 0.55  0.663 0.569 0.523]\n",
      "\n",
      "[Epoch 015/100]\n",
      "  Train - loss:0.9371 acc:0.5409 off1:0.9002 mae:0.5806\n",
      "  Val   - loss:1.0410 acc:0.5789 off1:0.9594 mae:0.4636\n",
      "  Part losses (Val): [1.011 1.064 1.088 0.895 1.052 1.136]\n",
      "  Region Acc (Val): [0.606 0.561 0.55  0.663 0.569 0.523]\n",
      "  LR:1.00e-04 | 14.1s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0541 acc:0.5789 off1:0.9492 mae:0.4753\n",
      "  Region Acc: [0.623 0.555 0.541 0.691 0.561 0.503]\n",
      "\n",
      "[Epoch 016/100]\n",
      "  Train - loss:0.9381 acc:0.5517 off1:0.9065 mae:0.5589\n",
      "  Val   - loss:1.0541 acc:0.5789 off1:0.9492 mae:0.4753\n",
      "  Part losses (Val): [1.016 1.069 1.103 0.914 1.064 1.16 ]\n",
      "  Region Acc (Val): [0.623 0.555 0.541 0.691 0.561 0.503]\n",
      "  LR:1.00e-04 | 13.7s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0446 acc:0.5835 off1:0.9538 mae:0.4651\n",
      "  Region Acc: [0.606 0.591 0.537 0.681 0.567 0.519]\n",
      "\n",
      "[Epoch 017/100]\n",
      "  Train - loss:0.9321 acc:0.5712 off1:0.9179 mae:0.5278\n",
      "  Val   - loss:1.0446 acc:0.5835 off1:0.9538 mae:0.4651\n",
      "  Part losses (Val): [1.037 1.047 1.093 0.907 1.042 1.142]\n",
      "  Region Acc (Val): [0.606 0.591 0.537 0.681 0.567 0.519]\n",
      "  LR:5.00e-05 | 14.8s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0328 acc:0.5859 off1:0.9552 mae:0.4618\n",
      "  Region Acc: [0.612 0.573 0.564 0.667 0.57  0.53 ]\n",
      "\n",
      "[Epoch 018/100]\n",
      "  Train - loss:0.9015 acc:0.5732 off1:0.9140 mae:0.5310\n",
      "  Val   - loss:1.0328 acc:0.5859 off1:0.9552 mae:0.4618\n",
      "  Part losses (Val): [0.988 1.04  1.066 0.905 1.048 1.15 ]\n",
      "  Region Acc (Val): [0.612 0.573 0.564 0.667 0.57  0.53 ]\n",
      "  LR:5.00e-05 | 14.0s | Patience:7/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0522 acc:0.5870 off1:0.9556 mae:0.4607\n",
      "  Region Acc: [0.609 0.582 0.561 0.678 0.58  0.512]\n",
      "\n",
      "[Epoch 019/100]\n",
      "  Train - loss:0.8709 acc:0.5857 off1:0.9109 mae:0.5229\n",
      "  Val   - loss:1.0522 acc:0.5870 off1:0.9556 mae:0.4607\n",
      "  Part losses (Val): [1.015 1.057 1.089 0.932 1.049 1.17 ]\n",
      "  Region Acc (Val): [0.609 0.582 0.561 0.678 0.58  0.512]\n",
      "  LR:5.00e-05 | 15.0s | Patience:8/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0585 acc:0.5866 off1:0.9576 mae:0.4585\n",
      "  Region Acc: [0.607 0.567 0.568 0.673 0.572 0.532]\n",
      "\n",
      "[Epoch 020/100]\n",
      "  Train - loss:0.8829 acc:0.5706 off1:0.9075 mae:0.5420\n",
      "  Val   - loss:1.0585 acc:0.5866 off1:0.9576 mae:0.4585\n",
      "  Part losses (Val): [1.037 1.068 1.08  0.951 1.069 1.147]\n",
      "  Region Acc (Val): [0.607 0.567 0.568 0.673 0.572 0.532]\n",
      "  LR:5.00e-05 | 14.1s | Patience:9/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0761 acc:0.5850 off1:0.9552 mae:0.4625\n",
      "  Region Acc: [0.62  0.572 0.55  0.672 0.578 0.518]\n",
      "\n",
      "[Epoch 021/100]\n",
      "  Train - loss:0.8491 acc:0.6151 off1:0.9310 mae:0.4679\n",
      "  Val   - loss:1.0761 acc:0.5850 off1:0.9552 mae:0.4625\n",
      "  Part losses (Val): [1.066 1.064 1.118 0.948 1.081 1.179]\n",
      "  Region Acc (Val): [0.62  0.572 0.55  0.672 0.578 0.518]\n",
      "  LR:5.00e-05 | 14.9s | Patience:10/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0741 acc:0.5779 off1:0.9554 mae:0.4695\n",
      "  Region Acc: [0.602 0.57  0.556 0.657 0.555 0.527]\n",
      "\n",
      "[Epoch 022/100]\n",
      "  Train - loss:0.8681 acc:0.5972 off1:0.9173 mae:0.5017\n",
      "  Val   - loss:1.0741 acc:0.5779 off1:0.9554 mae:0.4695\n",
      "  Part losses (Val): [1.043 1.08  1.112 0.949 1.085 1.174]\n",
      "  Region Acc (Val): [0.602 0.57  0.556 0.657 0.555 0.527]\n",
      "  LR:5.00e-05 | 14.8s | Patience:11/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0966 acc:0.5830 off1:0.9545 mae:0.4660\n",
      "  Region Acc: [0.615 0.56  0.559 0.679 0.573 0.511]\n",
      "\n",
      "[Epoch 023/100]\n",
      "  Train - loss:0.8471 acc:0.5946 off1:0.9120 mae:0.5131\n",
      "  Val   - loss:1.0966 acc:0.5830 off1:0.9545 mae:0.4660\n",
      "  Part losses (Val): [1.094 1.089 1.113 0.989 1.104 1.191]\n",
      "  Region Acc (Val): [0.615 0.56  0.559 0.679 0.573 0.511]\n",
      "  LR:2.50e-05 | 13.9s | Patience:12/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0951 acc:0.5808 off1:0.9547 mae:0.4678\n",
      "  Region Acc: [0.616 0.566 0.565 0.664 0.565 0.509]\n",
      "\n",
      "[Epoch 024/100]\n",
      "  Train - loss:0.8297 acc:0.6269 off1:0.9249 mae:0.4644\n",
      "  Val   - loss:1.0951 acc:0.5808 off1:0.9547 mae:0.4678\n",
      "  Part losses (Val): [1.074 1.094 1.131 0.968 1.106 1.198]\n",
      "  Region Acc (Val): [0.616 0.566 0.565 0.664 0.565 0.509]\n",
      "  LR:2.50e-05 | 15.0s | Patience:13/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0956 acc:0.5810 off1:0.9576 mae:0.4653\n",
      "  Region Acc: [0.606 0.562 0.557 0.673 0.575 0.512]\n",
      "\n",
      "[Epoch 025/100]\n",
      "  Train - loss:0.7912 acc:0.6228 off1:0.9186 mae:0.4758\n",
      "  Val   - loss:1.0956 acc:0.5810 off1:0.9576 mae:0.4653\n",
      "  Part losses (Val): [1.064 1.108 1.137 0.971 1.089 1.205]\n",
      "  Region Acc (Val): [0.606 0.562 0.557 0.673 0.575 0.512]\n",
      "  LR:2.50e-05 | 14.4s | Patience:14/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:1.0993 acc:0.5833 off1:0.9543 mae:0.4656\n",
      "  Region Acc: [0.621 0.562 0.557 0.673 0.562 0.524]\n",
      "\n",
      "‚èπÔ∏è Early stopping at epoch 26\n",
      "\n",
      "======================================================================\n",
      "üéâ Training Finished!\n",
      "======================================================================\n",
      "\n",
      "üìä Test evaluation with best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] loss:1.0082 acc:0.5868 off1:0.9646 mae:0.4521\n",
      "  Region Acc: [0.616 0.521 0.596 0.671 0.555 0.562]\n",
      "\n",
      "üèÜ Test Results:\n",
      "   Accuracy: 0.5868\n",
      "   Off-by-1: 0.9646\n",
      "   MAE: 0.4521\n",
      "   Region Acc: [0.616 0.521 0.596 0.671 0.555 0.562]\n",
      "   Part losses: [0.937 1.083 1.057 0.859 1.013 1.1  ]\n",
      "\n",
      "üíæ Best model saved: runs_severity_classification/best_efficientnet_b0_classification.pth\n",
      "üìà Best Validation Accuracy: 0.5872\n",
      "üìâ Best Validation MAE: 0.4629\n",
      "\n",
      "‚úÖ Ïù¥Ï†ú gradcam_inference.pyÎ•º Ïã§ÌñâÌïòÏó¨ Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÏÑ∏Ïöî!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- Í≤ΩÎ°ú ÏÑ§Ï†ï Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ---\n",
    "BASE_DIR     = f\"./data/covid19-xray-severity-scoring/\"\n",
    "CSV_PATH     = str(Path(BASE_DIR) / \"Brixia.csv\")\n",
    "IMAGE_DIR    = str(Path(BASE_DIR) / \"segmented_png\")\n",
    "\n",
    "OUT_DIR      = \"./runs_severity_classification\"\n",
    "BEST_PATH    = str(Path(OUT_DIR) / \"best_efficientnet_b0_classification.pth\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED         = 42\n",
    "IMG_SIZE     = 224\n",
    "BATCH_SIZE   = 32\n",
    "NUM_CLASSES  = 4   # 0, 1, 2, 3\n",
    "EPOCHS       = 100  # Single phase training\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "AMP          = True\n",
    "EARLY_STOP_ACC = 0.75  # üîÑ MAE ‚Üí Accuracy\n",
    "DROP_RATIO   = 0.3\n",
    "AUG_RATIO    = 0.5\n",
    "MIXUP_ALPHA  = 0.2\n",
    "LABEL_SMOOTHING = 0.1  # ‚úÖ NEW: Label smoothing\n",
    "\n",
    "# --- ÏãúÎìú Í≥†Ï†ï ---\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "def make_transform_with_label(train: bool, img_size: int = IMG_SIZE, aug_ratio=AUG_RATIO):\n",
    "    \"\"\"Brixia ScoreÏùò Ï¢åÏö∞ Íµ¨Ï°∞Î•º Í≥†Î†§Ìïú transform\"\"\"\n",
    "    def _tfm(img: Image.Image, label: torch.Tensor = None):\n",
    "        img = img.convert('RGB')\n",
    "        img = TF.resize(\n",
    "            img, \n",
    "            [img_size, img_size], \n",
    "            interpolation=TF.InterpolationMode.BILINEAR,\n",
    "            antialias=True\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            # 1. Horizontal Flip (Ï¢åÏö∞ Î∞òÏ†Ñ: ABC ‚Üî DEF)\n",
    "            if random.random() < aug_ratio:\n",
    "                img = TF.hflip(img)\n",
    "                if label is not None:\n",
    "                    # [A, B, C, D, E, F] ‚Üí [D, E, F, A, B, C]\n",
    "                    label = label[[3, 4, 5, 0, 1, 2]]\n",
    "            \n",
    "            # 2. ÏïΩÌïú ÌöåÏ†Ñ (¬±5ÎèÑ)\n",
    "            if random.random() < aug_ratio:\n",
    "                angle = float(torch.empty(1).uniform_(-5, 5))\n",
    "                img = TF.rotate(\n",
    "                    img, \n",
    "                    angle, \n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 3. ÏïΩÌïú Translation\n",
    "            if random.random() < aug_ratio:\n",
    "                max_dx = 0.05 * img_size\n",
    "                max_dy = 0.05 * img_size\n",
    "                translations = (\n",
    "                    float(torch.empty(1).uniform_(-max_dx, max_dx)),\n",
    "                    float(torch.empty(1).uniform_(-max_dy, max_dy))\n",
    "                )\n",
    "                img = TF.affine(\n",
    "                    img,\n",
    "                    angle=0,\n",
    "                    translate=translations,\n",
    "                    scale=1.0,\n",
    "                    shear=0,\n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 4. Brightness & Contrast\n",
    "            if random.random() < aug_ratio:\n",
    "                brightness_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_brightness(img, brightness_factor)\n",
    "            \n",
    "            if random.random() < aug_ratio:\n",
    "                contrast_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_contrast(img, contrast_factor)\n",
    "            \n",
    "            # 5. Gamma Correction\n",
    "            if random.random() < 0.3:\n",
    "                gamma = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_gamma(img, gamma)\n",
    "        \n",
    "        # Tensor Î≥ÄÌôò\n",
    "        img = TF.to_tensor(img)\n",
    "        \n",
    "        # Gaussian Noise (train only)\n",
    "        if train and random.random() < 0.2:\n",
    "            noise = torch.randn_like(img) * 0.01\n",
    "            img = img + noise\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôî\n",
    "        img = TF.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        if label is not None:\n",
    "            return img, label\n",
    "        return img\n",
    "    \n",
    "    return _tfm\n",
    "\n",
    "def load_and_split_brixia(csv_path, val_ratio=0.2, seed=SEED):\n",
    "    df = pd.read_csv(csv_path, dtype={'BrixiaScore': str})\n",
    "    df = df.dropna(subset=['BrixiaScore'])\n",
    "    df = df[df['BrixiaScore'] != 'nan']\n",
    "    df = df[df['BrixiaScore'].str.len() == 6].copy()\n",
    "    \n",
    "    print(f\"Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: {len(df)}Í∞ú\")\n",
    "    \n",
    "    if 'ConsensusTestset' in df.columns:\n",
    "        test_df = df[df['ConsensusTestset'] == 1].copy()\n",
    "        train_val_df = df[df['ConsensusTestset'] == 0].copy()\n",
    "    else:\n",
    "        test_df = pd.DataFrame()\n",
    "        train_val_df = df.copy()\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
    "    train_idx, val_idx = next(gss.split(\n",
    "        train_val_df, \n",
    "        groups=train_val_df['StudyId']\n",
    "    ))\n",
    "    \n",
    "    tr_df = train_val_df.iloc[train_idx].copy()\n",
    "    val_df = train_val_df.iloc[val_idx].copy()\n",
    "    \n",
    "    print(f\"Train: {len(tr_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    validate_split(tr_df, val_df, test_df)\n",
    "    \n",
    "    return tr_df, val_df, test_df\n",
    "\n",
    "def validate_split(tr_df, val_df, tt_df):\n",
    "    train_studies = set(tr_df['StudyId'])\n",
    "    val_studies = set(val_df['StudyId'])\n",
    "    test_studies = set(tt_df['StudyId']) if len(tt_df) > 0 else set()\n",
    "    \n",
    "    assert len(train_studies & val_studies) == 0, \"Train-Val Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(train_studies & test_studies) == 0, \"Train-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(val_studies & test_studies) == 0, \"Val-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    \n",
    "    for name, data in [('Train', tr_df), ('Val', val_df), ('Test', tt_df)]:\n",
    "        if len(data) > 0:\n",
    "            scores = data['BrixiaScore'].apply(lambda x: sum(int(c) for c in x))\n",
    "            print(f\"{name} - Mean: {scores.mean():.2f}, Std: {scores.std():.2f}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class BrixiaDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_col = \"Filename\"\n",
    "        self.label_col = \"BrixiaScore\"\n",
    "        self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        assert self.img_col in self.df.columns\n",
    "        assert self.label_col in self.df.columns\n",
    "        \n",
    "        invalid_scores = self.df[self.df[self.label_col].str.len() != 6]\n",
    "        if len(invalid_scores) > 0:\n",
    "            print(f\"‚ö†Ô∏è Í≤ΩÍ≥†: {len(invalid_scores)}Í∞úÏùò ÏûòÎ™ªÎêú BrixiaScore Î∞úÍ≤¨\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_name_from_csv = row[self.img_col]\n",
    "        img_name = img_name_from_csv.replace('.dcm', '.png')\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïò§Î•ò: {img_path}\")\n",
    "            raise\n",
    "        \n",
    "        scores_str = row[self.label_col]\n",
    "        scores_list = [int(c) for c in scores_str]\n",
    "        labels = torch.tensor(scores_list, dtype=torch.long)  # üîÑ longÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "        \n",
    "        if self.transform:\n",
    "            image, labels = self.transform(image, labels)\n",
    "        else:\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "def create_dataloaders(tr_df, val_df, tt_df, img_dir, \n",
    "                       batch_size=32, img_size=224, num_workers=4):\n",
    "    train_transform = make_transform_with_label(train=True, img_size=img_size)\n",
    "    val_transform = make_transform_with_label(train=False, img_size=img_size)\n",
    "    \n",
    "    tr_ds = BrixiaDataset(tr_df, img_dir, transform=train_transform)\n",
    "    val_ds = BrixiaDataset(val_df, img_dir, transform=val_transform)\n",
    "    tt_ds = BrixiaDataset(tt_df, img_dir, transform=val_transform)\n",
    "    \n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    tt_loader = DataLoader(\n",
    "        tt_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "    print(f\"   Train: {len(tr_ds)} samples, {len(tr_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_ds)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(tt_ds)} samples, {len(tt_loader)} batches\")\n",
    "    \n",
    "    return tr_loader, val_loader, tt_loader\n",
    "\n",
    "# ============================================================\n",
    "# Loss Function (Î∂ÑÎ•òÏö©ÏúºÎ°ú Î≥ÄÍ≤Ω)\n",
    "# ============================================================\n",
    "def calculate_class_weights(labels, num_classes=4, method='sqrt_inverse'):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    counts = np.bincount(labels_flat.astype(int), minlength=num_classes)\n",
    "    \n",
    "    if method == 'sqrt_inverse':\n",
    "        weights = 1.0 / (np.sqrt(counts) + 1e-6)\n",
    "    elif method == 'inverse':\n",
    "        weights = 1.0 / (counts + 1e-6)\n",
    "    else:\n",
    "        total = len(labels_flat)\n",
    "        weights = total / (num_classes * (counts + 1e-6))\n",
    "    \n",
    "    weights = weights / weights.mean()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def print_class_distribution(labels):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Class Distribution Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    region_names = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    for idx, name in enumerate(region_names):\n",
    "        region_labels = labels[:, idx]\n",
    "        counts = np.bincount(region_labels.astype(int), minlength=4)\n",
    "        total = counts.sum()\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        for cls in range(4):\n",
    "            pct = 100 * counts[cls] / total if total > 0 else 0\n",
    "            bar = '‚ñà' * int(pct / 2)\n",
    "            print(f\"  Class {cls}: {counts[cls]:4d} ({pct:5.1f}%) {bar}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "class AdaptiveClassificationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Îã§Ï§ë ÌÅ¥ÎûòÏä§ Î∂ÑÎ•òÎ•º ÏúÑÌïú ÏÜêÏã§Ìï®Ïàò\n",
    "    - CrossEntropyLoss Í∏∞Î∞ò\n",
    "    - ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "    - Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "    - Label Smoothing ÏßÄÏõê\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_labels, num_classes=4, use_class_weights=True, \n",
    "                 part_weights=None, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        if isinstance(train_labels, torch.Tensor):\n",
    "            train_labels_np = train_labels.cpu().numpy()\n",
    "        else:\n",
    "            train_labels_np = train_labels\n",
    "        \n",
    "        print_class_distribution(train_labels_np)\n",
    "        \n",
    "        # ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "        if use_class_weights:\n",
    "            class_weights = calculate_class_weights(train_labels_np, num_classes=num_classes)\n",
    "            self.register_buffer('class_weights', class_weights)\n",
    "            print(f\"‚úÖ Class weights: {class_weights.numpy()}\")\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò\n",
    "        if part_weights is None:\n",
    "            self.part_weights = torch.ones(6)\n",
    "        else:\n",
    "            self.part_weights = torch.tensor(part_weights, dtype=torch.float32)\n",
    "        self.register_buffer('part_weights_buf', self.part_weights)\n",
    "        print(f\"‚úÖ Part weights: {self.part_weights.numpy()}\")\n",
    "\n",
    "    def forward(self, pred_logits, target, use_mixup=False):\n",
    "        \"\"\"\n",
    "        pred_logits: [B, 6, 4] - Í∞Å Î∂ÄÏúÑÎ≥Ñ 4Í∞ú ÌÅ¥ÎûòÏä§Ïóê ÎåÄÌïú logits\n",
    "        target: [B, 6] - Í∞Å Î∂ÄÏúÑÎ≥Ñ ÌÅ¥ÎûòÏä§ Î†àÏù¥Î∏î (0~3)\n",
    "        \"\"\"\n",
    "        B, num_regions, num_classes = pred_logits.shape\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        pred_logits_flat = pred_logits.view(B * num_regions, num_classes)  # [B*6, 4]\n",
    "        target_flat = target.view(B * num_regions)  # [B*6]\n",
    "        \n",
    "        # CrossEntropyLoss with label smoothing\n",
    "        if self.class_weights is not None and not use_mixup:\n",
    "            criterion = nn.CrossEntropyLoss(\n",
    "                weight=self.class_weights.to(pred_logits.device),\n",
    "                label_smoothing=self.label_smoothing,\n",
    "                reduction='none'\n",
    "            )\n",
    "        else:\n",
    "            criterion = nn.CrossEntropyLoss(\n",
    "                label_smoothing=self.label_smoothing,\n",
    "                reduction='none'\n",
    "            )\n",
    "        \n",
    "        loss = criterion(pred_logits_flat, target_flat)  # [B*6]\n",
    "        loss = loss.view(B, num_regions)  # [B, 6]\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "        part_weights = self.part_weights_buf.to(pred_logits.device)\n",
    "        loss = loss * part_weights.unsqueeze(0)  # [B, 6]\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ ÌèâÍ∑† ÏÜêÏã§ (Î™®ÎãàÌÑ∞ÎßÅÏö©)\n",
    "        part_losses = loss.mean(dim=0)  # [6]\n",
    "        \n",
    "        return loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Mixup (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "def mixup_data_classification(x, y, alpha=MIXUP_ALPHA):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•òÏö© Mixup\n",
    "    yÎäî one-hotÏúºÎ°ú Î≥ÄÌôò ÌõÑ mixup\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    \n",
    "    # One-hot encoding\n",
    "    y_onehot = F.one_hot(y, num_classes=NUM_CLASSES).float()  # [B, 6, 4]\n",
    "    y_onehot_shuffled = y_onehot[index]\n",
    "    \n",
    "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot_shuffled  # [B, 6, 4]\n",
    "    \n",
    "    return mixed_x, mixed_y, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred_logits, y_mixed, lam):\n",
    "    \"\"\"MixupÏùÑ ÏúÑÌïú ÏÜêÏã§ Í≥ÑÏÇ∞\"\"\"\n",
    "    # y_mixedÎäî [B, 6, 4] soft labels\n",
    "    # pred_logitsÎäî [B, 6, 4]\n",
    "    \n",
    "    B, num_regions, num_classes = pred_logits.shape\n",
    "    \n",
    "    # Reshape\n",
    "    pred_flat = pred_logits.view(B * num_regions, num_classes)  # [B*6, 4]\n",
    "    target_flat = y_mixed.view(B * num_regions, num_classes)  # [B*6, 4]\n",
    "    \n",
    "    # Soft target loss\n",
    "    log_probs = F.log_softmax(pred_flat, dim=1)\n",
    "    loss = -(target_flat * log_probs).sum(dim=1)  # [B*6]\n",
    "    loss = loss.view(B, num_regions)  # [B, 6]\n",
    "    \n",
    "    # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö© (criterionÏóêÏÑú Í∞ÄÏ†∏Ïò¥)\n",
    "    if hasattr(criterion, 'part_weights_buf'):\n",
    "        part_weights = criterion.part_weights_buf.to(pred_logits.device)\n",
    "        loss = loss * part_weights.unsqueeze(0)\n",
    "    \n",
    "    part_losses = loss.mean(dim=0)\n",
    "    return loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Metrics (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def calculate_classification_metrics(pred_logits, labels):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•ò ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    pred_logits: [B, 6, 4]\n",
    "    labels: [B, 6]\n",
    "    \"\"\"\n",
    "    # ÏòàÏ∏° ÌÅ¥ÎûòÏä§\n",
    "    preds = pred_logits.argmax(dim=-1)  # [B, 6]\n",
    "    \n",
    "    # Exact match accuracy\n",
    "    exact_acc = (preds == labels).float().mean().item()\n",
    "    \n",
    "    # Off-by-1 accuracy (Ïù∏Ï†ë ÌÅ¥ÎûòÏä§ ÌóàÏö©)\n",
    "    off_by_1 = (torch.abs(preds - labels) <= 1).float().mean().item()\n",
    "    \n",
    "    # Per-region accuracy\n",
    "    region_acc = (preds == labels).float().mean(dim=0)  # [6]\n",
    "    \n",
    "    # MAE (Ï∞∏Í≥†Ïö©)\n",
    "    mae = torch.abs(preds.float() - labels.float()).mean().item()\n",
    "    \n",
    "    return exact_acc, off_by_1, mae, region_acc\n",
    "\n",
    "# ============================================================\n",
    "# Model (Î∂ÑÎ•òÏö©ÏúºÎ°ú Î≥ÄÍ≤Ω)\n",
    "# ============================================================\n",
    "class EfficientNetB0Classification(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ Î∂ÑÎ•òÎ•º ÏúÑÌïú Î™®Îç∏ (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Í≥†Î†§)\n",
    "    Í∞Å Î∂ÄÏúÑ(A~F)ÎßàÎã§ 4Í∞ú ÌÅ¥ÎûòÏä§(0~3) ÏòàÏ∏°\n",
    "    + Self-AttentionÏúºÎ°ú Î∂ÄÏúÑ Í∞Ñ Í¥ÄÍ≥Ñ ÌïôÏäµ\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True, drop=0.3, num_regions=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = efficientnet_b0(weights=weights)\n",
    "        \n",
    "        self.features = backbone.features\n",
    "        in_feat = 1280\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 49, in_feat))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Region queries\n",
    "        self.region_queries = nn.Parameter(torch.randn(num_regions, in_feat))\n",
    "        nn.init.xavier_uniform_(self.region_queries)\n",
    "        \n",
    "        # Cross attention (Ïù¥ÎØ∏ÏßÄ ÌäπÏßï ‚Üí Î∂ÄÏúÑÎ≥Ñ ÌäπÏßï)\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # ‚úÖ NEW: Self-Attention (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ)\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(in_feat)\n",
    "        self.norm3 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_feat * 2, in_feat),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        \n",
    "        self.norm4 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # Classification heads: Í∞Å Î∂ÄÏúÑÎßàÎã§ 4Í∞ú ÌÅ¥ÎûòÏä§ ÏòàÏ∏°\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feat, 256),\n",
    "                nn.LayerNorm(256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(128, num_classes)\n",
    "            ) for _ in range(num_regions)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # Feature extraction\n",
    "        feat = self.features(x)  # [B, 1280, 7, 7]\n",
    "        feat = feat.flatten(2).transpose(1, 2)  # [B, 49, 1280]\n",
    "        feat = feat + self.pos_embed\n",
    "        \n",
    "        # Region queries\n",
    "        queries = self.region_queries.unsqueeze(0).expand(B, -1, -1)  # [B, 6, 1280]\n",
    "        \n",
    "        # Cross attention (Ïù¥ÎØ∏ÏßÄ ‚Üí Î∂ÄÏúÑ)\n",
    "        attn_out, _ = self.cross_attention(\n",
    "            query=queries,\n",
    "            key=feat,\n",
    "            value=feat\n",
    "        )\n",
    "        attn_out = self.norm1(attn_out + queries)  # [B, 6, 1280]\n",
    "        \n",
    "        # ‚úÖ Self-Attention (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ)\n",
    "        # A, B, CÍ∞Ä ÏÑúÎ°ú ÏòÅÌñ•ÏùÑ Ï£ºÍ≥†, D, E, FÎèÑ ÏÑúÎ°ú ÏòÅÌñ•\n",
    "        self_attn_out, attn_weights = self.self_attention(\n",
    "            query=attn_out,\n",
    "            key=attn_out,\n",
    "            value=attn_out\n",
    "        )\n",
    "        attn_out = self.norm2(attn_out + self_attn_out)  # [B, 6, 1280]\n",
    "        \n",
    "        # FFN\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        attn_out = self.norm3(attn_out + ffn_out)  # [B, 6, 1280]\n",
    "        \n",
    "        # Í∞Å Î∂ÄÏúÑÎ≥Ñ classification head ÌÜµÍ≥º\n",
    "        outputs = []\n",
    "        for i in range(len(self.heads)):\n",
    "            region_feat = attn_out[:, i, :]  # [B, 1280]\n",
    "            logits = self.heads[i](region_feat)  # [B, 4]\n",
    "            outputs.append(logits)\n",
    "        \n",
    "        out = torch.stack(outputs, dim=1)  # [B, 6, 4]\n",
    "        return out, attn_weights  # ‚úÖ attention weights Î∞òÌôò (Grad-CAMÏö©)\n",
    "\n",
    "# ============================================================\n",
    "# Training Functions (Î∂ÑÎ•òÏö© ÏàòÏ†ï)\n",
    "# ============================================================\n",
    "def train_epoch(model, tr_loader, criterion, optimizer, scaler, device, \n",
    "                amp=True, use_mixup=True):\n",
    "    model.train()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(tr_loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)  # [B, 6]\n",
    "        \n",
    "        # Mixup Ï†ÅÏö©\n",
    "        is_mixup = use_mixup and (random.random() < 0.5)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast(enabled=amp):\n",
    "            if is_mixup:\n",
    "                imgs_mixed, labels_mixed, lam = mixup_data_classification(imgs, labels)\n",
    "                pred_logits, _ = model(imgs_mixed)  # attention weights Î¨¥Ïãú\n",
    "                loss, part_losses = mixup_criterion(criterion, pred_logits, labels_mixed, lam)\n",
    "            else:\n",
    "                pred_logits, _ = model(imgs)  # attention weights Î¨¥Ïãú\n",
    "                loss, part_losses = criterion(pred_logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Metrics (ÏõêÎ≥∏ labelsÎ°ú Í≥ÑÏÇ∞)\n",
    "        exact_acc, off_by_1, mae, _ = calculate_classification_metrics(pred_logits.detach(), labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\",\n",
    "            mae=f\"{run_mae/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    \n",
    "    return run_loss/n, run_acc/n, run_off1/n, run_mae/n, part_losses_avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, split='val'):\n",
    "    model.eval()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    region_acc_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"{split.capitalize()}\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        pred_logits, _ = model(imgs)  # [B, 6, 4], attention weights Î¨¥Ïãú\n",
    "        loss, part_losses = criterion(pred_logits, labels)\n",
    "        \n",
    "        exact_acc, off_by_1, mae, region_acc = calculate_classification_metrics(pred_logits, labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        region_acc_sum += region_acc.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    avg_loss = run_loss/n\n",
    "    avg_acc = run_acc/n\n",
    "    avg_off1 = run_off1/n\n",
    "    avg_mae = run_mae/n\n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    region_acc_avg = region_acc_sum / n\n",
    "    \n",
    "    print(f\"[{split}] loss:{avg_loss:.4f} acc:{avg_acc:.4f} \"\n",
    "          f\"off1:{avg_off1:.4f} mae:{avg_mae:.4f}\")\n",
    "    print(f\"  Region Acc: {region_acc_avg.numpy().round(3)}\")\n",
    "    \n",
    "    return avg_loss, avg_acc, avg_off1, avg_mae, part_losses_avg, region_acc_avg\n",
    "\n",
    "def get_lrs(optimizer):\n",
    "    return [pg['lr'] for pg in optimizer.param_groups]\n",
    "\n",
    "# ============================================================\n",
    "# Main Function\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ Brixia COVID-19 Classification Training\")\n",
    "    print(\"   ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ (ÏûêÎèô Í∞ÄÏ§ëÏπò)\")\n",
    "    print(\"   ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ (Self-Attention)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "    print(\"\\nüìÇ Loading data...\")\n",
    "    tr_df, val_df, tt_df = load_and_split_brixia(CSV_PATH)\n",
    "    \n",
    "    print(\"\\nüì¶ Creating DataLoaders...\")\n",
    "    tr_loader, val_loader, tt_loader = create_dataloaders(\n",
    "        tr_df, val_df, tt_df, img_dir=IMAGE_DIR, \n",
    "        batch_size=BATCH_SIZE, img_size=IMG_SIZE, num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Train labels Ï∂îÏ∂ú\n",
    "    train_labels = torch.cat([labels for _, labels in tr_loader], dim=0)\n",
    "    \n",
    "    # ========================================\n",
    "    # ÌïôÏäµ Ï§ÄÎπÑ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìç Training Setup\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï ÏûêÎèô Ï≤òÎ¶¨\n",
    "    criterion = AdaptiveClassificationLoss(\n",
    "        train_labels, \n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_class_weights=True,  # ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò ÏûêÎèô Í≥ÑÏÇ∞\n",
    "        part_weights=None,  # Î∂ÄÏúÑÎ≥Ñ Í∑†Îì± Í∞ÄÏ§ëÏπò\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    )\n",
    "    \n",
    "    # ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º Í≥†Î†§ÌïòÎäî Î™®Îç∏\n",
    "    model = EfficientNetB0Classification(\n",
    "        pretrained=True, \n",
    "        drop=DROP_RATIO,\n",
    "        num_regions=6,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    scaler = GradScaler(enabled=AMP)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_mae = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 15\n",
    "    \n",
    "    # ========================================\n",
    "    # ÌïôÏäµ Î£®ÌîÑ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèãÔ∏è Training Start\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        tr_loss, tr_acc, tr_off1, tr_mae, tr_part_losses = train_epoch(\n",
    "            model, tr_loader, criterion, optimizer, scaler, DEVICE, AMP\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_off1, val_mae, val_part_losses, val_region_acc = evaluate(\n",
    "            model, val_loader, criterion, DEVICE, split='val'\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_mae = val_mae\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': best_acc,\n",
    "                'val_mae': best_mae,\n",
    "                'val_off1': val_off1,\n",
    "                'region_acc': val_region_acc,\n",
    "            }, BEST_PATH)\n",
    "            print(f\"‚úÖ New Best! (Acc={best_acc:.4f}, MAE={best_mae:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping at epoch {ep}\")\n",
    "            break\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"\\n[Epoch {ep:03d}/{EPOCHS}]\")\n",
    "        print(f\"  Train - loss:{tr_loss:.4f} acc:{tr_acc:.4f} off1:{tr_off1:.4f} mae:{tr_mae:.4f}\")\n",
    "        print(f\"  Val   - loss:{val_loss:.4f} acc:{val_acc:.4f} off1:{val_off1:.4f} mae:{val_mae:.4f}\")\n",
    "        print(f\"  Part losses (Val): {val_part_losses.numpy().round(3)}\")\n",
    "        print(f\"  Region Acc (Val): {val_region_acc.numpy().round(3)}\")\n",
    "        print(f\"  LR:{get_lrs(optimizer)[0]:.2e} | {elapsed:.1f}s | Patience:{patience_counter}/{max_patience}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # ========================================\n",
    "    # Test Evaluation\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ Training Finished!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(tt_loader) > 0:\n",
    "        print(\"\\nüìä Test evaluation with best model...\")\n",
    "        checkpoint = torch.load(BEST_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        tt_loss, tt_acc, tt_off1, tt_mae, tt_part_losses, tt_region_acc = evaluate(\n",
    "            model, tt_loader, criterion, DEVICE, split='test'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüèÜ Test Results:\")\n",
    "        print(f\"   Accuracy: {tt_acc:.4f}\")\n",
    "        print(f\"   Off-by-1: {tt_off1:.4f}\")\n",
    "        print(f\"   MAE: {tt_mae:.4f}\")\n",
    "        print(f\"   Region Acc: {tt_region_acc.numpy().round(3)}\")\n",
    "        print(f\"   Part losses: {tt_part_losses.numpy().round(3)}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Best model saved: {BEST_PATH}\")\n",
    "    print(f\"üìà Best Validation Accuracy: {best_acc:.4f}\")\n",
    "    print(f\"üìâ Best Validation MAE: {best_mae:.4f}\")\n",
    "    print(\"\\n‚úÖ Ïù¥Ï†ú gradcam_inference.pyÎ•º Ïã§ÌñâÌïòÏó¨ Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÏÑ∏Ïöî!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d03b648a-5e41-42d2-936e-95c95c758be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üî¨ Brixia COVID-19 Grad-CAM ÏãúÍ∞ÅÌôî\n",
      "======================================================================\n",
      "\n",
      "üì¶ Î™®Îç∏ Î°úÎî©: ./runs_severity_classification/best_efficientnet_b0_classification.pth\n",
      "‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\n",
      "\n",
      "============================================================\n",
      "üìä ÏòàÏ∏° Í≤∞Í≥º: 5032497707401895.png\n",
      "============================================================\n",
      "A (Ï¢åÏÉÅ): Ï†ïÏÉÅ (0)       (Ïã†Î¢∞ÎèÑ:  65.3%)\n",
      "B (Ï¢åÏ§ë): Í≤ΩÏ¶ù (1)       (Ïã†Î¢∞ÎèÑ:  51.2%)\n",
      "C (Ï¢åÌïò): Ï§ëÎì±ÎèÑ (2)      (Ïã†Î¢∞ÎèÑ:  56.5%)\n",
      "D (Ïö∞ÏÉÅ): Ï†ïÏÉÅ (0)       (Ïã†Î¢∞ÎèÑ:  56.0%)\n",
      "E (Ïö∞Ï§ë): Ï§ëÎì±ÎèÑ (2)      (Ïã†Î¢∞ÎèÑ:  50.0%)\n",
      "F (Ïö∞Ìïò): Ï§ëÏ¶ù (3)       (Ïã†Î¢∞ÎèÑ:  66.3%)\n",
      "\n",
      "Ï†ÑÏ≤¥ Brixia Score: 8/18\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 46321 (\\N{HANGUL SYLLABLE DEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 44033 (\\N{HANGUL SYLLABLE GAG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 50948 (\\N{HANGUL SYLLABLE WI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 55176 (\\N{HANGUL SYLLABLE HI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:254: UserWarning: Glyph 47605 (\\N{HANGUL SYLLABLE MAEB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 44221 (\\N{HANGUL SYLLABLE GYEONG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 46321 (\\N{HANGUL SYLLABLE DEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 44033 (\\N{HANGUL SYLLABLE GAG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 50948 (\\N{HANGUL SYLLABLE WI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 55176 (\\N{HANGUL SYLLABLE HI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:257: UserWarning: Glyph 47605 (\\N{HANGUL SYLLABLE MAEB}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï†ÑÏ≤¥ ÏãúÍ∞ÅÌôî Ï†ÄÏû•: ./gradcam_results/5032497707401895_all_regions.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 51216 (\\N{HANGUL SYLLABLE JEOM}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 50948 (\\N{HANGUL SYLLABLE WI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 54224 (\\N{HANGUL SYLLABLE PYE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 44368 (\\N{HANGUL SYLLABLE GYO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:309: UserWarning: Glyph 52404 (\\N{HANGUL SYLLABLE CE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 51473 (\\N{HANGUL SYLLABLE JUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 51613 (\\N{HANGUL SYLLABLE JEUNG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 51216 (\\N{HANGUL SYLLABLE JEOM}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 50948 (\\N{HANGUL SYLLABLE WI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 48324 (\\N{HANGUL SYLLABLE BYEOL}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 51340 (\\N{HANGUL SYLLABLE JWA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 54224 (\\N{HANGUL SYLLABLE PYE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 44368 (\\N{HANGUL SYLLABLE GYO}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
      "/tmp/ipykernel_1647408/2806019575.py:312: UserWarning: Glyph 52404 (\\N{HANGUL SYLLABLE CE}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(save_path, dpi=150, bbox_inches='tight')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÏöîÏïΩ Ï∞®Ìä∏ Ï†ÄÏû•: ./gradcam_results/5032497707401895_summary.png\n",
      "\n",
      "‚úÖ Í≤∞Í≥ºÍ∞Ä ./gradcam_results/ Ìè¥ÎçîÏóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "# ============================================================\n",
    "# ÏÑ§Ï†ï\n",
    "# ============================================================\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 4\n",
    "NUM_REGIONS = 6\n",
    "\n",
    "MODEL_PATH = \"./runs_severity_classification/best_efficientnet_b0_classification.pth\"\n",
    "OUTPUT_DIR = \"./gradcam_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "REGION_NAMES = ['A (Ï¢åÏÉÅ)', 'B (Ï¢åÏ§ë)', 'C (Ï¢åÌïò)', 'D (Ïö∞ÏÉÅ)', 'E (Ïö∞Ï§ë)', 'F (Ïö∞Ìïò)']\n",
    "CLASS_NAMES = ['Ï†ïÏÉÅ (0)', 'Í≤ΩÏ¶ù (1)', 'Ï§ëÎì±ÎèÑ (2)', 'Ï§ëÏ¶ù (3)']\n",
    "\n",
    "# Î∂ÄÏúÑÎ≥Ñ ÏÉâÏÉÅ (ÏãúÍ∞ÅÌôîÏö©)\n",
    "REGION_COLORS = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F']\n",
    "\n",
    "# ============================================================\n",
    "# Î™®Îç∏ Ï†ïÏùò (ÌïôÏäµ ÏΩîÎìúÏôÄ ÎèôÏùº)\n",
    "# ============================================================\n",
    "class EfficientNetB0Classification(nn.Module):\n",
    "    def __init__(self, pretrained=False, drop=0.3, num_regions=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = efficientnet_b0(weights=weights)\n",
    "        \n",
    "        self.features = backbone.features\n",
    "        in_feat = 1280\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 49, in_feat))\n",
    "        self.region_queries = nn.Parameter(torch.randn(num_regions, in_feat))\n",
    "        \n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(in_feat)\n",
    "        self.norm3 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_feat * 2, in_feat),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        self.norm4 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_feat, 256),\n",
    "                nn.LayerNorm(256),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.LayerNorm(128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(drop),\n",
    "                nn.Linear(128, num_classes)\n",
    "            ) for _ in range(num_regions)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        feat = self.features(x)\n",
    "        feat = feat.flatten(2).transpose(1, 2)\n",
    "        feat = feat + self.pos_embed\n",
    "        \n",
    "        queries = self.region_queries.unsqueeze(0).expand(B, -1, -1)\n",
    "        attn_out, _ = self.cross_attention(query=queries, key=feat, value=feat)\n",
    "        attn_out = self.norm1(attn_out + queries)\n",
    "        \n",
    "        self_attn_out, attn_weights = self.self_attention(\n",
    "            query=attn_out, key=attn_out, value=attn_out\n",
    "        )\n",
    "        attn_out = self.norm2(attn_out + self_attn_out)\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        attn_out = self.norm3(attn_out + ffn_out)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(len(self.heads)):\n",
    "            region_feat = attn_out[:, i, :]\n",
    "            logits = self.heads[i](region_feat)\n",
    "            outputs.append(logits)\n",
    "        \n",
    "        out = torch.stack(outputs, dim=1)\n",
    "        return out, attn_weights\n",
    "\n",
    "# ============================================================\n",
    "# Grad-CAM Íµ¨ÌòÑ\n",
    "# ============================================================\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Hook Îì±Î°ù\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_full_backward_hook(self.save_gradient)\n",
    "    \n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "    \n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image, region_idx, class_idx=None):\n",
    "        \"\"\"\n",
    "        ÌäπÏ†ï Î∂ÄÏúÑ(region_idx)ÏôÄ ÌÅ¥ÎûòÏä§(class_idx)Ïóê ÎåÄÌïú CAM ÏÉùÏÑ±\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = self.model(input_image)  # [1, 6, 4]\n",
    "        \n",
    "        # ÌäπÏ†ï Î∂ÄÏúÑÏùò ÏòàÏ∏°\n",
    "        region_output = output[0, region_idx, :]  # [4]\n",
    "        \n",
    "        # class_idxÍ∞Ä ÏóÜÏúºÎ©¥ ÏòàÏ∏°Îêú ÌÅ¥ÎûòÏä§ ÏÇ¨Ïö©\n",
    "        if class_idx is None:\n",
    "            class_idx = region_output.argmax().item()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        target = region_output[class_idx]\n",
    "        target.backward(retain_graph=True)\n",
    "        \n",
    "        # Grad-CAM Í≥ÑÏÇ∞\n",
    "        gradients = self.gradients[0]  # [C, H, W]\n",
    "        activations = self.activations[0]  # [C, H, W]\n",
    "        \n",
    "        # Global Average Pooling on gradients\n",
    "        weights = gradients.mean(dim=(1, 2), keepdim=True)  # [C, 1, 1]\n",
    "        \n",
    "        # Weighted combination\n",
    "        cam = (weights * activations).sum(dim=0)  # [H, W]\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # Normalize\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "        \n",
    "        return cam.cpu().numpy(), class_idx\n",
    "\n",
    "# ============================================================\n",
    "# ÏãúÍ∞ÅÌôî Ìï®Ïàò\n",
    "# ============================================================\n",
    "def preprocess_image(image_path, img_size=IMG_SIZE):\n",
    "    \"\"\"Ïù¥ÎØ∏ÏßÄ Ï†ÑÏ≤òÎ¶¨\"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_resized = img.resize((img_size, img_size), Image.BILINEAR)\n",
    "    \n",
    "    # Tensor Î≥ÄÌôò\n",
    "    img_tensor = TF.to_tensor(img_resized)\n",
    "    img_tensor = TF.normalize(img_tensor, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    return img_tensor.unsqueeze(0), img_resized\n",
    "\n",
    "def visualize_gradcam(original_img, cam, region_name, pred_class, confidence, \n",
    "                      save_path=None, alpha=0.4):\n",
    "    \"\"\"Grad-CAM ÌûàÌä∏Îßµ ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    # CAMÏùÑ ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Î¶¨ÏÇ¨Ïù¥Ï¶à\n",
    "    cam_resized = np.array(Image.fromarray(cam).resize(\n",
    "        original_img.size, Image.BILINEAR\n",
    "    ))\n",
    "    \n",
    "    # ÌûàÌä∏Îßµ ÏÉùÏÑ±\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title(f'{region_name}\\nÏòàÏ∏°: {CLASS_NAMES[pred_class]} ({confidence:.1f}%)', \n",
    "              fontsize=12, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Grad-CAM Ïò§Î≤ÑÎ†àÏù¥\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(original_img)\n",
    "    plt.imshow(cam_resized, cmap='jet', alpha=alpha)\n",
    "    plt.title(f'Grad-CAM Heatmap', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Ï†ÄÏû•: {save_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def visualize_all_regions(original_img, model, img_tensor, predictions, \n",
    "                          confidences, save_path=None):\n",
    "    \"\"\"6Í∞ú Î∂ÄÏúÑ Î™®Îëê ÏãúÍ∞ÅÌôî (2x3 grid)\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Grad-CAM ÏÉùÏÑ±\n",
    "    target_layer = model.features[-1]  # EfficientNet ÎßàÏßÄÎßâ Conv layer\n",
    "    grad_cam = GradCAM(model, target_layer)\n",
    "    \n",
    "    for idx in range(6):\n",
    "        cam, pred_class = grad_cam.generate_cam(img_tensor, region_idx=idx)\n",
    "        \n",
    "        # CAM Î¶¨ÏÇ¨Ïù¥Ï¶à\n",
    "        cam_resized = np.array(Image.fromarray(cam).resize(\n",
    "            original_img.size, Image.BILINEAR\n",
    "        ))\n",
    "        \n",
    "        # ÏãúÍ∞ÅÌôî\n",
    "        axes[idx].imshow(original_img)\n",
    "        im = axes[idx].imshow(cam_resized, cmap='jet', alpha=0.4)\n",
    "        \n",
    "        axes[idx].set_title(\n",
    "            f'{REGION_NAMES[idx]}\\n{CLASS_NAMES[pred_class]} ({confidences[idx]:.1f}%)',\n",
    "            fontsize=12, fontweight='bold', color=REGION_COLORS[idx]\n",
    "        )\n",
    "        axes[idx].axis('off')\n",
    "        \n",
    "        # Colorbar\n",
    "        cbar = plt.colorbar(im, ax=axes[idx], fraction=0.046, pad=0.04)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "    \n",
    "    # Ï†ÑÏ≤¥ Brixia Score ÌëúÏãú\n",
    "    total_score = sum(predictions)\n",
    "    fig.suptitle(\n",
    "        f'Brixia COVID-19 Severity Score: {total_score}/18\\n'\n",
    "        f'Í∞Å Î∂ÄÏúÑÎ≥Ñ Grad-CAM ÌûàÌä∏Îßµ',\n",
    "        fontsize=16, fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úÖ Ï†ÑÏ≤¥ ÏãúÍ∞ÅÌôî Ï†ÄÏû•: {save_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "def visualize_prediction_summary(predictions, confidences, save_path=None):\n",
    "    \"\"\"ÏòàÏ∏° Í≤∞Í≥º ÏöîÏïΩ Ï∞®Ìä∏\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 1. Î∂ÄÏúÑÎ≥Ñ Ï§ëÏ¶ùÎèÑ ÎßâÎåÄ Í∑∏ÎûòÌîÑ\n",
    "    x = np.arange(6)\n",
    "    bars = ax1.bar(x, predictions, color=REGION_COLORS, alpha=0.7, edgecolor='black')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(REGION_NAMES, rotation=45, ha='right')\n",
    "    ax1.set_ylabel('Ï§ëÏ¶ùÎèÑ Ï†êÏàò', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylim(0, 3.5)\n",
    "    ax1.set_title('Î∂ÄÏúÑÎ≥Ñ Ï§ëÏ¶ùÎèÑ Ï†êÏàò', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Í∞Å ÎßâÎåÄ ÏúÑÏóê Í∞í ÌëúÏãú\n",
    "    for i, (bar, conf) in enumerate(zip(bars, confidences)):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}\\n({conf:.0f}%)',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # 2. Ï¢åÏö∞ ÎπÑÍµê\n",
    "    left_scores = predictions[:3]  # A, B, C\n",
    "    right_scores = predictions[3:]  # D, E, F\n",
    "    \n",
    "    categories = ['ÏÉÅÎ∂Ä', 'Ï§ëÎ∂Ä', 'ÌïòÎ∂Ä']\n",
    "    x2 = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x2 - width/2, left_scores, width, label='Ï¢åÌèê', \n",
    "            color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "    ax2.bar(x2 + width/2, right_scores, width, label='Ïö∞Ìèê', \n",
    "            color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax2.set_xticks(x2)\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylabel('Ï§ëÏ¶ùÎèÑ Ï†êÏàò', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylim(0, 3.5)\n",
    "    ax2.set_title('Ï¢åÏö∞ Ìèê ÎπÑÍµê', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Ï¥ùÏ†ê ÌëúÏãú\n",
    "    total_score = sum(predictions)\n",
    "    fig.suptitle(f'Ï†ÑÏ≤¥ Brixia Score: {total_score}/18', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"‚úÖ ÏöîÏïΩ Ï∞®Ìä∏ Ï†ÄÏû•: {save_path}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# Ï∂îÎ°† Ìï®Ïàò\n",
    "# ============================================================\n",
    "# @torch.no_grad()\n",
    "def predict_single_image(model, image_path, visualize=True):\n",
    "    \"\"\"Îã®Ïùº Ïù¥ÎØ∏ÏßÄ Ï∂îÎ°† Î∞è ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "        img_tensor, original_img = preprocess_image(image_path)\n",
    "        img_tensor = img_tensor.to(DEVICE)\n",
    "        \n",
    "        # ÏòàÏ∏°\n",
    "        output, attn_weights = model(img_tensor)  # [1, 6, 4]\n",
    "        probs = F.softmax(output, dim=-1)  # [1, 6, 4]\n",
    "        \n",
    "        predictions = output[0].argmax(dim=-1).cpu().numpy()  # [6]\n",
    "        confidences = probs[0].max(dim=-1).values.detach().cpu().numpy() * 100  # [6]\n",
    "    \n",
    "    # Í≤∞Í≥º Ï∂úÎ†•\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üìä ÏòàÏ∏° Í≤∞Í≥º: {os.path.basename(image_path)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, (name, pred, conf) in enumerate(zip(REGION_NAMES, predictions, confidences)):\n",
    "        print(f\"{name}: {CLASS_NAMES[pred]:12s} (Ïã†Î¢∞ÎèÑ: {conf:5.1f}%)\")\n",
    "    \n",
    "    total_score = predictions.sum()\n",
    "    print(f\"\\nÏ†ÑÏ≤¥ Brixia Score: {total_score}/18\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # ÏãúÍ∞ÅÌôî\n",
    "    if visualize:\n",
    "        base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        \n",
    "        # 1. Ï†ÑÏ≤¥ 6Í∞ú Î∂ÄÏúÑ Grad-CAM\n",
    "        all_regions_path = os.path.join(OUTPUT_DIR, f\"{base_name}_all_regions.png\")\n",
    "        visualize_all_regions(original_img, model, img_tensor, \n",
    "                             predictions, confidences, all_regions_path)\n",
    "        \n",
    "        # 2. ÏòàÏ∏° ÏöîÏïΩ Ï∞®Ìä∏\n",
    "        summary_path = os.path.join(OUTPUT_DIR, f\"{base_name}_summary.png\")\n",
    "        visualize_prediction_summary(predictions, confidences, summary_path)\n",
    "        \n",
    "        # 3. Í∞úÎ≥Ñ Î∂ÄÏúÑ Grad-CAM (ÏÑ†ÌÉùÏ†Å)\n",
    "        # for idx in range(6):\n",
    "        #     single_path = os.path.join(OUTPUT_DIR, f\"{base_name}_{REGION_NAMES[idx]}.png\")\n",
    "        #     target_layer = model.features[-1]\n",
    "        #     grad_cam = GradCAM(model, target_layer)\n",
    "        #     cam, pred_class = grad_cam.generate_cam(img_tensor, region_idx=idx)\n",
    "        #     visualize_gradcam(original_img, cam, REGION_NAMES[idx], \n",
    "        #                      pred_class, confidences[idx], single_path)\n",
    "    \n",
    "    return predictions, confidences, attn_weights\n",
    "\n",
    "# ============================================================\n",
    "# Î©îÏù∏ Ïã§Ìñâ\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üî¨ Brixia COVID-19 Grad-CAM ÏãúÍ∞ÅÌôî\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Î™®Îç∏ Î°úÎìú\n",
    "    print(f\"\\nüì¶ Î™®Îç∏ Î°úÎî©: {MODEL_PATH}\")\n",
    "    model = EfficientNetB0Classification(\n",
    "        pretrained=False,\n",
    "        drop=0.3,\n",
    "        num_regions=NUM_REGIONS,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(\"‚úÖ Î™®Îç∏ Î°úÎìú ÏôÑÎ£å!\")\n",
    "    \n",
    "    # ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú (ÏòàÏãú)\n",
    "    # Ïã§Ï†ú ÏÇ¨Ïö© Ïãú Í≤ΩÎ°ú ÏàòÏ†ï ÌïÑÏöî\n",
    "    test_image_path = \"./data/covid19-xray-severity-scoring/segmented_png/5032497707401895.png\"\n",
    "    \n",
    "    # Ïù¥ÎØ∏ÏßÄÍ∞Ä Ï°¥Ïû¨ÌïòÎäîÏßÄ ÌôïÏù∏\n",
    "    if not os.path.exists(test_image_path):\n",
    "        print(f\"\\n‚ö†Ô∏è ÌÖåÏä§Ìä∏ Ïù¥ÎØ∏ÏßÄÎ•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {test_image_path}\")\n",
    "        print(\"üìù ÏÇ¨Ïö©Î≤ï:\")\n",
    "        print(\"   1. test_image_path Î≥ÄÏàòÎ•º Ïã§Ï†ú Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°úÎ°ú ÏàòÏ†ï\")\n",
    "        print(\"   2. ÎòêÎäî ÏïÑÎûò Ìï®ÏàòÎ•º ÏßÅÏ†ë Ìò∏Ï∂ú:\")\n",
    "        print(\"      predict_single_image(model, 'your_image_path.png')\")\n",
    "        return\n",
    "    \n",
    "    # Ï∂îÎ°† Î∞è ÏãúÍ∞ÅÌôî\n",
    "    predictions, confidences, attn_weights = predict_single_image(\n",
    "        model, test_image_path, visualize=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Í≤∞Í≥ºÍ∞Ä {OUTPUT_DIR}/ Ìè¥ÎçîÏóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb4c378b-4cd7-4309-acf3-2dafb0a0b564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ Brixia COVID-19 Ordinal Classification Training\n",
      "   üí° 80% Î™©ÌëúÎ•º ÏúÑÌïú ÌïµÏã¨ Í∏∞Ïà†:\n",
      "   ‚úÖ Ordinal Loss - ÏàúÏÑúÌòï ÌöåÍ∑Ä (0<1<2<3)\n",
      "   ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ (ÏûêÎèô Í∞ÄÏ§ëÏπò)\n",
      "   ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ (Self-Attention)\n",
      "   ‚úÖ Mixed Precision Training (AMP)\n",
      "   ‚úÖ Mixup Augmentation\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: 4695Í∞ú\n",
      "Train: 3637, Val: 912, Test: 146\n",
      "Train - Mean: 8.31, Std: 4.26\n",
      "Val - Mean: 8.35, Std: 4.15\n",
      "Test - Mean: 7.78, Std: 4.20\n",
      "\n",
      "üì¶ Creating DataLoaders...\n",
      "‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\n",
      "   Train: 3637 samples, 113 batches\n",
      "   Val:   912 samples, 29 batches\n",
      "   Test:  146 samples, 5 batches\n",
      "\n",
      "======================================================================\n",
      "üìç Training Setup\n",
      "======================================================================\n",
      "============================================================\n",
      "Class Distribution Analysis\n",
      "============================================================\n",
      "\n",
      "A:\n",
      "  Class 0: 1810 ( 50.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1126 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  446 ( 12.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  234 (  6.5%) ‚ñà‚ñà‚ñà\n",
      "\n",
      "B:\n",
      "  Class 0:  721 ( 19.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  949 ( 26.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1171 ( 32.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  775 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "C:\n",
      "  Class 0:  406 ( 11.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  783 ( 21.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1300 ( 36.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1127 ( 31.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "D:\n",
      "  Class 0: 1804 ( 49.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1126 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  476 ( 13.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  210 (  5.8%) ‚ñà‚ñà\n",
      "\n",
      "E:\n",
      "  Class 0:  762 ( 21.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  963 ( 26.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1118 ( 30.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  773 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "F:\n",
      "  Class 0:  407 ( 11.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  795 ( 22.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1306 ( 36.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1108 ( 30.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "‚úÖ Class weights: [0.9512662  0.96508205 0.9588403  1.1248114 ]\n",
      "‚úÖ Part weights: [1. 1. 1. 1. 1. 1.]\n",
      "‚úÖ Ordinal weight: 0.5\n",
      "   Parameters: 24,130,086\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training Start\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.8177 acc:0.5000 off1:0.8849 mae:0.6274\n",
      "  Region Acc: [0.535 0.478 0.442 0.604 0.477 0.464]\n",
      "‚úÖ New Best! (Acc=0.5000, MAE=0.6274)\n",
      "\n",
      "[Epoch 001/100]\n",
      "  Train - loss:1.0706 acc:0.4070 off1:0.8209 mae:0.8038\n",
      "  Val   - loss:0.8177 acc:0.5000 off1:0.8849 mae:0.6274\n",
      "  Part losses (Val): [0.805 0.849 0.851 0.726 0.832 0.843]\n",
      "  Region Acc (Val): [0.535 0.478 0.442 0.604 0.477 0.464]\n",
      "  LR:1.00e-04 | 17.4s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7633 acc:0.5055 off1:0.8977 mae:0.6080\n",
      "  Region Acc: [0.553 0.469 0.433 0.623 0.48  0.475]\n",
      "‚úÖ New Best! (Acc=0.5055, MAE=0.6080)\n",
      "\n",
      "[Epoch 002/100]\n",
      "  Train - loss:0.9917 acc:0.4487 off1:0.8573 mae:0.7199\n",
      "  Val   - loss:0.7633 acc:0.5055 off1:0.8977 mae:0.6080\n",
      "  Part losses (Val): [0.741 0.79  0.816 0.671 0.777 0.785]\n",
      "  Region Acc (Val): [0.553 0.469 0.433 0.623 0.48  0.475]\n",
      "  LR:1.00e-04 | 13.4s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7017 acc:0.5481 off1:0.9262 mae:0.5305\n",
      "  Region Acc: [0.562 0.536 0.515 0.657 0.523 0.495]\n",
      "‚úÖ New Best! (Acc=0.5481, MAE=0.5305)\n",
      "\n",
      "[Epoch 003/100]\n",
      "  Train - loss:0.9166 acc:0.4787 off1:0.8809 mae:0.6574\n",
      "  Val   - loss:0.7017 acc:0.5481 off1:0.9262 mae:0.5305\n",
      "  Part losses (Val): [0.684 0.709 0.732 0.608 0.727 0.75 ]\n",
      "  Region Acc (Val): [0.562 0.536 0.515 0.657 0.523 0.495]\n",
      "  LR:1.00e-04 | 14.2s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6826 acc:0.5590 off1:0.9315 mae:0.5148\n",
      "  Region Acc: [0.586 0.539 0.524 0.649 0.548 0.508]\n",
      "‚úÖ New Best! (Acc=0.5590, MAE=0.5148)\n",
      "\n",
      "[Epoch 004/100]\n",
      "  Train - loss:0.8872 acc:0.4768 off1:0.8747 mae:0.6703\n",
      "  Val   - loss:0.6826 acc:0.5590 off1:0.9315 mae:0.5148\n",
      "  Part losses (Val): [0.677 0.703 0.711 0.587 0.697 0.72 ]\n",
      "  Region Acc (Val): [0.586 0.539 0.524 0.649 0.548 0.508]\n",
      "  LR:1.00e-04 | 14.2s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6711 acc:0.5607 off1:0.9412 mae:0.5031\n",
      "  Region Acc: [0.579 0.549 0.541 0.64  0.556 0.499]\n",
      "‚úÖ New Best! (Acc=0.5607, MAE=0.5031)\n",
      "\n",
      "[Epoch 005/100]\n",
      "  Train - loss:0.8796 acc:0.4918 off1:0.8787 mae:0.6533\n",
      "  Val   - loss:0.6711 acc:0.5607 off1:0.9412 mae:0.5031\n",
      "  Part losses (Val): [0.658 0.681 0.697 0.589 0.684 0.718]\n",
      "  Region Acc (Val): [0.579 0.549 0.541 0.64  0.556 0.499]\n",
      "  LR:1.00e-04 | 13.6s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6551 acc:0.5673 off1:0.9494 mae:0.4861\n",
      "  Region Acc: [0.547 0.553 0.538 0.658 0.583 0.524]\n",
      "‚úÖ New Best! (Acc=0.5673, MAE=0.4861)\n",
      "\n",
      "[Epoch 006/100]\n",
      "  Train - loss:0.8611 acc:0.4769 off1:0.8703 mae:0.6791\n",
      "  Val   - loss:0.6551 acc:0.5673 off1:0.9494 mae:0.4861\n",
      "  Part losses (Val): [0.653 0.665 0.682 0.573 0.652 0.706]\n",
      "  Region Acc (Val): [0.547 0.553 0.538 0.658 0.583 0.524]\n",
      "  LR:1.00e-04 | 14.4s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6416 acc:0.5768 off1:0.9485 mae:0.4783\n",
      "  Region Acc: [0.588 0.569 0.556 0.673 0.569 0.505]\n",
      "‚úÖ New Best! (Acc=0.5768, MAE=0.4783)\n",
      "\n",
      "[Epoch 007/100]\n",
      "  Train - loss:0.8326 acc:0.4980 off1:0.8850 mae:0.6393\n",
      "  Val   - loss:0.6416 acc:0.5768 off1:0.9485 mae:0.4783\n",
      "  Part losses (Val): [0.625 0.654 0.667 0.555 0.651 0.697]\n",
      "  Region Acc (Val): [0.588 0.569 0.556 0.673 0.569 0.505]\n",
      "  LR:1.00e-04 | 13.8s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6365 acc:0.5853 off1:0.9488 mae:0.4698\n",
      "  Region Acc: [0.601 0.558 0.555 0.689 0.605 0.504]\n",
      "‚úÖ New Best! (Acc=0.5853, MAE=0.4698)\n",
      "\n",
      "[Epoch 008/100]\n",
      "  Train - loss:0.8044 acc:0.5110 off1:0.8947 mae:0.6146\n",
      "  Val   - loss:0.6365 acc:0.5853 off1:0.9488 mae:0.4698\n",
      "  Part losses (Val): [0.62  0.662 0.662 0.54  0.641 0.695]\n",
      "  Region Acc (Val): [0.601 0.558 0.555 0.689 0.605 0.504]\n",
      "  LR:1.00e-04 | 14.8s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6281 acc:0.5848 off1:0.9598 mae:0.4580\n",
      "  Region Acc: [0.613 0.577 0.544 0.657 0.587 0.532]\n",
      "\n",
      "[Epoch 009/100]\n",
      "  Train - loss:0.7970 acc:0.5205 off1:0.9004 mae:0.5962\n",
      "  Val   - loss:0.6281 acc:0.5848 off1:0.9598 mae:0.4580\n",
      "  Part losses (Val): [0.613 0.635 0.654 0.549 0.628 0.69 ]\n",
      "  Region Acc (Val): [0.613 0.577 0.544 0.657 0.587 0.532]\n",
      "  LR:1.00e-04 | 13.6s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6311 acc:0.5879 off1:0.9510 mae:0.4638\n",
      "  Region Acc: [0.6   0.578 0.554 0.673 0.594 0.529]\n",
      "‚úÖ New Best! (Acc=0.5879, MAE=0.4638)\n",
      "\n",
      "[Epoch 010/100]\n",
      "  Train - loss:0.8100 acc:0.5379 off1:0.9179 mae:0.5582\n",
      "  Val   - loss:0.6311 acc:0.5879 off1:0.9510 mae:0.4638\n",
      "  Part losses (Val): [0.631 0.642 0.655 0.539 0.64  0.679]\n",
      "  Region Acc (Val): [0.6   0.578 0.554 0.673 0.594 0.529]\n",
      "  LR:1.00e-04 | 14.3s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6420 acc:0.5744 off1:0.9538 mae:0.4755\n",
      "  Region Acc: [0.602 0.541 0.537 0.669 0.577 0.521]\n",
      "\n",
      "[Epoch 011/100]\n",
      "  Train - loss:0.8088 acc:0.5138 off1:0.8934 mae:0.6139\n",
      "  Val   - loss:0.6420 acc:0.5744 off1:0.9538 mae:0.4755\n",
      "  Part losses (Val): [0.622 0.662 0.675 0.546 0.653 0.694]\n",
      "  Region Acc (Val): [0.602 0.541 0.537 0.669 0.577 0.521]\n",
      "  LR:1.00e-04 | 13.3s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6272 acc:0.5824 off1:0.9560 mae:0.4644\n",
      "  Region Acc: [0.596 0.577 0.549 0.639 0.595 0.537]\n",
      "\n",
      "[Epoch 012/100]\n",
      "  Train - loss:0.7947 acc:0.5263 off1:0.9006 mae:0.5932\n",
      "  Val   - loss:0.6272 acc:0.5824 off1:0.9560 mae:0.4644\n",
      "  Part losses (Val): [0.616 0.64  0.652 0.546 0.623 0.687]\n",
      "  Region Acc (Val): [0.596 0.577 0.549 0.639 0.595 0.537]\n",
      "  LR:1.00e-04 | 13.3s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6221 acc:0.5963 off1:0.9556 mae:0.4527\n",
      "  Region Acc: [0.626 0.579 0.564 0.68  0.592 0.537]\n",
      "‚úÖ New Best! (Acc=0.5963, MAE=0.4527)\n",
      "\n",
      "[Epoch 013/100]\n",
      "  Train - loss:0.8079 acc:0.5246 off1:0.8993 mae:0.5944\n",
      "  Val   - loss:0.6221 acc:0.5963 off1:0.9556 mae:0.4527\n",
      "  Part losses (Val): [0.599 0.635 0.654 0.528 0.637 0.68 ]\n",
      "  Region Acc (Val): [0.626 0.579 0.564 0.68  0.592 0.537]\n",
      "  LR:1.00e-04 | 14.0s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6289 acc:0.5894 off1:0.9538 mae:0.4611\n",
      "  Region Acc: [0.609 0.576 0.559 0.682 0.587 0.524]\n",
      "\n",
      "[Epoch 014/100]\n",
      "  Train - loss:0.7494 acc:0.5568 off1:0.9229 mae:0.5325\n",
      "  Val   - loss:0.6289 acc:0.5894 off1:0.9538 mae:0.4611\n",
      "  Part losses (Val): [0.621 0.639 0.654 0.532 0.631 0.696]\n",
      "  Region Acc (Val): [0.609 0.576 0.559 0.682 0.587 0.524]\n",
      "  LR:1.00e-04 | 13.9s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6283 acc:0.5892 off1:0.9558 mae:0.4583\n",
      "  Region Acc: [0.611 0.583 0.558 0.683 0.577 0.523]\n",
      "\n",
      "[Epoch 015/100]\n",
      "  Train - loss:0.7585 acc:0.5350 off1:0.8992 mae:0.5878\n",
      "  Val   - loss:0.6283 acc:0.5892 off1:0.9558 mae:0.4583\n",
      "  Part losses (Val): [0.607 0.636 0.655 0.534 0.638 0.7  ]\n",
      "  Region Acc (Val): [0.611 0.583 0.558 0.683 0.577 0.523]\n",
      "  LR:1.00e-04 | 13.0s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6338 acc:0.5822 off1:0.9510 mae:0.4700\n",
      "  Region Acc: [0.607 0.545 0.555 0.682 0.582 0.522]\n",
      "\n",
      "[Epoch 016/100]\n",
      "  Train - loss:0.7548 acc:0.5459 off1:0.9080 mae:0.5644\n",
      "  Val   - loss:0.6338 acc:0.5822 off1:0.9510 mae:0.4700\n",
      "  Part losses (Val): [0.62  0.655 0.658 0.539 0.641 0.689]\n",
      "  Region Acc (Val): [0.607 0.545 0.555 0.682 0.582 0.522]\n",
      "  LR:1.00e-04 | 13.4s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6293 acc:0.5808 off1:0.9560 mae:0.4675\n",
      "  Region Acc: [0.615 0.561 0.546 0.673 0.569 0.52 ]\n",
      "\n",
      "[Epoch 017/100]\n",
      "  Train - loss:0.7459 acc:0.5668 off1:0.9176 mae:0.5302\n",
      "  Val   - loss:0.6293 acc:0.5808 off1:0.9560 mae:0.4675\n",
      "  Part losses (Val): [0.612 0.643 0.655 0.542 0.632 0.691]\n",
      "  Region Acc (Val): [0.615 0.561 0.546 0.673 0.569 0.52 ]\n",
      "  LR:1.00e-04 | 13.2s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6285 acc:0.5928 off1:0.9545 mae:0.4558\n",
      "  Region Acc: [0.617 0.577 0.561 0.683 0.572 0.546]\n",
      "\n",
      "[Epoch 018/100]\n",
      "  Train - loss:0.7330 acc:0.5635 off1:0.9139 mae:0.5399\n",
      "  Val   - loss:0.6285 acc:0.5928 off1:0.9545 mae:0.4558\n",
      "  Part losses (Val): [0.615 0.645 0.663 0.535 0.629 0.683]\n",
      "  Region Acc (Val): [0.617 0.577 0.561 0.683 0.572 0.546]\n",
      "  LR:1.00e-04 | 13.1s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6385 acc:0.5828 off1:0.9549 mae:0.4655\n",
      "  Region Acc: [0.603 0.562 0.537 0.679 0.59  0.525]\n",
      "\n",
      "[Epoch 019/100]\n",
      "  Train - loss:0.7070 acc:0.5570 off1:0.9095 mae:0.5519\n",
      "  Val   - loss:0.6385 acc:0.5828 off1:0.9549 mae:0.4655\n",
      "  Part losses (Val): [0.621 0.655 0.674 0.549 0.636 0.696]\n",
      "  Region Acc (Val): [0.603 0.562 0.537 0.679 0.59  0.525]\n",
      "  LR:5.00e-05 | 13.1s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6363 acc:0.5906 off1:0.9554 mae:0.4572\n",
      "  Region Acc: [0.616 0.56  0.566 0.684 0.588 0.53 ]\n",
      "\n",
      "[Epoch 020/100]\n",
      "  Train - loss:0.7481 acc:0.5572 off1:0.9085 mae:0.5517\n",
      "  Val   - loss:0.6363 acc:0.5906 off1:0.9554 mae:0.4572\n",
      "  Part losses (Val): [0.631 0.651 0.662 0.55  0.632 0.692]\n",
      "  Region Acc (Val): [0.616 0.56  0.566 0.684 0.588 0.53 ]\n",
      "  LR:5.00e-05 | 12.8s | Patience:7/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6478 acc:0.5808 off1:0.9545 mae:0.4691\n",
      "  Region Acc: [0.622 0.57  0.538 0.668 0.575 0.512]\n",
      "\n",
      "[Epoch 021/100]\n",
      "  Train - loss:0.6738 acc:0.5989 off1:0.9288 mae:0.4856\n",
      "  Val   - loss:0.6478 acc:0.5808 off1:0.9545 mae:0.4691\n",
      "  Part losses (Val): [0.646 0.654 0.677 0.558 0.639 0.712]\n",
      "  Region Acc (Val): [0.622 0.57  0.538 0.668 0.575 0.512]\n",
      "  LR:5.00e-05 | 12.2s | Patience:8/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6346 acc:0.5883 off1:0.9585 mae:0.4560\n",
      "  Region Acc: [0.607 0.575 0.558 0.681 0.587 0.522]\n",
      "\n",
      "[Epoch 022/100]\n",
      "  Train - loss:0.7160 acc:0.5860 off1:0.9175 mae:0.5125\n",
      "  Val   - loss:0.6346 acc:0.5883 off1:0.9585 mae:0.4560\n",
      "  Part losses (Val): [0.615 0.645 0.669 0.534 0.646 0.698]\n",
      "  Region Acc (Val): [0.607 0.575 0.558 0.681 0.587 0.522]\n",
      "  LR:5.00e-05 | 12.9s | Patience:9/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6528 acc:0.5824 off1:0.9539 mae:0.4682\n",
      "  Region Acc: [0.602 0.573 0.554 0.668 0.594 0.503]\n",
      "\n",
      "[Epoch 023/100]\n",
      "  Train - loss:0.6885 acc:0.5806 off1:0.9145 mae:0.5240\n",
      "  Val   - loss:0.6528 acc:0.5824 off1:0.9539 mae:0.4682\n",
      "  Part losses (Val): [0.65  0.652 0.683 0.559 0.658 0.716]\n",
      "  Region Acc (Val): [0.602 0.573 0.554 0.668 0.594 0.503]\n",
      "  LR:5.00e-05 | 13.3s | Patience:10/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6476 acc:0.5906 off1:0.9541 mae:0.4583\n",
      "  Region Acc: [0.61  0.578 0.569 0.672 0.584 0.531]\n",
      "\n",
      "[Epoch 024/100]\n",
      "  Train - loss:0.6519 acc:0.6068 off1:0.9256 mae:0.4819\n",
      "  Val   - loss:0.6476 acc:0.5906 off1:0.9541 mae:0.4583\n",
      "  Part losses (Val): [0.644 0.66  0.67  0.553 0.647 0.711]\n",
      "  Region Acc (Val): [0.61  0.578 0.569 0.672 0.584 0.531]\n",
      "  LR:5.00e-05 | 13.8s | Patience:11/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6555 acc:0.5828 off1:0.9536 mae:0.4682\n",
      "  Region Acc: [0.598 0.577 0.557 0.669 0.578 0.519]\n",
      "\n",
      "[Epoch 025/100]\n",
      "  Train - loss:0.6583 acc:0.5978 off1:0.9216 mae:0.4983\n",
      "  Val   - loss:0.6555 acc:0.5828 off1:0.9536 mae:0.4682\n",
      "  Part losses (Val): [0.659 0.658 0.684 0.569 0.647 0.717]\n",
      "  Region Acc (Val): [0.598 0.577 0.557 0.669 0.578 0.519]\n",
      "  LR:2.50e-05 | 13.2s | Patience:12/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6572 acc:0.5828 off1:0.9583 mae:0.4620\n",
      "  Region Acc: [0.606 0.576 0.541 0.664 0.582 0.527]\n",
      "\n",
      "[Epoch 026/100]\n",
      "  Train - loss:0.6394 acc:0.6116 off1:0.9248 mae:0.4810\n",
      "  Val   - loss:0.6572 acc:0.5828 off1:0.9583 mae:0.4620\n",
      "  Part losses (Val): [0.647 0.659 0.692 0.571 0.659 0.715]\n",
      "  Region Acc (Val): [0.606 0.576 0.541 0.664 0.582 0.527]\n",
      "  LR:2.50e-05 | 13.1s | Patience:13/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6470 acc:0.5846 off1:0.9572 mae:0.4614\n",
      "  Region Acc: [0.613 0.567 0.567 0.662 0.57  0.529]\n",
      "\n",
      "[Epoch 027/100]\n",
      "  Train - loss:0.6684 acc:0.5970 off1:0.9136 mae:0.5092\n",
      "  Val   - loss:0.6470 acc:0.5846 off1:0.9572 mae:0.4614\n",
      "  Part losses (Val): [0.633 0.653 0.68  0.56  0.646 0.709]\n",
      "  Region Acc (Val): [0.613 0.567 0.567 0.662 0.57  0.529]\n",
      "  LR:2.50e-05 | 13.8s | Patience:14/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6601 acc:0.5793 off1:0.9556 mae:0.4682\n",
      "  Region Acc: [0.599 0.561 0.559 0.669 0.57  0.518]\n",
      "\n",
      "‚èπÔ∏è Early stopping at epoch 28\n",
      "\n",
      "======================================================================\n",
      "üéâ Training Finished!\n",
      "======================================================================\n",
      "\n",
      "üìä Test evaluation with best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] loss:0.5895 acc:0.5982 off1:0.9703 mae:0.4326\n",
      "  Region Acc: [0.664 0.555 0.603 0.719 0.521 0.527]\n",
      "\n",
      "üèÜ Test Results:\n",
      "   Accuracy: 0.5982\n",
      "   Off-by-1: 0.9703\n",
      "   MAE: 0.4326\n",
      "   Region Acc: [0.664 0.555 0.603 0.719 0.521 0.527]\n",
      "   Part losses: [0.53  0.62  0.607 0.503 0.614 0.663]\n",
      "\n",
      "üíæ Best model saved: runs_severity_classification/best_efficientnet_b0_classification.pth\n",
      "üìà Best Validation Accuracy: 0.5963\n",
      "üìâ Best Validation MAE: 0.4527\n",
      "\n",
      "======================================================================\n",
      "üí° Ï∂îÍ∞Ä ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú Ï†úÏïà:\n",
      "======================================================================\n",
      "1. üéØ ÏïôÏÉÅÎ∏î: Îã§Î•∏ ÏãúÎìúÎ°ú 3~5Í∞ú Î™®Îç∏ ÌïôÏäµ ÌõÑ Ìà¨Ìëú\n",
      "2. üî¨ ÏùòÎ£å Ï†ÑÏö© pretrained model ÏÇ¨Ïö©:\n",
      "   - CheXpert, MIMIC-CXR Îì±ÏúºÎ°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏\n",
      "3. üìä Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä:\n",
      "   - Ïô∏Î∂Ä COVID-19 Îç∞Ïù¥ÌÑ∞ÏÖã ÌôúÏö©\n",
      "   - Pseudo-labelingÏúºÎ°ú unlabeled Îç∞Ïù¥ÌÑ∞ ÌôúÏö©\n",
      "4. üé® Í≥†Í∏â Ï¶ùÍ∞ï:\n",
      "   - CutMix, AugMix, RandAugment\n",
      "   - Test-Time Augmentation (TTA)\n",
      "5. üß™ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù:\n",
      "   - ordinal_weight Ï°∞Ï†ï (0.3~0.7)\n",
      "   - Learning rate, batch size Ïã§Ìóò\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Ïù¥Ï†ú gradcam_inference.pyÎ•º Ïã§ÌñâÌïòÏó¨ Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÏÑ∏Ïöî!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- Í≤ΩÎ°ú ÏÑ§Ï†ï Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ---\n",
    "BASE_DIR     = f\"./data/covid19-xray-severity-scoring/\"\n",
    "CSV_PATH     = str(Path(BASE_DIR) / \"Brixia.csv\")\n",
    "IMAGE_DIR    = str(Path(BASE_DIR) / \"segmented_png\")\n",
    "\n",
    "OUT_DIR      = \"./runs_severity_classification\"\n",
    "BEST_PATH    = str(Path(OUT_DIR) / \"best_efficientnet_b0_classification.pth\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED         = 42\n",
    "IMG_SIZE     = 224\n",
    "BATCH_SIZE   = 32\n",
    "NUM_CLASSES  = 4   # 0, 1, 2, 3\n",
    "EPOCHS       = 100  # Single phase training\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "AMP          = True\n",
    "EARLY_STOP_ACC = 0.75  # üîÑ MAE ‚Üí Accuracy\n",
    "DROP_RATIO   = 0.3\n",
    "AUG_RATIO    = 0.5\n",
    "MIXUP_ALPHA  = 0.2\n",
    "LABEL_SMOOTHING = 0.1  # ‚úÖ NEW: Label smoothing\n",
    "\n",
    "# --- ÏãúÎìú Í≥†Ï†ï ---\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "def make_transform_with_label(train: bool, img_size: int = IMG_SIZE, aug_ratio=AUG_RATIO):\n",
    "    \"\"\"Brixia ScoreÏùò Ï¢åÏö∞ Íµ¨Ï°∞Î•º Í≥†Î†§Ìïú transform\"\"\"\n",
    "    def _tfm(img: Image.Image, label: torch.Tensor = None):\n",
    "        img = img.convert('RGB')\n",
    "        img = TF.resize(\n",
    "            img, \n",
    "            [img_size, img_size], \n",
    "            interpolation=TF.InterpolationMode.BILINEAR,\n",
    "            antialias=True\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            # 1. Horizontal Flip (Ï¢åÏö∞ Î∞òÏ†Ñ: ABC ‚Üî DEF)\n",
    "            if random.random() < aug_ratio:\n",
    "                img = TF.hflip(img)\n",
    "                if label is not None:\n",
    "                    # [A, B, C, D, E, F] ‚Üí [D, E, F, A, B, C]\n",
    "                    label = label[[3, 4, 5, 0, 1, 2]]\n",
    "            \n",
    "            # 2. ÏïΩÌïú ÌöåÏ†Ñ (¬±5ÎèÑ)\n",
    "            if random.random() < aug_ratio:\n",
    "                angle = float(torch.empty(1).uniform_(-5, 5))\n",
    "                img = TF.rotate(\n",
    "                    img, \n",
    "                    angle, \n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 3. ÏïΩÌïú Translation\n",
    "            if random.random() < aug_ratio:\n",
    "                max_dx = 0.05 * img_size\n",
    "                max_dy = 0.05 * img_size\n",
    "                translations = (\n",
    "                    float(torch.empty(1).uniform_(-max_dx, max_dx)),\n",
    "                    float(torch.empty(1).uniform_(-max_dy, max_dy))\n",
    "                )\n",
    "                img = TF.affine(\n",
    "                    img,\n",
    "                    angle=0,\n",
    "                    translate=translations,\n",
    "                    scale=1.0,\n",
    "                    shear=0,\n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 4. Brightness & Contrast\n",
    "            if random.random() < aug_ratio:\n",
    "                brightness_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_brightness(img, brightness_factor)\n",
    "            \n",
    "            if random.random() < aug_ratio:\n",
    "                contrast_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_contrast(img, contrast_factor)\n",
    "            \n",
    "            # 5. Gamma Correction\n",
    "            if random.random() < 0.3:\n",
    "                gamma = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_gamma(img, gamma)\n",
    "        \n",
    "        # Tensor Î≥ÄÌôò\n",
    "        img = TF.to_tensor(img)\n",
    "        \n",
    "        # Gaussian Noise (train only)\n",
    "        if train and random.random() < 0.2:\n",
    "            noise = torch.randn_like(img) * 0.01\n",
    "            img = img + noise\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôî\n",
    "        img = TF.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        if label is not None:\n",
    "            return img, label\n",
    "        return img\n",
    "    \n",
    "    return _tfm\n",
    "\n",
    "def load_and_split_brixia(csv_path, val_ratio=0.2, seed=SEED):\n",
    "    df = pd.read_csv(csv_path, dtype={'BrixiaScore': str})\n",
    "    df = df.dropna(subset=['BrixiaScore'])\n",
    "    df = df[df['BrixiaScore'] != 'nan']\n",
    "    df = df[df['BrixiaScore'].str.len() == 6].copy()\n",
    "    \n",
    "    print(f\"Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: {len(df)}Í∞ú\")\n",
    "    \n",
    "    if 'ConsensusTestset' in df.columns:\n",
    "        test_df = df[df['ConsensusTestset'] == 1].copy()\n",
    "        train_val_df = df[df['ConsensusTestset'] == 0].copy()\n",
    "    else:\n",
    "        test_df = pd.DataFrame()\n",
    "        train_val_df = df.copy()\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
    "    train_idx, val_idx = next(gss.split(\n",
    "        train_val_df, \n",
    "        groups=train_val_df['StudyId']\n",
    "    ))\n",
    "    \n",
    "    tr_df = train_val_df.iloc[train_idx].copy()\n",
    "    val_df = train_val_df.iloc[val_idx].copy()\n",
    "    \n",
    "    print(f\"Train: {len(tr_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    validate_split(tr_df, val_df, test_df)\n",
    "    \n",
    "    return tr_df, val_df, test_df\n",
    "\n",
    "def validate_split(tr_df, val_df, tt_df):\n",
    "    train_studies = set(tr_df['StudyId'])\n",
    "    val_studies = set(val_df['StudyId'])\n",
    "    test_studies = set(tt_df['StudyId']) if len(tt_df) > 0 else set()\n",
    "    \n",
    "    assert len(train_studies & val_studies) == 0, \"Train-Val Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(train_studies & test_studies) == 0, \"Train-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(val_studies & test_studies) == 0, \"Val-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    \n",
    "    for name, data in [('Train', tr_df), ('Val', val_df), ('Test', tt_df)]:\n",
    "        if len(data) > 0:\n",
    "            scores = data['BrixiaScore'].apply(lambda x: sum(int(c) for c in x))\n",
    "            print(f\"{name} - Mean: {scores.mean():.2f}, Std: {scores.std():.2f}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class BrixiaDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_col = \"Filename\"\n",
    "        self.label_col = \"BrixiaScore\"\n",
    "        self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        assert self.img_col in self.df.columns\n",
    "        assert self.label_col in self.df.columns\n",
    "        \n",
    "        invalid_scores = self.df[self.df[self.label_col].str.len() != 6]\n",
    "        if len(invalid_scores) > 0:\n",
    "            print(f\"‚ö†Ô∏è Í≤ΩÍ≥†: {len(invalid_scores)}Í∞úÏùò ÏûòÎ™ªÎêú BrixiaScore Î∞úÍ≤¨\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_name_from_csv = row[self.img_col]\n",
    "        img_name = img_name_from_csv.replace('.dcm', '.png')\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïò§Î•ò: {img_path}\")\n",
    "            raise\n",
    "        \n",
    "        scores_str = row[self.label_col]\n",
    "        scores_list = [int(c) for c in scores_str]\n",
    "        labels = torch.tensor(scores_list, dtype=torch.long)  # üîÑ longÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "        \n",
    "        if self.transform:\n",
    "            image, labels = self.transform(image, labels)\n",
    "        else:\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "def create_dataloaders(tr_df, val_df, tt_df, img_dir, \n",
    "                       batch_size=32, img_size=224, num_workers=4):\n",
    "    train_transform = make_transform_with_label(train=True, img_size=img_size)\n",
    "    val_transform = make_transform_with_label(train=False, img_size=img_size)\n",
    "    \n",
    "    tr_ds = BrixiaDataset(tr_df, img_dir, transform=train_transform)\n",
    "    val_ds = BrixiaDataset(val_df, img_dir, transform=val_transform)\n",
    "    tt_ds = BrixiaDataset(tt_df, img_dir, transform=val_transform)\n",
    "    \n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    tt_loader = DataLoader(\n",
    "        tt_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "    print(f\"   Train: {len(tr_ds)} samples, {len(tr_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_ds)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(tt_ds)} samples, {len(tt_loader)} batches\")\n",
    "    \n",
    "    return tr_loader, val_loader, tt_loader\n",
    "\n",
    "# ============================================================\n",
    "# Loss Function - Ordinal Classification\n",
    "# ============================================================\n",
    "def calculate_class_weights(labels, num_classes=4, method='sqrt_inverse'):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    counts = np.bincount(labels_flat.astype(int), minlength=num_classes)\n",
    "    \n",
    "    if method == 'sqrt_inverse':\n",
    "        weights = 1.0 / (np.sqrt(counts) + 1e-6)\n",
    "    elif method == 'inverse':\n",
    "        weights = 1.0 / (counts + 1e-6)\n",
    "    else:\n",
    "        total = len(labels_flat)\n",
    "        weights = total / (num_classes * (counts + 1e-6))\n",
    "    \n",
    "    weights = weights / weights.mean()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def print_class_distribution(labels):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Class Distribution Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    region_names = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    for idx, name in enumerate(region_names):\n",
    "        region_labels = labels[:, idx]\n",
    "        counts = np.bincount(region_labels.astype(int), minlength=4)\n",
    "        total = counts.sum()\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        for cls in range(4):\n",
    "            pct = 100 * counts[cls] / total if total > 0 else 0\n",
    "            bar = '‚ñà' * int(pct / 2)\n",
    "            print(f\"  Class {cls}: {counts[cls]:4d} ({pct:5.1f}%) {bar}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "class OrdinalRegressionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ Ordinal Regression Loss (ÏàúÏÑúÌòï ÌöåÍ∑Ä)\n",
    "    0 < 1 < 2 < 3Ïùò ÏàúÏÑúÎ•º Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌïôÏäµ\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [B*6, num_classes-1] - cumulative logits\n",
    "        target: [B*6] - class labels (0~3)\n",
    "        \"\"\"\n",
    "        # Cumulative labels ÏÉùÏÑ±\n",
    "        # Class 0: [0, 0, 0]\n",
    "        # Class 1: [1, 0, 0]\n",
    "        # Class 2: [1, 1, 0]\n",
    "        # Class 3: [1, 1, 1]\n",
    "        batch_size = target.size(0)\n",
    "        target_expanded = target.unsqueeze(1)  # [B*6, 1]\n",
    "        \n",
    "        # [0, 1, 2, ..., num_classes-2]\n",
    "        thresholds = torch.arange(self.num_classes - 1).to(target.device)\n",
    "        thresholds = thresholds.unsqueeze(0).expand(batch_size, -1)  # [B*6, 3]\n",
    "        \n",
    "        # target > thresholdÏù¥Î©¥ 1, ÏïÑÎãàÎ©¥ 0\n",
    "        cumulative_target = (target_expanded > thresholds).float()  # [B*6, 3]\n",
    "        \n",
    "        # Binary cross entropy for each threshold\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, cumulative_target, reduction='none'\n",
    "        )\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "class AdaptiveOrdinalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ Ordinal Loss + Class Weights + Part Weights\n",
    "    ÏàúÏÑúÎ•º Í≥†Î†§ÌïòÎ©¥ÏÑú Î∂àÍ∑†ÌòïÎèÑ Ìï¥Í≤∞\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_labels, num_classes=4, use_class_weights=True, \n",
    "                 part_weights=None, ordinal_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ordinal_weight = ordinal_weight  # Ordinal lossÏôÄ CE lossÏùò ÎπÑÏú®\n",
    "        \n",
    "        if isinstance(train_labels, torch.Tensor):\n",
    "            train_labels_np = train_labels.cpu().numpy()\n",
    "        else:\n",
    "            train_labels_np = train_labels\n",
    "        \n",
    "        print_class_distribution(train_labels_np)\n",
    "        \n",
    "        # ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "        if use_class_weights:\n",
    "            class_weights = calculate_class_weights(train_labels_np, num_classes=num_classes)\n",
    "            self.register_buffer('class_weights', class_weights)\n",
    "            print(f\"‚úÖ Class weights: {class_weights.numpy()}\")\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò\n",
    "        if part_weights is None:\n",
    "            self.part_weights = torch.ones(6)\n",
    "        else:\n",
    "            self.part_weights = torch.tensor(part_weights, dtype=torch.float32)\n",
    "        self.register_buffer('part_weights_buf', self.part_weights)\n",
    "        print(f\"‚úÖ Part weights: {self.part_weights.numpy()}\")\n",
    "        print(f\"‚úÖ Ordinal weight: {ordinal_weight}\")\n",
    "        \n",
    "        # Ordinal loss\n",
    "        self.ordinal_loss = OrdinalRegressionLoss(num_classes)\n",
    "\n",
    "    def forward(self, pred_dict, target, use_mixup=False):\n",
    "        \"\"\"\n",
    "        pred_dict: {'logits': [B, 6, 4], 'ordinal_logits': [B, 6, 3]}\n",
    "        target: [B, 6]\n",
    "        \"\"\"\n",
    "        pred_logits = pred_dict['logits']  # [B, 6, 4]\n",
    "        ordinal_logits = pred_dict['ordinal_logits']  # [B, 6, 3]\n",
    "        \n",
    "        B, num_regions, num_classes = pred_logits.shape\n",
    "        \n",
    "        # Reshape\n",
    "        pred_logits_flat = pred_logits.view(B * num_regions, num_classes)\n",
    "        ordinal_logits_flat = ordinal_logits.view(B * num_regions, num_classes - 1)\n",
    "        target_flat = target.view(B * num_regions)\n",
    "        \n",
    "        # 1. CrossEntropyLoss (Í∏∞Î≥∏ Î∂ÑÎ•ò)\n",
    "        if self.class_weights is not None and not use_mixup:\n",
    "            ce_loss = F.cross_entropy(\n",
    "                pred_logits_flat, \n",
    "                target_flat,\n",
    "                weight=self.class_weights.to(pred_logits.device),\n",
    "                reduction='none'\n",
    "            )\n",
    "        else:\n",
    "            ce_loss = F.cross_entropy(\n",
    "                pred_logits_flat, \n",
    "                target_flat,\n",
    "                reduction='none'\n",
    "            )\n",
    "        \n",
    "        # 2. Ordinal Loss (ÏàúÏÑú ÌïôÏäµ)\n",
    "        ord_loss = self.ordinal_loss(ordinal_logits_flat, target_flat)\n",
    "        \n",
    "        # 3. Í≤∞Ìï©\n",
    "        total_loss_flat = (1 - self.ordinal_weight) * ce_loss + self.ordinal_weight * ord_loss\n",
    "        total_loss = total_loss_flat.view(B, num_regions)\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "        part_weights = self.part_weights_buf.to(pred_logits.device)\n",
    "        total_loss = total_loss * part_weights.unsqueeze(0)\n",
    "        \n",
    "        part_losses = total_loss.mean(dim=0)\n",
    "        \n",
    "        return total_loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Mixup (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "def mixup_data_classification(x, y, alpha=MIXUP_ALPHA):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•òÏö© Mixup\n",
    "    yÎäî one-hotÏúºÎ°ú Î≥ÄÌôò ÌõÑ mixup\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    \n",
    "    # One-hot encoding\n",
    "    y_onehot = F.one_hot(y, num_classes=NUM_CLASSES).float()  # [B, 6, 4]\n",
    "    y_onehot_shuffled = y_onehot[index]\n",
    "    \n",
    "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot_shuffled  # [B, 6, 4]\n",
    "    \n",
    "    return mixed_x, mixed_y, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred_dict, y_mixed, lam):\n",
    "    \"\"\"MixupÏùÑ ÏúÑÌïú ÏÜêÏã§ Í≥ÑÏÇ∞ (Ordinal Loss ÏßÄÏõê)\"\"\"\n",
    "    pred_logits = pred_dict['logits']\n",
    "    ordinal_logits = pred_dict['ordinal_logits']\n",
    "    \n",
    "    B, num_regions, num_classes = pred_logits.shape\n",
    "    \n",
    "    # Reshape\n",
    "    pred_flat = pred_logits.view(B * num_regions, num_classes)\n",
    "    ordinal_flat = ordinal_logits.view(B * num_regions, num_classes - 1)\n",
    "    target_flat = y_mixed.view(B * num_regions, num_classes)\n",
    "    \n",
    "    # Soft target loss (CE part)\n",
    "    log_probs = F.log_softmax(pred_flat, dim=1)\n",
    "    ce_loss = -(target_flat * log_probs).sum(dim=1)\n",
    "    \n",
    "    # Ordinal partÎäî mixupÏóêÏÑú skip (hard labelÎßå ÏÇ¨Ïö©)\n",
    "    loss = ce_loss  # Mixup ÏãúÏóêÎäî ordinal loss Ï†úÏô∏\n",
    "    loss = loss.view(B, num_regions)\n",
    "    \n",
    "    # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "    if hasattr(criterion, 'part_weights_buf'):\n",
    "        part_weights = criterion.part_weights_buf.to(pred_logits.device)\n",
    "        loss = loss * part_weights.unsqueeze(0)\n",
    "    \n",
    "    part_losses = loss.mean(dim=0)\n",
    "    return loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Metrics (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def calculate_classification_metrics(pred_dict, labels):\n",
    "    \"\"\"\n",
    "    ‚úÖ Î∂ÑÎ•ò ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    pred_dict: {'logits': [B, 6, 4], 'ordinal_logits': [B, 6, 3]}\n",
    "    labels: [B, 6]\n",
    "    \"\"\"\n",
    "    pred_logits = pred_dict['logits']\n",
    "    \n",
    "    # ÏòàÏ∏° ÌÅ¥ÎûòÏä§\n",
    "    preds = pred_logits.argmax(dim=-1)  # [B, 6]\n",
    "    \n",
    "    # Exact match accuracy\n",
    "    exact_acc = (preds == labels).float().mean().item()\n",
    "    \n",
    "    # Off-by-1 accuracy (Ïù∏Ï†ë ÌÅ¥ÎûòÏä§ ÌóàÏö©)\n",
    "    off_by_1 = (torch.abs(preds - labels) <= 1).float().mean().item()\n",
    "    \n",
    "    # Per-region accuracy\n",
    "    region_acc = (preds == labels).float().mean(dim=0)  # [6]\n",
    "    \n",
    "    # MAE (Ï∞∏Í≥†Ïö©)\n",
    "    mae = torch.abs(preds.float() - labels.float()).mean().item()\n",
    "    \n",
    "    return exact_acc, off_by_1, mae, region_acc\n",
    "\n",
    "# ============================================================\n",
    "# Model (Î∂ÑÎ•òÏö©ÏúºÎ°ú Î≥ÄÍ≤Ω)\n",
    "# ============================================================\n",
    "class EfficientNetB0Classification(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ Ordinal Classification Î™®Îç∏\n",
    "    - Í∞Å Î∂ÄÏúÑ(A~F)ÎßàÎã§ 4Í∞ú ÌÅ¥ÎûòÏä§(0~3) ÏòàÏ∏°\n",
    "    - Self-AttentionÏúºÎ°ú Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ\n",
    "    - Ordinal Regression Head Ï∂îÍ∞Ä (ÏàúÏÑú ÌïôÏäµ)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True, drop=0.3, num_regions=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        weights = EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        backbone = efficientnet_b0(weights=weights)\n",
    "        \n",
    "        self.features = backbone.features\n",
    "        in_feat = 1280\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 49, in_feat))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Region queries\n",
    "        self.region_queries = nn.Parameter(torch.randn(num_regions, in_feat))\n",
    "        nn.init.xavier_uniform_(self.region_queries)\n",
    "        \n",
    "        # Cross attention (Ïù¥ÎØ∏ÏßÄ ÌäπÏßï ‚Üí Î∂ÄÏúÑÎ≥Ñ ÌäπÏßï)\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # Self-Attention (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ)\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(in_feat)\n",
    "        self.norm3 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_feat * 2, in_feat),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        \n",
    "        self.norm4 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(in_feat, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        \n",
    "        # ‚úÖ Classification heads (ÏùºÎ∞ò Î∂ÑÎ•ò)\n",
    "        self.classification_heads = nn.ModuleList([\n",
    "            nn.Linear(128, num_classes) for _ in range(num_regions)\n",
    "        ])\n",
    "        \n",
    "        # ‚úÖ Ordinal Regression heads (ÏàúÏÑúÌòï ÌöåÍ∑Ä)\n",
    "        # num_classes-1 Í∞úÏùò thresholdÎ•º ÌïôÏäµ\n",
    "        self.ordinal_heads = nn.ModuleList([\n",
    "            nn.Linear(128, num_classes - 1) for _ in range(num_regions)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # Feature extraction\n",
    "        feat = self.features(x)  # [B, 1280, 7, 7]\n",
    "        feat = feat.flatten(2).transpose(1, 2)  # [B, 49, 1280]\n",
    "        feat = feat + self.pos_embed\n",
    "        \n",
    "        # Region queries\n",
    "        queries = self.region_queries.unsqueeze(0).expand(B, -1, -1)  # [B, 6, 1280]\n",
    "        \n",
    "        # Cross attention (Ïù¥ÎØ∏ÏßÄ ‚Üí Î∂ÄÏúÑ)\n",
    "        attn_out, _ = self.cross_attention(\n",
    "            query=queries,\n",
    "            key=feat,\n",
    "            value=feat\n",
    "        )\n",
    "        attn_out = self.norm1(attn_out + queries)  # [B, 6, 1280]\n",
    "        \n",
    "        # Self-Attention (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ)\n",
    "        self_attn_out, attn_weights = self.self_attention(\n",
    "            query=attn_out,\n",
    "            key=attn_out,\n",
    "            value=attn_out\n",
    "        )\n",
    "        attn_out = self.norm2(attn_out + self_attn_out)  # [B, 6, 1280]\n",
    "        \n",
    "        # FFN\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        attn_out = self.norm3(attn_out + ffn_out)  # [B, 6, 1280]\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        shared_features = self.shared_fc(attn_out)  # [B, 6, 128]\n",
    "        \n",
    "        # ‚úÖ Í∞Å Î∂ÄÏúÑÎ≥Ñ ÏòàÏ∏° (Classification + Ordinal)\n",
    "        classification_outputs = []\n",
    "        ordinal_outputs = []\n",
    "        \n",
    "        for i in range(len(self.classification_heads)):\n",
    "            region_feat = shared_features[:, i, :]  # [B, 128]\n",
    "            \n",
    "            # Classification logits\n",
    "            class_logits = self.classification_heads[i](region_feat)  # [B, 4]\n",
    "            classification_outputs.append(class_logits)\n",
    "            \n",
    "            # Ordinal logits (cumulative)\n",
    "            ordinal_logits = self.ordinal_heads[i](region_feat)  # [B, 3]\n",
    "            ordinal_outputs.append(ordinal_logits)\n",
    "        \n",
    "        class_out = torch.stack(classification_outputs, dim=1)  # [B, 6, 4]\n",
    "        ordinal_out = torch.stack(ordinal_outputs, dim=1)  # [B, 6, 3]\n",
    "        \n",
    "        return {\n",
    "            'logits': class_out,\n",
    "            'ordinal_logits': ordinal_out,\n",
    "            'attn_weights': attn_weights\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# Training Functions (Î∂ÑÎ•òÏö© ÏàòÏ†ï)\n",
    "# ============================================================\n",
    "def train_epoch(model, tr_loader, criterion, optimizer, scaler, device, \n",
    "                amp=True, use_mixup=True):\n",
    "    model.train()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(tr_loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)  # [B, 6]\n",
    "        \n",
    "        # Mixup Ï†ÅÏö©\n",
    "        is_mixup = use_mixup and (random.random() < 0.5)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast(enabled=amp):\n",
    "            if is_mixup:\n",
    "                imgs_mixed, labels_mixed, lam = mixup_data_classification(imgs, labels)\n",
    "                pred_dict = model(imgs_mixed)\n",
    "                loss, part_losses = mixup_criterion(criterion, pred_dict, labels_mixed, lam)\n",
    "            else:\n",
    "                pred_dict = model(imgs)\n",
    "                loss, part_losses = criterion(pred_dict, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Metrics (ÏõêÎ≥∏ labelsÎ°ú Í≥ÑÏÇ∞)\n",
    "        exact_acc, off_by_1, mae, _ = calculate_classification_metrics(pred_dict, labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\",\n",
    "            mae=f\"{run_mae/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    \n",
    "    return run_loss/n, run_acc/n, run_off1/n, run_mae/n, part_losses_avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, split='val'):\n",
    "    model.eval()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    region_acc_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"{split.capitalize()}\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        pred_dict = model(imgs)\n",
    "        loss, part_losses = criterion(pred_dict, labels)\n",
    "        \n",
    "        exact_acc, off_by_1, mae, region_acc = calculate_classification_metrics(pred_dict, labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        region_acc_sum += region_acc.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    avg_loss = run_loss/n\n",
    "    avg_acc = run_acc/n\n",
    "    avg_off1 = run_off1/n\n",
    "    avg_mae = run_mae/n\n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    region_acc_avg = region_acc_sum / n\n",
    "    \n",
    "    print(f\"[{split}] loss:{avg_loss:.4f} acc:{avg_acc:.4f} \"\n",
    "          f\"off1:{avg_off1:.4f} mae:{avg_mae:.4f}\")\n",
    "    print(f\"  Region Acc: {region_acc_avg.numpy().round(3)}\")\n",
    "    \n",
    "    return avg_loss, avg_acc, avg_off1, avg_mae, part_losses_avg, region_acc_avg\n",
    "\n",
    "def get_lrs(optimizer):\n",
    "    return [pg['lr'] for pg in optimizer.param_groups]\n",
    "\n",
    "# ============================================================\n",
    "# Main Function\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ Brixia COVID-19 Ordinal Classification Training\")\n",
    "    print(\"   üí° 80% Î™©ÌëúÎ•º ÏúÑÌïú ÌïµÏã¨ Í∏∞Ïà†:\")\n",
    "    print(\"   ‚úÖ Ordinal Loss - ÏàúÏÑúÌòï ÌöåÍ∑Ä (0<1<2<3)\")\n",
    "    print(\"   ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ (ÏûêÎèô Í∞ÄÏ§ëÏπò)\")\n",
    "    print(\"   ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ (Self-Attention)\")\n",
    "    print(\"   ‚úÖ Mixed Precision Training (AMP)\")\n",
    "    print(\"   ‚úÖ Mixup Augmentation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "    print(\"\\nüìÇ Loading data...\")\n",
    "    tr_df, val_df, tt_df = load_and_split_brixia(CSV_PATH)\n",
    "    \n",
    "    print(\"\\nüì¶ Creating DataLoaders...\")\n",
    "    tr_loader, val_loader, tt_loader = create_dataloaders(\n",
    "        tr_df, val_df, tt_df, img_dir=IMAGE_DIR, \n",
    "        batch_size=BATCH_SIZE, img_size=IMG_SIZE, num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Train labels Ï∂îÏ∂ú\n",
    "    train_labels = torch.cat([labels for _, labels in tr_loader], dim=0)\n",
    "    \n",
    "    # ========================================\n",
    "    # ÌïôÏäµ Ï§ÄÎπÑ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìç Training Setup\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ‚úÖ Ordinal Loss + ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "    criterion = AdaptiveOrdinalLoss(\n",
    "        train_labels, \n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_class_weights=True,\n",
    "        part_weights=None,\n",
    "        ordinal_weight=0.5  # CE:Ordinal = 50:50\n",
    "    )\n",
    "    \n",
    "    # ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥ÑÎ•º Í≥†Î†§ÌïòÎäî Î™®Îç∏\n",
    "    model = EfficientNetB0Classification(\n",
    "        pretrained=True, \n",
    "        drop=DROP_RATIO,\n",
    "        num_regions=6,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    scaler = GradScaler(enabled=AMP)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_mae = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 15\n",
    "    \n",
    "    # ========================================\n",
    "    # ÌïôÏäµ Î£®ÌîÑ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèãÔ∏è Training Start\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        tr_loss, tr_acc, tr_off1, tr_mae, tr_part_losses = train_epoch(\n",
    "            model, tr_loader, criterion, optimizer, scaler, DEVICE, AMP\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_off1, val_mae, val_part_losses, val_region_acc = evaluate(\n",
    "            model, val_loader, criterion, DEVICE, split='val'\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_mae = val_mae\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': best_acc,\n",
    "                'val_mae': best_mae,\n",
    "                'val_off1': val_off1,\n",
    "                'region_acc': val_region_acc,\n",
    "            }, BEST_PATH)\n",
    "            print(f\"‚úÖ New Best! (Acc={best_acc:.4f}, MAE={best_mae:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping at epoch {ep}\")\n",
    "            break\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"\\n[Epoch {ep:03d}/{EPOCHS}]\")\n",
    "        print(f\"  Train - loss:{tr_loss:.4f} acc:{tr_acc:.4f} off1:{tr_off1:.4f} mae:{tr_mae:.4f}\")\n",
    "        print(f\"  Val   - loss:{val_loss:.4f} acc:{val_acc:.4f} off1:{val_off1:.4f} mae:{val_mae:.4f}\")\n",
    "        print(f\"  Part losses (Val): {val_part_losses.numpy().round(3)}\")\n",
    "        print(f\"  Region Acc (Val): {val_region_acc.numpy().round(3)}\")\n",
    "        print(f\"  LR:{get_lrs(optimizer)[0]:.2e} | {elapsed:.1f}s | Patience:{patience_counter}/{max_patience}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # ========================================\n",
    "    # Test Evaluation\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ Training Finished!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(tt_loader) > 0:\n",
    "        print(\"\\nüìä Test evaluation with best model...\")\n",
    "        checkpoint = torch.load(BEST_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        tt_loss, tt_acc, tt_off1, tt_mae, tt_part_losses, tt_region_acc = evaluate(\n",
    "            model, tt_loader, criterion, DEVICE, split='test'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüèÜ Test Results:\")\n",
    "        print(f\"   Accuracy: {tt_acc:.4f}\")\n",
    "        print(f\"   Off-by-1: {tt_off1:.4f}\")\n",
    "        print(f\"   MAE: {tt_mae:.4f}\")\n",
    "        print(f\"   Region Acc: {tt_region_acc.numpy().round(3)}\")\n",
    "        print(f\"   Part losses: {tt_part_losses.numpy().round(3)}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Best model saved: {BEST_PATH}\")\n",
    "    print(f\"üìà Best Validation Accuracy: {best_acc:.4f}\")\n",
    "    print(f\"üìâ Best Validation MAE: {best_mae:.4f}\")\n",
    "    \n",
    "    # ÏÑ±Îä• Ìñ•ÏÉÅ Ï†úÏïà\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üí° Ï∂îÍ∞Ä ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú Ï†úÏïà:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"1. üéØ ÏïôÏÉÅÎ∏î: Îã§Î•∏ ÏãúÎìúÎ°ú 3~5Í∞ú Î™®Îç∏ ÌïôÏäµ ÌõÑ Ìà¨Ìëú\")\n",
    "    print(\"2. üî¨ ÏùòÎ£å Ï†ÑÏö© pretrained model ÏÇ¨Ïö©:\")\n",
    "    print(\"   - CheXpert, MIMIC-CXR Îì±ÏúºÎ°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏\")\n",
    "    print(\"3. üìä Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä:\")\n",
    "    print(\"   - Ïô∏Î∂Ä COVID-19 Îç∞Ïù¥ÌÑ∞ÏÖã ÌôúÏö©\")\n",
    "    print(\"   - Pseudo-labelingÏúºÎ°ú unlabeled Îç∞Ïù¥ÌÑ∞ ÌôúÏö©\")\n",
    "    print(\"4. üé® Í≥†Í∏â Ï¶ùÍ∞ï:\")\n",
    "    print(\"   - CutMix, AugMix, RandAugment\")\n",
    "    print(\"   - Test-Time Augmentation (TTA)\")\n",
    "    print(\"5. üß™ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù:\")\n",
    "    print(\"   - ordinal_weight Ï°∞Ï†ï (0.3~0.7)\")\n",
    "    print(\"   - Learning rate, batch size Ïã§Ìóò\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚úÖ Ïù¥Ï†ú gradcam_inference.pyÎ•º Ïã§ÌñâÌïòÏó¨ Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÏÑ∏Ïöî!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44d769d-81d3-43cd-b32e-ff2942c22177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ Brixia COVID-19 Ordinal Classification Training\n",
      "   üè• DenseNet121-MIMIC (ÏùòÎ£å ÏòÅÏÉÅ Ï†ÑÏö© ÏÇ¨Ï†ÑÌïôÏäµ!)\n",
      "   üí° 80% Î™©ÌëúÎ•º ÏúÑÌïú ÌïµÏã¨ Í∏∞Ïà†:\n",
      "   ‚úÖ MIMIC-CXR ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ (Í∞ÄÏû• Í∞ïÎ†•!)\n",
      "   ‚úÖ Ordinal Loss - ÏàúÏÑúÌòï ÌöåÍ∑Ä (0<1<2<3)\n",
      "   ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ (ÏûêÎèô Í∞ÄÏ§ëÏπò)\n",
      "   ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ (Self-Attention)\n",
      "   ‚úÖ Mixed Precision Training (AMP)\n",
      "   ‚úÖ Mixup Augmentation\n",
      "======================================================================\n",
      "\n",
      "üìÇ Loading data...\n",
      "Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: 4695Í∞ú\n",
      "Train: 3637, Val: 912, Test: 146\n",
      "Train - Mean: 8.31, Std: 4.26\n",
      "Val - Mean: 8.35, Std: 4.15\n",
      "Test - Mean: 7.78, Std: 4.20\n",
      "\n",
      "üì¶ Creating DataLoaders...\n",
      "‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\n",
      "   Train: 3637 samples, 113 batches\n",
      "   Val:   912 samples, 29 batches\n",
      "   Test:  146 samples, 5 batches\n",
      "\n",
      "======================================================================\n",
      "üìç Training Setup\n",
      "======================================================================\n",
      "============================================================\n",
      "Class Distribution Analysis\n",
      "============================================================\n",
      "\n",
      "A:\n",
      "  Class 0: 1810 ( 50.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1126 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  446 ( 12.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  234 (  6.5%) ‚ñà‚ñà‚ñà\n",
      "\n",
      "B:\n",
      "  Class 0:  721 ( 19.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  949 ( 26.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1171 ( 32.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  775 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "C:\n",
      "  Class 0:  406 ( 11.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  783 ( 21.7%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1300 ( 36.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1127 ( 31.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "D:\n",
      "  Class 0: 1804 ( 49.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1: 1126 ( 31.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2:  476 ( 13.2%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  210 (  5.8%) ‚ñà‚ñà\n",
      "\n",
      "E:\n",
      "  Class 0:  762 ( 21.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  963 ( 26.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1118 ( 30.9%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3:  773 ( 21.4%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "F:\n",
      "  Class 0:  407 ( 11.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 1:  795 ( 22.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 2: 1306 ( 36.1%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Class 3: 1108 ( 30.6%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "============================================================\n",
      "‚úÖ Class weights: [0.9512662  0.96508205 0.9588403  1.1248114 ]\n",
      "‚úÖ Part weights: [1. 1. 1. 1. 1. 1.]\n",
      "‚úÖ Ordinal weight: 0.5\n",
      "Downloading weights...\n",
      "If this fails you can run `wget https://github.com/mlmed/torchxrayvision/releases/download/v1/chex-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt -O /root/.torchxrayvision/models_data/chex-densenet121-d121-tw-lr001-rot45-tr15-sc15-seed0-best.pt`\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "‚úÖ Loaded DenseNet121 pretrained on CXR!\n",
      "   Parameters: 20,293,180\n",
      "\n",
      "======================================================================\n",
      "üèãÔ∏è Training Start\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.8672 acc:0.4346 off1:0.8299 mae:0.7577\n",
      "  Region Acc: [0.471 0.373 0.404 0.578 0.374 0.408]\n",
      "‚úÖ New Best! (Acc=0.4346, MAE=0.7577)\n",
      "\n",
      "[Epoch 001/100]\n",
      "  Train - loss:1.0916 acc:0.3780 off1:0.8085 mae:0.8496\n",
      "  Val   - loss:0.8672 acc:0.4346 off1:0.8299 mae:0.7577\n",
      "  Part losses (Val): [0.847 0.924 0.884 0.753 0.911 0.884]\n",
      "  Region Acc (Val): [0.471 0.373 0.404 0.578 0.374 0.408]\n",
      "  LR:1.00e-04 | 20.3s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7997 acc:0.4713 off1:0.8628 mae:0.6838\n",
      "  Region Acc: [0.525 0.407 0.416 0.607 0.414 0.458]\n",
      "‚úÖ New Best! (Acc=0.4713, MAE=0.6838)\n",
      "\n",
      "[Epoch 002/100]\n",
      "  Train - loss:1.0302 acc:0.4215 off1:0.8358 mae:0.7681\n",
      "  Val   - loss:0.7997 acc:0.4713 off1:0.8628 mae:0.6838\n",
      "  Part losses (Val): [0.774 0.837 0.839 0.695 0.829 0.824]\n",
      "  Region Acc (Val): [0.525 0.407 0.416 0.607 0.414 0.458]\n",
      "  LR:1.00e-04 | 18.3s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7686 acc:0.4839 off1:0.8792 mae:0.6488\n",
      "  Region Acc: [0.532 0.445 0.438 0.614 0.419 0.456]\n",
      "‚úÖ New Best! (Acc=0.4839, MAE=0.6488)\n",
      "\n",
      "[Epoch 003/100]\n",
      "  Train - loss:0.9762 acc:0.4330 off1:0.8454 mae:0.7468\n",
      "  Val   - loss:0.7686 acc:0.4839 off1:0.8792 mae:0.6488\n",
      "  Part losses (Val): [0.749 0.791 0.806 0.675 0.793 0.798]\n",
      "  Region Acc (Val): [0.532 0.445 0.438 0.614 0.419 0.456]\n",
      "  LR:1.00e-04 | 17.7s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7622 acc:0.4978 off1:0.8867 mae:0.6232\n",
      "  Region Acc: [0.544 0.467 0.452 0.612 0.452 0.461]\n",
      "‚úÖ New Best! (Acc=0.4978, MAE=0.6232)\n",
      "\n",
      "[Epoch 004/100]\n",
      "  Train - loss:0.9641 acc:0.4335 off1:0.8455 mae:0.7491\n",
      "  Val   - loss:0.7622 acc:0.4978 off1:0.8867 mae:0.6232\n",
      "  Part losses (Val): [0.739 0.775 0.798 0.682 0.787 0.793]\n",
      "  Region Acc (Val): [0.544 0.467 0.452 0.612 0.452 0.461]\n",
      "  LR:1.00e-04 | 19.1s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7375 acc:0.5042 off1:0.9115 mae:0.5903\n",
      "  Region Acc: [0.558 0.479 0.463 0.598 0.455 0.473]\n",
      "‚úÖ New Best! (Acc=0.5042, MAE=0.5903)\n",
      "\n",
      "[Epoch 005/100]\n",
      "  Train - loss:0.9621 acc:0.4500 off1:0.8544 mae:0.7192\n",
      "  Val   - loss:0.7375 acc:0.5042 off1:0.9115 mae:0.5903\n",
      "  Part losses (Val): [0.716 0.756 0.767 0.656 0.759 0.77 ]\n",
      "  Region Acc (Val): [0.558 0.479 0.463 0.598 0.455 0.473]\n",
      "  LR:1.00e-04 | 19.6s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7404 acc:0.5031 off1:0.9110 mae:0.5932\n",
      "  Region Acc: [0.534 0.507 0.461 0.612 0.477 0.429]\n",
      "\n",
      "[Epoch 006/100]\n",
      "  Train - loss:0.9336 acc:0.4463 off1:0.8519 mae:0.7278\n",
      "  Val   - loss:0.7404 acc:0.5031 off1:0.9110 mae:0.5932\n",
      "  Part losses (Val): [0.728 0.75  0.763 0.646 0.769 0.785]\n",
      "  Region Acc (Val): [0.534 0.507 0.461 0.612 0.477 0.429]\n",
      "  LR:1.00e-04 | 17.6s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7318 acc:0.5106 off1:0.9097 mae:0.5844\n",
      "  Region Acc: [0.552 0.505 0.443 0.626 0.485 0.453]\n",
      "‚úÖ New Best! (Acc=0.5106, MAE=0.5844)\n",
      "\n",
      "[Epoch 007/100]\n",
      "  Train - loss:0.9174 acc:0.4612 off1:0.8642 mae:0.6981\n",
      "  Val   - loss:0.7318 acc:0.5106 off1:0.9097 mae:0.5844\n",
      "  Part losses (Val): [0.705 0.743 0.766 0.649 0.753 0.775]\n",
      "  Region Acc (Val): [0.552 0.505 0.443 0.626 0.485 0.453]\n",
      "  LR:1.00e-04 | 19.9s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7085 acc:0.5302 off1:0.9225 mae:0.5545\n",
      "  Region Acc: [0.576 0.509 0.473 0.627 0.523 0.474]\n",
      "‚úÖ New Best! (Acc=0.5302, MAE=0.5545)\n",
      "\n",
      "[Epoch 008/100]\n",
      "  Train - loss:0.8942 acc:0.4646 off1:0.8681 mae:0.6881\n",
      "  Val   - loss:0.7085 acc:0.5302 off1:0.9225 mae:0.5545\n",
      "  Part losses (Val): [0.688 0.725 0.731 0.621 0.729 0.757]\n",
      "  Region Acc (Val): [0.576 0.509 0.473 0.627 0.523 0.474]\n",
      "  LR:1.00e-04 | 17.6s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6987 acc:0.5369 off1:0.9229 mae:0.5473\n",
      "  Region Acc: [0.555 0.522 0.511 0.615 0.513 0.505]\n",
      "‚úÖ New Best! (Acc=0.5369, MAE=0.5473)\n",
      "\n",
      "[Epoch 009/100]\n",
      "  Train - loss:0.8811 acc:0.4742 off1:0.8773 mae:0.6678\n",
      "  Val   - loss:0.6987 acc:0.5369 off1:0.9229 mae:0.5473\n",
      "  Part losses (Val): [0.693 0.713 0.716 0.614 0.708 0.748]\n",
      "  Region Acc (Val): [0.555 0.522 0.511 0.615 0.513 0.505]\n",
      "  LR:1.00e-04 | 19.0s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7003 acc:0.5292 off1:0.9154 mae:0.5621\n",
      "  Region Acc: [0.539 0.522 0.495 0.623 0.508 0.489]\n",
      "\n",
      "[Epoch 010/100]\n",
      "  Train - loss:0.9018 acc:0.4827 off1:0.8844 mae:0.6508\n",
      "  Val   - loss:0.7003 acc:0.5292 off1:0.9154 mae:0.5621\n",
      "  Part losses (Val): [0.688 0.714 0.725 0.613 0.718 0.744]\n",
      "  Region Acc (Val): [0.539 0.522 0.495 0.623 0.508 0.489]\n",
      "  LR:1.00e-04 | 17.9s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7023 acc:0.5289 off1:0.9165 mae:0.5610\n",
      "  Region Acc: [0.525 0.482 0.516 0.627 0.526 0.496]\n",
      "\n",
      "[Epoch 011/100]\n",
      "  Train - loss:0.8968 acc:0.4798 off1:0.8806 mae:0.6574\n",
      "  Val   - loss:0.7023 acc:0.5289 off1:0.9165 mae:0.5610\n",
      "  Part losses (Val): [0.711 0.726 0.712 0.61  0.713 0.742]\n",
      "  Region Acc (Val): [0.525 0.482 0.516 0.627 0.526 0.496]\n",
      "  LR:1.00e-04 | 17.9s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6857 acc:0.5453 off1:0.9282 mae:0.5322\n",
      "  Region Acc: [0.573 0.518 0.522 0.632 0.527 0.5  ]\n",
      "‚úÖ New Best! (Acc=0.5453, MAE=0.5322)\n",
      "\n",
      "[Epoch 012/100]\n",
      "  Train - loss:0.8887 acc:0.4794 off1:0.8793 mae:0.6607\n",
      "  Val   - loss:0.6857 acc:0.5453 off1:0.9282 mae:0.5322\n",
      "  Part losses (Val): [0.676 0.704 0.699 0.598 0.704 0.734]\n",
      "  Region Acc (Val): [0.573 0.518 0.522 0.632 0.527 0.5  ]\n",
      "  LR:1.00e-04 | 17.8s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6973 acc:0.5307 off1:0.9346 mae:0.5389\n",
      "  Region Acc: [0.541 0.512 0.503 0.629 0.501 0.498]\n",
      "\n",
      "[Epoch 013/100]\n",
      "  Train - loss:0.8933 acc:0.4809 off1:0.8812 mae:0.6563\n",
      "  Val   - loss:0.6973 acc:0.5307 off1:0.9346 mae:0.5389\n",
      "  Part losses (Val): [0.686 0.717 0.727 0.604 0.714 0.737]\n",
      "  Region Acc (Val): [0.541 0.512 0.503 0.629 0.501 0.498]\n",
      "  LR:1.00e-04 | 18.8s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6844 acc:0.5406 off1:0.9360 mae:0.5272\n",
      "  Region Acc: [0.572 0.526 0.52  0.605 0.532 0.488]\n",
      "\n",
      "[Epoch 014/100]\n",
      "  Train - loss:0.8470 acc:0.5058 off1:0.8990 mae:0.6087\n",
      "  Val   - loss:0.6844 acc:0.5406 off1:0.9360 mae:0.5272\n",
      "  Part losses (Val): [0.665 0.698 0.704 0.607 0.699 0.734]\n",
      "  Region Acc (Val): [0.572 0.526 0.52  0.605 0.532 0.488]\n",
      "  LR:1.00e-04 | 17.4s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6767 acc:0.5450 off1:0.9304 mae:0.5291\n",
      "  Region Acc: [0.576 0.513 0.514 0.635 0.533 0.499]\n",
      "\n",
      "[Epoch 015/100]\n",
      "  Train - loss:0.8611 acc:0.4864 off1:0.8817 mae:0.6524\n",
      "  Val   - loss:0.6767 acc:0.5450 off1:0.9304 mae:0.5291\n",
      "  Part losses (Val): [0.652 0.696 0.699 0.593 0.687 0.732]\n",
      "  Region Acc (Val): [0.576 0.513 0.514 0.635 0.533 0.499]\n",
      "  LR:1.00e-04 | 18.8s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.7055 acc:0.5320 off1:0.9167 mae:0.5585\n",
      "  Region Acc: [0.555 0.484 0.515 0.623 0.526 0.489]\n",
      "\n",
      "[Epoch 016/100]\n",
      "  Train - loss:0.8567 acc:0.4936 off1:0.8904 mae:0.6340\n",
      "  Val   - loss:0.7055 acc:0.5320 off1:0.9167 mae:0.5585\n",
      "  Part losses (Val): [0.713 0.743 0.705 0.611 0.717 0.744]\n",
      "  Region Acc (Val): [0.555 0.484 0.515 0.623 0.526 0.489]\n",
      "  LR:1.00e-04 | 17.4s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6831 acc:0.5376 off1:0.9346 mae:0.5323\n",
      "  Region Acc: [0.577 0.499 0.515 0.618 0.527 0.489]\n",
      "\n",
      "[Epoch 017/100]\n",
      "  Train - loss:0.8588 acc:0.4971 off1:0.8942 mae:0.6244\n",
      "  Val   - loss:0.6831 acc:0.5376 off1:0.9346 mae:0.5323\n",
      "  Part losses (Val): [0.664 0.712 0.7   0.594 0.7   0.728]\n",
      "  Region Acc (Val): [0.577 0.499 0.515 0.618 0.527 0.489]\n",
      "  LR:1.00e-04 | 18.6s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6910 acc:0.5444 off1:0.9287 mae:0.5325\n",
      "  Region Acc: [0.576 0.523 0.523 0.624 0.527 0.493]\n",
      "\n",
      "[Epoch 018/100]\n",
      "  Train - loss:0.8501 acc:0.4913 off1:0.8891 mae:0.6382\n",
      "  Val   - loss:0.6910 acc:0.5444 off1:0.9287 mae:0.5325\n",
      "  Part losses (Val): [0.696 0.711 0.694 0.599 0.708 0.737]\n",
      "  Region Acc (Val): [0.576 0.523 0.523 0.624 0.527 0.493]\n",
      "  LR:5.00e-05 | 17.5s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6776 acc:0.5457 off1:0.9351 mae:0.5236\n",
      "  Region Acc: [0.577 0.518 0.537 0.627 0.521 0.495]\n",
      "‚úÖ New Best! (Acc=0.5457, MAE=0.5236)\n",
      "\n",
      "[Epoch 019/100]\n",
      "  Train - loss:0.8274 acc:0.4940 off1:0.8858 mae:0.6399\n",
      "  Val   - loss:0.6776 acc:0.5457 off1:0.9351 mae:0.5236\n",
      "  Part losses (Val): [0.671 0.7   0.686 0.588 0.695 0.726]\n",
      "  Region Acc (Val): [0.577 0.518 0.537 0.627 0.521 0.495]\n",
      "  LR:5.00e-05 | 17.7s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6783 acc:0.5431 off1:0.9366 mae:0.5239\n",
      "  Region Acc: [0.564 0.522 0.51  0.622 0.543 0.499]\n",
      "\n",
      "[Epoch 020/100]\n",
      "  Train - loss:0.8877 acc:0.4857 off1:0.8825 mae:0.6522\n",
      "  Val   - loss:0.6783 acc:0.5431 off1:0.9366 mae:0.5239\n",
      "  Part losses (Val): [0.673 0.698 0.693 0.589 0.689 0.728]\n",
      "  Region Acc (Val): [0.564 0.522 0.51  0.622 0.543 0.499]\n",
      "  LR:5.00e-05 | 18.0s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6673 acc:0.5563 off1:0.9335 mae:0.5146\n",
      "  Region Acc: [0.58  0.533 0.527 0.65  0.543 0.504]\n",
      "‚úÖ New Best! (Acc=0.5563, MAE=0.5146)\n",
      "\n",
      "[Epoch 021/100]\n",
      "  Train - loss:0.8301 acc:0.5150 off1:0.9030 mae:0.5958\n",
      "  Val   - loss:0.6673 acc:0.5563 off1:0.9335 mae:0.5146\n",
      "  Part losses (Val): [0.666 0.681 0.68  0.58  0.677 0.721]\n",
      "  Region Acc (Val): [0.58  0.533 0.527 0.65  0.543 0.504]\n",
      "  LR:5.00e-05 | 18.1s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6715 acc:0.5523 off1:0.9346 mae:0.5174\n",
      "  Region Acc: [0.591 0.529 0.527 0.633 0.526 0.508]\n",
      "\n",
      "[Epoch 022/100]\n",
      "  Train - loss:0.8610 acc:0.4992 off1:0.8936 mae:0.6237\n",
      "  Val   - loss:0.6715 acc:0.5523 off1:0.9346 mae:0.5174\n",
      "  Part losses (Val): [0.658 0.688 0.69  0.58  0.691 0.722]\n",
      "  Region Acc (Val): [0.591 0.529 0.527 0.633 0.526 0.508]\n",
      "  LR:5.00e-05 | 18.8s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6623 acc:0.5557 off1:0.9428 mae:0.5049\n",
      "  Region Acc: [0.583 0.546 0.53  0.637 0.534 0.504]\n",
      "\n",
      "[Epoch 023/100]\n",
      "  Train - loss:0.8472 acc:0.4939 off1:0.8897 mae:0.6346\n",
      "  Val   - loss:0.6623 acc:0.5557 off1:0.9428 mae:0.5049\n",
      "  Part losses (Val): [0.646 0.671 0.682 0.58  0.671 0.724]\n",
      "  Region Acc (Val): [0.583 0.546 0.53  0.637 0.534 0.504]\n",
      "  LR:5.00e-05 | 19.4s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6671 acc:0.5535 off1:0.9368 mae:0.5143\n",
      "  Region Acc: [0.573 0.536 0.529 0.626 0.552 0.505]\n",
      "\n",
      "[Epoch 024/100]\n",
      "  Train - loss:0.8177 acc:0.5126 off1:0.9019 mae:0.5998\n",
      "  Val   - loss:0.6671 acc:0.5535 off1:0.9368 mae:0.5143\n",
      "  Part losses (Val): [0.664 0.674 0.68  0.583 0.68  0.722]\n",
      "  Region Acc (Val): [0.573 0.536 0.529 0.626 0.552 0.505]\n",
      "  LR:5.00e-05 | 18.4s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6751 acc:0.5451 off1:0.9306 mae:0.5287\n",
      "  Region Acc: [0.557 0.53  0.515 0.621 0.547 0.501]\n",
      "\n",
      "[Epoch 025/100]\n",
      "  Train - loss:0.8402 acc:0.5044 off1:0.8991 mae:0.6127\n",
      "  Val   - loss:0.6751 acc:0.5451 off1:0.9306 mae:0.5287\n",
      "  Part losses (Val): [0.686 0.686 0.689 0.59  0.676 0.723]\n",
      "  Region Acc (Val): [0.557 0.53  0.515 0.621 0.547 0.501]\n",
      "  LR:5.00e-05 | 18.0s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6654 acc:0.5590 off1:0.9371 mae:0.5077\n",
      "  Region Acc: [0.588 0.55  0.527 0.638 0.542 0.509]\n",
      "‚úÖ New Best! (Acc=0.5590, MAE=0.5077)\n",
      "\n",
      "[Epoch 026/100]\n",
      "  Train - loss:0.8228 acc:0.5098 off1:0.9005 mae:0.6065\n",
      "  Val   - loss:0.6654 acc:0.5590 off1:0.9371 mae:0.5077\n",
      "  Part losses (Val): [0.651 0.675 0.687 0.582 0.676 0.721]\n",
      "  Region Acc (Val): [0.588 0.55  0.527 0.638 0.542 0.509]\n",
      "  LR:5.00e-05 | 19.6s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6688 acc:0.5524 off1:0.9340 mae:0.5190\n",
      "  Region Acc: [0.569 0.546 0.522 0.632 0.542 0.504]\n",
      "\n",
      "[Epoch 027/100]\n",
      "  Train - loss:0.8548 acc:0.4988 off1:0.8900 mae:0.6298\n",
      "  Val   - loss:0.6688 acc:0.5524 off1:0.9340 mae:0.5190\n",
      "  Part losses (Val): [0.666 0.676 0.688 0.585 0.68  0.719]\n",
      "  Region Acc (Val): [0.569 0.546 0.522 0.632 0.542 0.504]\n",
      "  LR:5.00e-05 | 18.7s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6654 acc:0.5504 off1:0.9370 mae:0.5174\n",
      "  Region Acc: [0.579 0.543 0.525 0.627 0.541 0.488]\n",
      "\n",
      "[Epoch 028/100]\n",
      "  Train - loss:0.8235 acc:0.5106 off1:0.9012 mae:0.6044\n",
      "  Val   - loss:0.6654 acc:0.5504 off1:0.9370 mae:0.5174\n",
      "  Part losses (Val): [0.653 0.675 0.68  0.578 0.682 0.725]\n",
      "  Region Acc (Val): [0.579 0.543 0.525 0.627 0.541 0.488]\n",
      "  LR:5.00e-05 | 17.3s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6635 acc:0.5504 off1:0.9399 mae:0.5135\n",
      "  Region Acc: [0.587 0.544 0.501 0.627 0.539 0.504]\n",
      "\n",
      "[Epoch 029/100]\n",
      "  Train - loss:0.8174 acc:0.5270 off1:0.9137 mae:0.5713\n",
      "  Val   - loss:0.6635 acc:0.5504 off1:0.9399 mae:0.5135\n",
      "  Part losses (Val): [0.646 0.678 0.682 0.578 0.677 0.72 ]\n",
      "  Region Acc (Val): [0.587 0.544 0.501 0.627 0.539 0.504]\n",
      "  LR:5.00e-05 | 17.7s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6637 acc:0.5526 off1:0.9320 mae:0.5208\n",
      "  Region Acc: [0.587 0.532 0.529 0.632 0.549 0.488]\n",
      "\n",
      "[Epoch 030/100]\n",
      "  Train - loss:0.8478 acc:0.4937 off1:0.8897 mae:0.6381\n",
      "  Val   - loss:0.6637 acc:0.5526 off1:0.9320 mae:0.5208\n",
      "  Part losses (Val): [0.647 0.68  0.682 0.57  0.682 0.722]\n",
      "  Region Acc (Val): [0.587 0.532 0.529 0.632 0.549 0.488]\n",
      "  LR:5.00e-05 | 17.4s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6684 acc:0.5495 off1:0.9393 mae:0.5148\n",
      "  Region Acc: [0.567 0.539 0.536 0.627 0.529 0.499]\n",
      "\n",
      "[Epoch 031/100]\n",
      "  Train - loss:0.8314 acc:0.4912 off1:0.8807 mae:0.6519\n",
      "  Val   - loss:0.6684 acc:0.5495 off1:0.9393 mae:0.5148\n",
      "  Part losses (Val): [0.658 0.677 0.681 0.589 0.686 0.719]\n",
      "  Region Acc (Val): [0.567 0.539 0.536 0.627 0.529 0.499]\n",
      "  LR:5.00e-05 | 17.9s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6595 acc:0.5636 off1:0.9419 mae:0.4976\n",
      "  Region Acc: [0.602 0.558 0.534 0.647 0.542 0.499]\n",
      "‚úÖ New Best! (Acc=0.5636, MAE=0.4976)\n",
      "\n",
      "[Epoch 032/100]\n",
      "  Train - loss:0.8033 acc:0.5241 off1:0.9147 mae:0.5741\n",
      "  Val   - loss:0.6595 acc:0.5636 off1:0.9419 mae:0.4976\n",
      "  Part losses (Val): [0.635 0.676 0.681 0.576 0.67  0.72 ]\n",
      "  Region Acc (Val): [0.602 0.558 0.534 0.647 0.542 0.499]\n",
      "  LR:5.00e-05 | 19.4s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6704 acc:0.5508 off1:0.9331 mae:0.5216\n",
      "  Region Acc: [0.591 0.529 0.522 0.641 0.539 0.482]\n",
      "\n",
      "[Epoch 033/100]\n",
      "  Train - loss:0.8003 acc:0.5166 off1:0.9006 mae:0.5985\n",
      "  Val   - loss:0.6704 acc:0.5508 off1:0.9331 mae:0.5216\n",
      "  Part losses (Val): [0.66  0.68  0.682 0.578 0.692 0.731]\n",
      "  Region Acc (Val): [0.591 0.529 0.522 0.641 0.539 0.482]\n",
      "  LR:5.00e-05 | 17.4s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6662 acc:0.5504 off1:0.9417 mae:0.5115\n",
      "  Region Acc: [0.581 0.539 0.526 0.639 0.522 0.495]\n",
      "\n",
      "[Epoch 034/100]\n",
      "  Train - loss:0.7914 acc:0.5291 off1:0.9135 mae:0.5706\n",
      "  Val   - loss:0.6662 acc:0.5504 off1:0.9417 mae:0.5115\n",
      "  Part losses (Val): [0.656 0.676 0.682 0.575 0.684 0.724]\n",
      "  Region Acc (Val): [0.581 0.539 0.526 0.639 0.522 0.495]\n",
      "  LR:5.00e-05 | 17.1s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6835 acc:0.5450 off1:0.9335 mae:0.5263\n",
      "  Region Acc: [0.567 0.515 0.534 0.629 0.526 0.498]\n",
      "\n",
      "[Epoch 035/100]\n",
      "  Train - loss:0.8367 acc:0.5101 off1:0.8977 mae:0.6082\n",
      "  Val   - loss:0.6835 acc:0.5450 off1:0.9335 mae:0.5263\n",
      "  Part losses (Val): [0.68  0.694 0.692 0.602 0.705 0.729]\n",
      "  Region Acc (Val): [0.567 0.515 0.534 0.629 0.526 0.498]\n",
      "  LR:5.00e-05 | 17.2s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6598 acc:0.5568 off1:0.9393 mae:0.5073\n",
      "  Region Acc: [0.575 0.545 0.537 0.643 0.536 0.505]\n",
      "\n",
      "[Epoch 036/100]\n",
      "  Train - loss:0.8250 acc:0.5210 off1:0.9086 mae:0.5852\n",
      "  Val   - loss:0.6598 acc:0.5568 off1:0.9393 mae:0.5073\n",
      "  Part losses (Val): [0.656 0.667 0.674 0.57  0.672 0.718]\n",
      "  Region Acc (Val): [0.575 0.545 0.537 0.643 0.536 0.505]\n",
      "  LR:5.00e-05 | 17.3s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6565 acc:0.5616 off1:0.9402 mae:0.5022\n",
      "  Region Acc: [0.596 0.538 0.552 0.645 0.544 0.495]\n",
      "\n",
      "[Epoch 037/100]\n",
      "  Train - loss:0.8154 acc:0.5037 off1:0.8937 mae:0.6219\n",
      "  Val   - loss:0.6565 acc:0.5616 off1:0.9402 mae:0.5022\n",
      "  Part losses (Val): [0.636 0.668 0.672 0.568 0.675 0.718]\n",
      "  Region Acc (Val): [0.596 0.538 0.552 0.645 0.544 0.495]\n",
      "  LR:5.00e-05 | 19.3s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6624 acc:0.5532 off1:0.9388 mae:0.5122\n",
      "  Region Acc: [0.587 0.524 0.529 0.645 0.541 0.495]\n",
      "\n",
      "[Epoch 038/100]\n",
      "  Train - loss:0.8366 acc:0.5041 off1:0.8913 mae:0.6245\n",
      "  Val   - loss:0.6624 acc:0.5532 off1:0.9388 mae:0.5122\n",
      "  Part losses (Val): [0.647 0.677 0.681 0.57  0.678 0.721]\n",
      "  Region Acc (Val): [0.587 0.524 0.529 0.645 0.541 0.495]\n",
      "  LR:2.50e-05 | 17.6s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6591 acc:0.5601 off1:0.9348 mae:0.5099\n",
      "  Region Acc: [0.581 0.549 0.546 0.639 0.545 0.5  ]\n",
      "\n",
      "[Epoch 039/100]\n",
      "  Train - loss:0.8497 acc:0.4973 off1:0.8867 mae:0.6362\n",
      "  Val   - loss:0.6591 acc:0.5601 off1:0.9348 mae:0.5099\n",
      "  Part losses (Val): [0.652 0.668 0.675 0.569 0.67  0.72 ]\n",
      "  Region Acc (Val): [0.581 0.549 0.546 0.639 0.545 0.5  ]\n",
      "  LR:2.50e-05 | 18.5s | Patience:7/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6635 acc:0.5539 off1:0.9412 mae:0.5086\n",
      "  Region Acc: [0.581 0.53  0.533 0.643 0.529 0.509]\n",
      "\n",
      "[Epoch 040/100]\n",
      "  Train - loss:0.8224 acc:0.5152 off1:0.8948 mae:0.6085\n",
      "  Val   - loss:0.6635 acc:0.5539 off1:0.9412 mae:0.5086\n",
      "  Part losses (Val): [0.648 0.672 0.679 0.572 0.69  0.721]\n",
      "  Region Acc (Val): [0.581 0.53  0.533 0.643 0.529 0.509]\n",
      "  LR:2.50e-05 | 17.4s | Patience:8/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6573 acc:0.5694 off1:0.9404 mae:0.4940\n",
      "  Region Acc: [0.611 0.545 0.554 0.658 0.556 0.493]\n",
      "‚úÖ New Best! (Acc=0.5694, MAE=0.4940)\n",
      "\n",
      "[Epoch 041/100]\n",
      "  Train - loss:0.7996 acc:0.5173 off1:0.9003 mae:0.6003\n",
      "  Val   - loss:0.6573 acc:0.5694 off1:0.9404 mae:0.4940\n",
      "  Part losses (Val): [0.642 0.665 0.676 0.563 0.679 0.72 ]\n",
      "  Region Acc (Val): [0.611 0.545 0.554 0.658 0.556 0.493]\n",
      "  LR:2.50e-05 | 19.2s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6507 acc:0.5671 off1:0.9397 mae:0.4973\n",
      "  Region Acc: [0.596 0.552 0.541 0.66  0.547 0.507]\n",
      "\n",
      "[Epoch 042/100]\n",
      "  Train - loss:0.8156 acc:0.5126 off1:0.8940 mae:0.6141\n",
      "  Val   - loss:0.6507 acc:0.5671 off1:0.9397 mae:0.4973\n",
      "  Part losses (Val): [0.63  0.659 0.671 0.561 0.666 0.718]\n",
      "  Region Acc (Val): [0.596 0.552 0.541 0.66  0.547 0.507]\n",
      "  LR:2.50e-05 | 17.5s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6637 acc:0.5627 off1:0.9371 mae:0.5051\n",
      "  Region Acc: [0.588 0.559 0.544 0.641 0.548 0.496]\n",
      "\n",
      "[Epoch 043/100]\n",
      "  Train - loss:0.8040 acc:0.5170 off1:0.8972 mae:0.6056\n",
      "  Val   - loss:0.6637 acc:0.5627 off1:0.9371 mae:0.5051\n",
      "  Part losses (Val): [0.654 0.673 0.675 0.573 0.688 0.72 ]\n",
      "  Region Acc (Val): [0.588 0.559 0.544 0.641 0.548 0.496]\n",
      "  LR:2.50e-05 | 18.6s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6545 acc:0.5643 off1:0.9395 mae:0.5005\n",
      "  Region Acc: [0.594 0.543 0.547 0.651 0.552 0.499]\n",
      "\n",
      "[Epoch 044/100]\n",
      "  Train - loss:0.7883 acc:0.5403 off1:0.9158 mae:0.5579\n",
      "  Val   - loss:0.6545 acc:0.5643 off1:0.9395 mae:0.5005\n",
      "  Part losses (Val): [0.641 0.664 0.669 0.564 0.675 0.715]\n",
      "  Region Acc (Val): [0.594 0.543 0.547 0.651 0.552 0.499]\n",
      "  LR:2.50e-05 | 18.6s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6536 acc:0.5674 off1:0.9377 mae:0.4987\n",
      "  Region Acc: [0.59  0.55  0.553 0.658 0.555 0.499]\n",
      "\n",
      "[Epoch 045/100]\n",
      "  Train - loss:0.7794 acc:0.5219 off1:0.9006 mae:0.5956\n",
      "  Val   - loss:0.6536 acc:0.5674 off1:0.9377 mae:0.4987\n",
      "  Part losses (Val): [0.637 0.659 0.675 0.566 0.668 0.716]\n",
      "  Region Acc (Val): [0.59  0.55  0.553 0.658 0.555 0.499]\n",
      "  LR:2.50e-05 | 18.5s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6545 acc:0.5614 off1:0.9435 mae:0.4995\n",
      "  Region Acc: [0.594 0.535 0.542 0.647 0.557 0.493]\n",
      "\n",
      "[Epoch 046/100]\n",
      "  Train - loss:0.8012 acc:0.5180 off1:0.8922 mae:0.6080\n",
      "  Val   - loss:0.6545 acc:0.5614 off1:0.9435 mae:0.4995\n",
      "  Part losses (Val): [0.638 0.671 0.672 0.563 0.665 0.718]\n",
      "  Region Acc (Val): [0.594 0.535 0.542 0.647 0.557 0.493]\n",
      "  LR:2.50e-05 | 17.7s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6541 acc:0.5649 off1:0.9413 mae:0.4980\n",
      "  Region Acc: [0.594 0.555 0.55  0.649 0.542 0.499]\n",
      "\n",
      "[Epoch 047/100]\n",
      "  Train - loss:0.7861 acc:0.5156 off1:0.8971 mae:0.6075\n",
      "  Val   - loss:0.6541 acc:0.5649 off1:0.9413 mae:0.4980\n",
      "  Part losses (Val): [0.643 0.661 0.666 0.566 0.671 0.717]\n",
      "  Region Acc (Val): [0.594 0.555 0.55  0.649 0.542 0.499]\n",
      "  LR:1.25e-05 | 17.4s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6571 acc:0.5656 off1:0.9443 mae:0.4942\n",
      "  Region Acc: [0.594 0.538 0.547 0.657 0.555 0.502]\n",
      "\n",
      "[Epoch 048/100]\n",
      "  Train - loss:0.8056 acc:0.5219 off1:0.8993 mae:0.5959\n",
      "  Val   - loss:0.6571 acc:0.5656 off1:0.9443 mae:0.4942\n",
      "  Part losses (Val): [0.642 0.667 0.669 0.565 0.678 0.722]\n",
      "  Region Acc (Val): [0.594 0.538 0.547 0.657 0.555 0.502]\n",
      "  LR:1.25e-05 | 17.2s | Patience:7/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6525 acc:0.5674 off1:0.9410 mae:0.4958\n",
      "  Region Acc: [0.6   0.554 0.547 0.649 0.548 0.507]\n",
      "\n",
      "[Epoch 049/100]\n",
      "  Train - loss:0.7890 acc:0.5274 off1:0.9059 mae:0.5826\n",
      "  Val   - loss:0.6525 acc:0.5674 off1:0.9410 mae:0.4958\n",
      "  Part losses (Val): [0.638 0.658 0.667 0.561 0.674 0.717]\n",
      "  Region Acc (Val): [0.6   0.554 0.547 0.649 0.548 0.507]\n",
      "  LR:1.25e-05 | 19.3s | Patience:8/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6512 acc:0.5643 off1:0.9430 mae:0.4962\n",
      "  Region Acc: [0.602 0.543 0.541 0.651 0.546 0.503]\n",
      "\n",
      "[Epoch 050/100]\n",
      "  Train - loss:0.7866 acc:0.5281 off1:0.9087 mae:0.5780\n",
      "  Val   - loss:0.6512 acc:0.5643 off1:0.9430 mae:0.4962\n",
      "  Part losses (Val): [0.636 0.657 0.669 0.561 0.669 0.716]\n",
      "  Region Acc (Val): [0.602 0.543 0.541 0.651 0.546 0.503]\n",
      "  LR:1.25e-05 | 18.2s | Patience:9/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6507 acc:0.5694 off1:0.9437 mae:0.4912\n",
      "  Region Acc: [0.598 0.558 0.555 0.652 0.547 0.507]\n",
      "‚úÖ New Best! (Acc=0.5694, MAE=0.4912)\n",
      "\n",
      "[Epoch 051/100]\n",
      "  Train - loss:0.7964 acc:0.5183 off1:0.9005 mae:0.5982\n",
      "  Val   - loss:0.6507 acc:0.5694 off1:0.9437 mae:0.4912\n",
      "  Part losses (Val): [0.632 0.655 0.668 0.56  0.673 0.717]\n",
      "  Region Acc (Val): [0.598 0.558 0.555 0.652 0.547 0.507]\n",
      "  LR:1.25e-05 | 19.1s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6515 acc:0.5678 off1:0.9433 mae:0.4931\n",
      "  Region Acc: [0.602 0.543 0.544 0.658 0.553 0.508]\n",
      "\n",
      "[Epoch 052/100]\n",
      "  Train - loss:0.8136 acc:0.5113 off1:0.8946 mae:0.6136\n",
      "  Val   - loss:0.6515 acc:0.5678 off1:0.9433 mae:0.4931\n",
      "  Part losses (Val): [0.634 0.663 0.67  0.559 0.667 0.715]\n",
      "  Region Acc (Val): [0.602 0.543 0.544 0.658 0.553 0.508]\n",
      "  LR:1.25e-05 | 18.9s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6539 acc:0.5640 off1:0.9417 mae:0.4989\n",
      "  Region Acc: [0.59  0.543 0.548 0.656 0.554 0.493]\n",
      "\n",
      "[Epoch 053/100]\n",
      "  Train - loss:0.7919 acc:0.5083 off1:0.8883 mae:0.6238\n",
      "  Val   - loss:0.6539 acc:0.5640 off1:0.9417 mae:0.4989\n",
      "  Part losses (Val): [0.64  0.662 0.671 0.564 0.671 0.716]\n",
      "  Region Acc (Val): [0.59  0.543 0.548 0.656 0.554 0.493]\n",
      "  LR:6.25e-06 | 18.3s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6524 acc:0.5669 off1:0.9439 mae:0.4936\n",
      "  Region Acc: [0.596 0.549 0.544 0.654 0.553 0.505]\n",
      "\n",
      "[Epoch 054/100]\n",
      "  Train - loss:0.7871 acc:0.5313 off1:0.9054 mae:0.5788\n",
      "  Val   - loss:0.6524 acc:0.5669 off1:0.9439 mae:0.4936\n",
      "  Part losses (Val): [0.638 0.661 0.669 0.563 0.669 0.714]\n",
      "  Region Acc (Val): [0.596 0.549 0.544 0.654 0.553 0.505]\n",
      "  LR:6.25e-06 | 17.7s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6613 acc:0.5598 off1:0.9393 mae:0.5053\n",
      "  Region Acc: [0.586 0.53  0.539 0.646 0.555 0.503]\n",
      "\n",
      "[Epoch 055/100]\n",
      "  Train - loss:0.8262 acc:0.5085 off1:0.8881 mae:0.6250\n",
      "  Val   - loss:0.6613 acc:0.5598 off1:0.9393 mae:0.5053\n",
      "  Part losses (Val): [0.653 0.675 0.675 0.572 0.676 0.717]\n",
      "  Region Acc (Val): [0.586 0.53  0.539 0.646 0.555 0.503]\n",
      "  LR:6.25e-06 | 17.7s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6523 acc:0.5658 off1:0.9433 mae:0.4952\n",
      "  Region Acc: [0.596 0.548 0.548 0.647 0.555 0.5  ]\n",
      "\n",
      "[Epoch 056/100]\n",
      "  Train - loss:0.7855 acc:0.5261 off1:0.9063 mae:0.5839\n",
      "  Val   - loss:0.6523 acc:0.5658 off1:0.9433 mae:0.4952\n",
      "  Part losses (Val): [0.636 0.659 0.669 0.56  0.671 0.718]\n",
      "  Region Acc (Val): [0.596 0.548 0.548 0.647 0.555 0.5  ]\n",
      "  LR:6.25e-06 | 18.8s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6504 acc:0.5652 off1:0.9448 mae:0.4942\n",
      "  Region Acc: [0.598 0.544 0.555 0.651 0.545 0.499]\n",
      "\n",
      "[Epoch 057/100]\n",
      "  Train - loss:0.7870 acc:0.5304 off1:0.9004 mae:0.5857\n",
      "  Val   - loss:0.6504 acc:0.5652 off1:0.9448 mae:0.4942\n",
      "  Part losses (Val): [0.632 0.657 0.669 0.561 0.669 0.714]\n",
      "  Region Acc (Val): [0.598 0.544 0.555 0.651 0.545 0.499]\n",
      "  LR:6.25e-06 | 17.7s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6516 acc:0.5693 off1:0.9430 mae:0.4921\n",
      "  Region Acc: [0.602 0.556 0.553 0.65  0.552 0.503]\n",
      "\n",
      "[Epoch 058/100]\n",
      "  Train - loss:0.7643 acc:0.5468 off1:0.9177 mae:0.5496\n",
      "  Val   - loss:0.6516 acc:0.5693 off1:0.9430 mae:0.4921\n",
      "  Part losses (Val): [0.636 0.658 0.668 0.56  0.672 0.716]\n",
      "  Region Acc (Val): [0.602 0.556 0.553 0.65  0.552 0.503]\n",
      "  LR:6.25e-06 | 18.6s | Patience:7/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6532 acc:0.5658 off1:0.9432 mae:0.4956\n",
      "  Region Acc: [0.589 0.552 0.552 0.648 0.553 0.502]\n",
      "\n",
      "[Epoch 059/100]\n",
      "  Train - loss:0.7643 acc:0.5393 off1:0.9133 mae:0.5619\n",
      "  Val   - loss:0.6532 acc:0.5658 off1:0.9432 mae:0.4956\n",
      "  Part losses (Val): [0.638 0.659 0.668 0.563 0.674 0.716]\n",
      "  Region Acc (Val): [0.589 0.552 0.552 0.648 0.553 0.502]\n",
      "  LR:3.13e-06 | 18.5s | Patience:8/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6527 acc:0.5640 off1:0.9435 mae:0.4969\n",
      "  Region Acc: [0.593 0.547 0.547 0.65  0.547 0.499]\n",
      "\n",
      "[Epoch 060/100]\n",
      "  Train - loss:0.8112 acc:0.5295 off1:0.9054 mae:0.5804\n",
      "  Val   - loss:0.6527 acc:0.5640 off1:0.9435 mae:0.4969\n",
      "  Part losses (Val): [0.637 0.659 0.669 0.563 0.673 0.716]\n",
      "  Region Acc (Val): [0.593 0.547 0.547 0.65  0.547 0.499]\n",
      "  LR:3.13e-06 | 18.5s | Patience:9/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6534 acc:0.5702 off1:0.9424 mae:0.4920\n",
      "  Region Acc: [0.598 0.559 0.55  0.654 0.557 0.503]\n",
      "‚úÖ New Best! (Acc=0.5702, MAE=0.4920)\n",
      "\n",
      "[Epoch 061/100]\n",
      "  Train - loss:0.7857 acc:0.5311 off1:0.9057 mae:0.5790\n",
      "  Val   - loss:0.6534 acc:0.5702 off1:0.9424 mae:0.4920\n",
      "  Part losses (Val): [0.641 0.66  0.668 0.562 0.672 0.716]\n",
      "  Region Acc (Val): [0.598 0.559 0.55  0.654 0.557 0.503]\n",
      "  LR:3.13e-06 | 18.1s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6538 acc:0.5671 off1:0.9428 mae:0.4943\n",
      "  Region Acc: [0.601 0.556 0.536 0.654 0.554 0.502]\n",
      "\n",
      "[Epoch 062/100]\n",
      "  Train - loss:0.7663 acc:0.5316 off1:0.9076 mae:0.5775\n",
      "  Val   - loss:0.6538 acc:0.5671 off1:0.9428 mae:0.4943\n",
      "  Part losses (Val): [0.638 0.658 0.67  0.561 0.677 0.718]\n",
      "  Region Acc (Val): [0.601 0.556 0.536 0.654 0.554 0.502]\n",
      "  LR:3.13e-06 | 18.3s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6509 acc:0.5707 off1:0.9419 mae:0.4918\n",
      "  Region Acc: [0.599 0.55  0.559 0.66  0.547 0.509]\n",
      "‚úÖ New Best! (Acc=0.5707, MAE=0.4918)\n",
      "\n",
      "[Epoch 063/100]\n",
      "  Train - loss:0.7755 acc:0.5223 off1:0.8970 mae:0.5977\n",
      "  Val   - loss:0.6509 acc:0.5707 off1:0.9419 mae:0.4918\n",
      "  Part losses (Val): [0.637 0.656 0.668 0.559 0.67  0.716]\n",
      "  Region Acc (Val): [0.599 0.55  0.559 0.66  0.547 0.509]\n",
      "  LR:3.13e-06 | 17.9s | Patience:0/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6557 acc:0.5651 off1:0.9433 mae:0.4960\n",
      "  Region Acc: [0.595 0.543 0.535 0.654 0.561 0.502]\n",
      "\n",
      "[Epoch 064/100]\n",
      "  Train - loss:0.8399 acc:0.5060 off1:0.8857 mae:0.6321\n",
      "  Val   - loss:0.6557 acc:0.5651 off1:0.9433 mae:0.4960\n",
      "  Part losses (Val): [0.644 0.663 0.671 0.563 0.676 0.717]\n",
      "  Region Acc (Val): [0.595 0.543 0.535 0.654 0.561 0.502]\n",
      "  LR:3.13e-06 | 16.2s | Patience:1/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6507 acc:0.5704 off1:0.9430 mae:0.4910\n",
      "  Region Acc: [0.603 0.556 0.555 0.656 0.55  0.502]\n",
      "\n",
      "[Epoch 065/100]\n",
      "  Train - loss:0.8018 acc:0.5219 off1:0.8960 mae:0.6027\n",
      "  Val   - loss:0.6507 acc:0.5704 off1:0.9430 mae:0.4910\n",
      "  Part losses (Val): [0.634 0.657 0.669 0.559 0.67  0.716]\n",
      "  Region Acc (Val): [0.603 0.556 0.555 0.656 0.55  0.502]\n",
      "  LR:3.13e-06 | 17.2s | Patience:2/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6542 acc:0.5676 off1:0.9424 mae:0.4943\n",
      "  Region Acc: [0.6   0.556 0.549 0.649 0.557 0.495]\n",
      "\n",
      "[Epoch 066/100]\n",
      "  Train - loss:0.8034 acc:0.5085 off1:0.8901 mae:0.6239\n",
      "  Val   - loss:0.6542 acc:0.5676 off1:0.9424 mae:0.4943\n",
      "  Part losses (Val): [0.641 0.661 0.67  0.562 0.673 0.718]\n",
      "  Region Acc (Val): [0.6   0.556 0.549 0.649 0.557 0.495]\n",
      "  LR:3.13e-06 | 17.3s | Patience:3/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6539 acc:0.5658 off1:0.9437 mae:0.4949\n",
      "  Region Acc: [0.594 0.545 0.545 0.652 0.557 0.501]\n",
      "\n",
      "[Epoch 067/100]\n",
      "  Train - loss:0.7759 acc:0.5279 off1:0.9004 mae:0.5894\n",
      "  Val   - loss:0.6539 acc:0.5658 off1:0.9437 mae:0.4949\n",
      "  Part losses (Val): [0.64  0.662 0.671 0.562 0.672 0.716]\n",
      "  Region Acc (Val): [0.594 0.545 0.545 0.652 0.557 0.501]\n",
      "  LR:3.13e-06 | 17.2s | Patience:4/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6510 acc:0.5682 off1:0.9441 mae:0.4921\n",
      "  Region Acc: [0.603 0.55  0.547 0.657 0.55  0.501]\n",
      "\n",
      "[Epoch 068/100]\n",
      "  Train - loss:0.7806 acc:0.5362 off1:0.9084 mae:0.5731\n",
      "  Val   - loss:0.6510 acc:0.5682 off1:0.9441 mae:0.4921\n",
      "  Part losses (Val): [0.636 0.659 0.668 0.559 0.669 0.715]\n",
      "  Region Acc (Val): [0.603 0.55  0.547 0.657 0.55  0.501]\n",
      "  LR:3.13e-06 | 18.3s | Patience:5/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6507 acc:0.5702 off1:0.9430 mae:0.4912\n",
      "  Region Acc: [0.605 0.557 0.552 0.655 0.55  0.502]\n",
      "\n",
      "[Epoch 069/100]\n",
      "  Train - loss:0.8111 acc:0.5303 off1:0.9033 mae:0.5846\n",
      "  Val   - loss:0.6507 acc:0.5702 off1:0.9430 mae:0.4912\n",
      "  Part losses (Val): [0.635 0.657 0.669 0.558 0.67  0.715]\n",
      "  Region Acc (Val): [0.605 0.557 0.552 0.655 0.55  0.502]\n",
      "  LR:1.56e-06 | 17.6s | Patience:6/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6539 acc:0.5649 off1:0.9430 mae:0.4962\n",
      "  Region Acc: [0.594 0.543 0.545 0.651 0.555 0.501]\n",
      "\n",
      "[Epoch 070/100]\n",
      "  Train - loss:0.7732 acc:0.5215 off1:0.8985 mae:0.6003\n",
      "  Val   - loss:0.6539 acc:0.5649 off1:0.9430 mae:0.4962\n",
      "  Part losses (Val): [0.641 0.661 0.671 0.563 0.673 0.715]\n",
      "  Region Acc (Val): [0.594 0.543 0.545 0.651 0.555 0.501]\n",
      "  LR:1.56e-06 | 17.6s | Patience:7/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6554 acc:0.5636 off1:0.9435 mae:0.4973\n",
      "  Region Acc: [0.596 0.548 0.533 0.647 0.555 0.502]\n",
      "\n",
      "[Epoch 071/100]\n",
      "  Train - loss:0.7650 acc:0.5304 off1:0.9022 mae:0.5847\n",
      "  Val   - loss:0.6554 acc:0.5636 off1:0.9435 mae:0.4973\n",
      "  Part losses (Val): [0.641 0.661 0.672 0.562 0.676 0.719]\n",
      "  Region Acc (Val): [0.596 0.548 0.533 0.647 0.555 0.502]\n",
      "  LR:1.56e-06 | 16.2s | Patience:8/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6513 acc:0.5676 off1:0.9439 mae:0.4927\n",
      "  Region Acc: [0.604 0.555 0.544 0.652 0.547 0.503]\n",
      "\n",
      "[Epoch 072/100]\n",
      "  Train - loss:0.7572 acc:0.5496 off1:0.9161 mae:0.5473\n",
      "  Val   - loss:0.6513 acc:0.5676 off1:0.9439 mae:0.4927\n",
      "  Part losses (Val): [0.635 0.657 0.669 0.559 0.671 0.716]\n",
      "  Region Acc (Val): [0.604 0.555 0.544 0.652 0.547 0.503]\n",
      "  LR:1.56e-06 | 17.5s | Patience:9/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6542 acc:0.5645 off1:0.9430 mae:0.4967\n",
      "  Region Acc: [0.6   0.545 0.538 0.652 0.554 0.498]\n",
      "\n",
      "[Epoch 073/100]\n",
      "  Train - loss:0.7572 acc:0.5263 off1:0.9057 mae:0.5849\n",
      "  Val   - loss:0.6542 acc:0.5645 off1:0.9430 mae:0.4967\n",
      "  Part losses (Val): [0.641 0.662 0.671 0.563 0.673 0.716]\n",
      "  Region Acc (Val): [0.6   0.545 0.538 0.652 0.554 0.498]\n",
      "  LR:1.56e-06 | 17.5s | Patience:10/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6500 acc:0.5691 off1:0.9459 mae:0.4889\n",
      "  Region Acc: [0.595 0.56  0.554 0.656 0.545 0.504]\n",
      "\n",
      "[Epoch 074/100]\n",
      "  Train - loss:0.7784 acc:0.5338 off1:0.9080 mae:0.5738\n",
      "  Val   - loss:0.6500 acc:0.5691 off1:0.9459 mae:0.4889\n",
      "  Part losses (Val): [0.633 0.656 0.668 0.559 0.669 0.716]\n",
      "  Region Acc (Val): [0.595 0.56  0.554 0.656 0.545 0.504]\n",
      "  LR:1.56e-06 | 17.4s | Patience:11/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6524 acc:0.5671 off1:0.9432 mae:0.4942\n",
      "  Region Acc: [0.6   0.554 0.55  0.649 0.55  0.499]\n",
      "\n",
      "[Epoch 075/100]\n",
      "  Train - loss:0.7535 acc:0.5369 off1:0.9142 mae:0.5637\n",
      "  Val   - loss:0.6524 acc:0.5671 off1:0.9432 mae:0.4942\n",
      "  Part losses (Val): [0.639 0.659 0.669 0.561 0.671 0.716]\n",
      "  Region Acc (Val): [0.6   0.554 0.55  0.649 0.55  0.499]\n",
      "  LR:1.00e-06 | 19.0s | Patience:12/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6509 acc:0.5660 off1:0.9433 mae:0.4947\n",
      "  Region Acc: [0.591 0.545 0.554 0.652 0.555 0.499]\n",
      "\n",
      "[Epoch 076/100]\n",
      "  Train - loss:0.8149 acc:0.5184 off1:0.8945 mae:0.6076\n",
      "  Val   - loss:0.6509 acc:0.5660 off1:0.9433 mae:0.4947\n",
      "  Part losses (Val): [0.636 0.658 0.669 0.561 0.668 0.714]\n",
      "  Region Acc (Val): [0.591 0.545 0.554 0.652 0.555 0.499]\n",
      "  LR:1.00e-06 | 15.9s | Patience:13/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6527 acc:0.5665 off1:0.9439 mae:0.4940\n",
      "  Region Acc: [0.6   0.55  0.547 0.65  0.552 0.5  ]\n",
      "\n",
      "[Epoch 077/100]\n",
      "  Train - loss:0.7989 acc:0.5044 off1:0.8874 mae:0.6282\n",
      "  Val   - loss:0.6527 acc:0.5665 off1:0.9439 mae:0.4940\n",
      "  Part losses (Val): [0.638 0.658 0.669 0.561 0.674 0.717]\n",
      "  Region Acc (Val): [0.6   0.55  0.547 0.65  0.552 0.5  ]\n",
      "  LR:1.00e-06 | 18.3s | Patience:14/15\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val] loss:0.6558 acc:0.5627 off1:0.9421 mae:0.4996\n",
      "  Region Acc: [0.594 0.545 0.532 0.65  0.549 0.505]\n",
      "\n",
      "‚èπÔ∏è Early stopping at epoch 78\n",
      "\n",
      "======================================================================\n",
      "üéâ Training Finished!\n",
      "======================================================================\n",
      "\n",
      "üìä Test evaluation with best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] loss:0.6052 acc:0.5765 off1:0.9623 mae:0.4646\n",
      "  Region Acc: [0.568 0.582 0.534 0.692 0.603 0.479]\n",
      "\n",
      "üèÜ Test Results:\n",
      "   Accuracy: 0.5765\n",
      "   Off-by-1: 0.9623\n",
      "   MAE: 0.4646\n",
      "   Region Acc: [0.568 0.582 0.534 0.692 0.603 0.479]\n",
      "   Part losses: [0.593 0.63  0.62  0.507 0.599 0.682]\n",
      "\n",
      "üíæ Best model saved: runs_severity_classification/best_densenet121_mimic_classification.pth\n",
      "üìà Best Validation Accuracy: 0.5707\n",
      "üìâ Best Validation MAE: 0.4918\n",
      "\n",
      "======================================================================\n",
      "üí° Ï∂îÍ∞Ä ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú Ï†úÏïà:\n",
      "======================================================================\n",
      "1. üéØ ÏïôÏÉÅÎ∏î: Îã§Î•∏ ÏãúÎìúÎ°ú 3~5Í∞ú Î™®Îç∏ ÌïôÏäµ ÌõÑ Ìà¨Ìëú\n",
      "2. üî¨ ÏùòÎ£å Ï†ÑÏö© pretrained model ÏÇ¨Ïö©:\n",
      "   - CheXpert, MIMIC-CXR Îì±ÏúºÎ°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏\n",
      "3. üìä Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä:\n",
      "   - Ïô∏Î∂Ä COVID-19 Îç∞Ïù¥ÌÑ∞ÏÖã ÌôúÏö©\n",
      "   - Pseudo-labelingÏúºÎ°ú unlabeled Îç∞Ïù¥ÌÑ∞ ÌôúÏö©\n",
      "4. üé® Í≥†Í∏â Ï¶ùÍ∞ï:\n",
      "   - CutMix, AugMix, RandAugment\n",
      "   - Test-Time Augmentation (TTA)\n",
      "5. üß™ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù:\n",
      "   - ordinal_weight Ï°∞Ï†ï (0.3~0.7)\n",
      "   - Learning rate, batch size Ïã§Ìóò\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Ïù¥Ï†ú gradcam_inference.pyÎ•º Ïã§ÌñâÌïòÏó¨ Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÏÑ∏Ïöî!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as TF\n",
    "import torchxrayvision as xrv  # ‚úÖ ÏùòÎ£å ÏòÅÏÉÅ Ï†ÑÏö© ÎùºÏù¥Î∏åÎü¨Î¶¨\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# --- Í≤ΩÎ°ú ÏÑ§Ï†ï Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ---\n",
    "BASE_DIR     = f\"./data/covid19-xray-severity-scoring/\"\n",
    "CSV_PATH     = str(Path(BASE_DIR) / \"Brixia.csv\")\n",
    "IMAGE_DIR    = str(Path(BASE_DIR) / \"segmented_png\")\n",
    "\n",
    "OUT_DIR      = \"./runs_severity_classification\"\n",
    "BEST_PATH    = str(Path(OUT_DIR) / \"best_densenet121_mimic_classification.pth\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED         = 42\n",
    "IMG_SIZE     = 224\n",
    "BATCH_SIZE   = 256\n",
    "NUM_CLASSES  = 4   # 0, 1, 2, 3\n",
    "EPOCHS       = 100  # Single phase training\n",
    "LR           = 1e-4\n",
    "WEIGHT_DECAY = 5e-4\n",
    "AMP          = True\n",
    "EARLY_STOP_ACC = 0.75  # üîÑ MAE ‚Üí Accuracy\n",
    "DROP_RATIO   = 0.3\n",
    "AUG_RATIO    = 0.5\n",
    "MIXUP_ALPHA  = 0.2\n",
    "LABEL_SMOOTHING = 0.1  # ‚úÖ NEW: Label smoothing\n",
    "\n",
    "# --- ÏãúÎìú Í≥†Ï†ï ---\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "def make_transform_with_label(train: bool, img_size: int = IMG_SIZE, aug_ratio=AUG_RATIO):\n",
    "    \"\"\"Brixia ScoreÏùò Ï¢åÏö∞ Íµ¨Ï°∞Î•º Í≥†Î†§Ìïú transform\"\"\"\n",
    "    def _tfm(img: Image.Image, label: torch.Tensor = None):\n",
    "        img = img.convert('RGB')\n",
    "        img = TF.resize(\n",
    "            img, \n",
    "            [img_size, img_size], \n",
    "            interpolation=TF.InterpolationMode.BILINEAR,\n",
    "            antialias=True\n",
    "        )\n",
    "        \n",
    "        if train:\n",
    "            # 1. Horizontal Flip (Ï¢åÏö∞ Î∞òÏ†Ñ: ABC ‚Üî DEF)\n",
    "            if random.random() < aug_ratio:\n",
    "                img = TF.hflip(img)\n",
    "                if label is not None:\n",
    "                    # [A, B, C, D, E, F] ‚Üí [D, E, F, A, B, C]\n",
    "                    label = label[[3, 4, 5, 0, 1, 2]]\n",
    "            \n",
    "            # 2. ÏïΩÌïú ÌöåÏ†Ñ (¬±5ÎèÑ)\n",
    "            if random.random() < aug_ratio:\n",
    "                angle = float(torch.empty(1).uniform_(-5, 5))\n",
    "                img = TF.rotate(\n",
    "                    img, \n",
    "                    angle, \n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 3. ÏïΩÌïú Translation\n",
    "            if random.random() < aug_ratio:\n",
    "                max_dx = 0.05 * img_size\n",
    "                max_dy = 0.05 * img_size\n",
    "                translations = (\n",
    "                    float(torch.empty(1).uniform_(-max_dx, max_dx)),\n",
    "                    float(torch.empty(1).uniform_(-max_dy, max_dy))\n",
    "                )\n",
    "                img = TF.affine(\n",
    "                    img,\n",
    "                    angle=0,\n",
    "                    translate=translations,\n",
    "                    scale=1.0,\n",
    "                    shear=0,\n",
    "                    interpolation=TF.InterpolationMode.BILINEAR,\n",
    "                    fill=0\n",
    "                )\n",
    "            \n",
    "            # 4. Brightness & Contrast\n",
    "            if random.random() < aug_ratio:\n",
    "                brightness_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_brightness(img, brightness_factor)\n",
    "            \n",
    "            if random.random() < aug_ratio:\n",
    "                contrast_factor = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_contrast(img, contrast_factor)\n",
    "            \n",
    "            # 5. Gamma Correction\n",
    "            if random.random() < 0.3:\n",
    "                gamma = float(torch.empty(1).uniform_(0.9, 1.1))\n",
    "                img = TF.adjust_gamma(img, gamma)\n",
    "        \n",
    "        # Tensor Î≥ÄÌôò\n",
    "        img = TF.to_tensor(img)\n",
    "        \n",
    "        # Gaussian Noise (train only)\n",
    "        if train and random.random() < 0.2:\n",
    "            noise = torch.randn_like(img) * 0.01\n",
    "            img = img + noise\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "        # Ï†ïÍ∑úÌôî (torchxrayvision ÌëúÏ§Ä)\n",
    "        # MIMIC-CXR Îç∞Ïù¥ÌÑ∞Î°ú ÌïôÏäµÎêú Î™®Îç∏ÏùÄ [-1024, 1024] Î≤îÏúÑÎ•º ÏÇ¨Ïö©\n",
    "        # ÌïòÏßÄÎßå ÏùºÎ∞ò RGB Ïù¥ÎØ∏ÏßÄÎäî ImageNet Ï†ïÍ∑úÌôî Ïú†ÏßÄ\n",
    "        img = TF.normalize(img, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        if label is not None:\n",
    "            return img, label\n",
    "        return img\n",
    "    \n",
    "    return _tfm\n",
    "\n",
    "def load_and_split_brixia(csv_path, val_ratio=0.2, seed=SEED):\n",
    "    df = pd.read_csv(csv_path, dtype={'BrixiaScore': str})\n",
    "    df = df.dropna(subset=['BrixiaScore'])\n",
    "    df = df[df['BrixiaScore'] != 'nan']\n",
    "    df = df[df['BrixiaScore'].str.len() == 6].copy()\n",
    "    \n",
    "    print(f\"Ïú†Ìö®Ìïú Îç∞Ïù¥ÌÑ∞: {len(df)}Í∞ú\")\n",
    "    \n",
    "    if 'ConsensusTestset' in df.columns:\n",
    "        test_df = df[df['ConsensusTestset'] == 1].copy()\n",
    "        train_val_df = df[df['ConsensusTestset'] == 0].copy()\n",
    "    else:\n",
    "        test_df = pd.DataFrame()\n",
    "        train_val_df = df.copy()\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=val_ratio, random_state=seed)\n",
    "    train_idx, val_idx = next(gss.split(\n",
    "        train_val_df, \n",
    "        groups=train_val_df['StudyId']\n",
    "    ))\n",
    "    \n",
    "    tr_df = train_val_df.iloc[train_idx].copy()\n",
    "    val_df = train_val_df.iloc[val_idx].copy()\n",
    "    \n",
    "    print(f\"Train: {len(tr_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "    validate_split(tr_df, val_df, test_df)\n",
    "    \n",
    "    return tr_df, val_df, test_df\n",
    "\n",
    "def validate_split(tr_df, val_df, tt_df):\n",
    "    train_studies = set(tr_df['StudyId'])\n",
    "    val_studies = set(val_df['StudyId'])\n",
    "    test_studies = set(tt_df['StudyId']) if len(tt_df) > 0 else set()\n",
    "    \n",
    "    assert len(train_studies & val_studies) == 0, \"Train-Val Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(train_studies & test_studies) == 0, \"Train-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    assert len(val_studies & test_studies) == 0, \"Val-Test Í∞Ñ StudyId Ï§ëÎ≥µ!\"\n",
    "    \n",
    "    for name, data in [('Train', tr_df), ('Val', val_df), ('Test', tt_df)]:\n",
    "        if len(data) > 0:\n",
    "            scores = data['BrixiaScore'].apply(lambda x: sum(int(c) for c in x))\n",
    "            print(f\"{name} - Mean: {scores.mean():.2f}, Std: {scores.std():.2f}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ============================================================\n",
    "# Dataset\n",
    "# ============================================================\n",
    "class BrixiaDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.img_col = \"Filename\"\n",
    "        self.label_col = \"BrixiaScore\"\n",
    "        self._validate_data()\n",
    "    \n",
    "    def _validate_data(self):\n",
    "        assert self.img_col in self.df.columns\n",
    "        assert self.label_col in self.df.columns\n",
    "        \n",
    "        invalid_scores = self.df[self.df[self.label_col].str.len() != 6]\n",
    "        if len(invalid_scores) > 0:\n",
    "            print(f\"‚ö†Ô∏è Í≤ΩÍ≥†: {len(invalid_scores)}Í∞úÏùò ÏûòÎ™ªÎêú BrixiaScore Î∞úÍ≤¨\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_name_from_csv = row[self.img_col]\n",
    "        img_name = img_name_from_csv.replace('.dcm', '.png')\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïò§Î•ò: {img_path}\")\n",
    "            raise\n",
    "        \n",
    "        scores_str = row[self.label_col]\n",
    "        scores_list = [int(c) for c in scores_str]\n",
    "        labels = torch.tensor(scores_list, dtype=torch.long)  # üîÑ longÏúºÎ°ú Î≥ÄÍ≤Ω\n",
    "        \n",
    "        if self.transform:\n",
    "            image, labels = self.transform(image, labels)\n",
    "        else:\n",
    "            image = TF.to_tensor(image)\n",
    "            image = TF.normalize(image, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image, labels\n",
    "\n",
    "def create_dataloaders(tr_df, val_df, tt_df, img_dir, \n",
    "                       batch_size=32, img_size=224, num_workers=4):\n",
    "    train_transform = make_transform_with_label(train=True, img_size=img_size)\n",
    "    val_transform = make_transform_with_label(train=False, img_size=img_size)\n",
    "    \n",
    "    tr_ds = BrixiaDataset(tr_df, img_dir, transform=train_transform)\n",
    "    val_ds = BrixiaDataset(val_df, img_dir, transform=val_transform)\n",
    "    tt_ds = BrixiaDataset(tt_df, img_dir, transform=val_transform)\n",
    "    \n",
    "    tr_loader = DataLoader(\n",
    "        tr_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    tt_loader = DataLoader(\n",
    "        tt_ds, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ DataLoader Ï§ÄÎπÑ ÏôÑÎ£å\")\n",
    "    print(f\"   Train: {len(tr_ds)} samples, {len(tr_loader)} batches\")\n",
    "    print(f\"   Val:   {len(val_ds)} samples, {len(val_loader)} batches\")\n",
    "    print(f\"   Test:  {len(tt_ds)} samples, {len(tt_loader)} batches\")\n",
    "    \n",
    "    return tr_loader, val_loader, tt_loader\n",
    "\n",
    "# ============================================================\n",
    "# Loss Function - Ordinal Classification\n",
    "# ============================================================\n",
    "def calculate_class_weights(labels, num_classes=4, method='sqrt_inverse'):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ìï¥Í≤∞ÏùÑ ÏúÑÌïú Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    counts = np.bincount(labels_flat.astype(int), minlength=num_classes)\n",
    "    \n",
    "    if method == 'sqrt_inverse':\n",
    "        weights = 1.0 / (np.sqrt(counts) + 1e-6)\n",
    "    elif method == 'inverse':\n",
    "        weights = 1.0 / (counts + 1e-6)\n",
    "    else:\n",
    "        total = len(labels_flat)\n",
    "        weights = total / (num_classes * (counts + 1e-6))\n",
    "    \n",
    "    weights = weights / weights.mean()\n",
    "    return torch.FloatTensor(weights)\n",
    "\n",
    "def print_class_distribution(labels):\n",
    "    \"\"\"ÌÅ¥ÎûòÏä§ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.cpu().numpy()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Class Distribution Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    region_names = ['A', 'B', 'C', 'D', 'E', 'F']\n",
    "    for idx, name in enumerate(region_names):\n",
    "        region_labels = labels[:, idx]\n",
    "        counts = np.bincount(region_labels.astype(int), minlength=4)\n",
    "        total = counts.sum()\n",
    "        \n",
    "        print(f\"\\n{name}:\")\n",
    "        for cls in range(4):\n",
    "            pct = 100 * counts[cls] / total if total > 0 else 0\n",
    "            bar = '‚ñà' * int(pct / 2)\n",
    "            print(f\"  Class {cls}: {counts[cls]:4d} ({pct:5.1f}%) {bar}\")\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "\n",
    "class OrdinalRegressionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ Ordinal Regression Loss (ÏàúÏÑúÌòï ÌöåÍ∑Ä)\n",
    "    0 < 1 < 2 < 3Ïùò ÏàúÏÑúÎ•º Î™ÖÏãúÏ†ÅÏúºÎ°ú ÌïôÏäµ\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits: [B*6, num_classes-1] - cumulative logits\n",
    "        target: [B*6] - class labels (0~3)\n",
    "        \"\"\"\n",
    "        # Cumulative labels ÏÉùÏÑ±\n",
    "        # Class 0: [0, 0, 0]\n",
    "        # Class 1: [1, 0, 0]\n",
    "        # Class 2: [1, 1, 0]\n",
    "        # Class 3: [1, 1, 1]\n",
    "        batch_size = target.size(0)\n",
    "        target_expanded = target.unsqueeze(1)  # [B*6, 1]\n",
    "        \n",
    "        # [0, 1, 2, ..., num_classes-2]\n",
    "        thresholds = torch.arange(self.num_classes - 1).to(target.device)\n",
    "        thresholds = thresholds.unsqueeze(0).expand(batch_size, -1)  # [B*6, 3]\n",
    "        \n",
    "        # target > thresholdÏù¥Î©¥ 1, ÏïÑÎãàÎ©¥ 0\n",
    "        cumulative_target = (target_expanded > thresholds).float()  # [B*6, 3]\n",
    "        \n",
    "        # Binary cross entropy for each threshold\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, cumulative_target, reduction='none'\n",
    "        )\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "class AdaptiveOrdinalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ Ordinal Loss + Class Weights + Part Weights\n",
    "    ÏàúÏÑúÎ•º Í≥†Î†§ÌïòÎ©¥ÏÑú Î∂àÍ∑†ÌòïÎèÑ Ìï¥Í≤∞\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_labels, num_classes=4, use_class_weights=True, \n",
    "                 part_weights=None, ordinal_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.ordinal_weight = ordinal_weight  # Ordinal lossÏôÄ CE lossÏùò ÎπÑÏú®\n",
    "        \n",
    "        if isinstance(train_labels, torch.Tensor):\n",
    "            train_labels_np = train_labels.cpu().numpy()\n",
    "        else:\n",
    "            train_labels_np = train_labels\n",
    "        \n",
    "        print_class_distribution(train_labels_np)\n",
    "        \n",
    "        # ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "        if use_class_weights:\n",
    "            class_weights = calculate_class_weights(train_labels_np, num_classes=num_classes)\n",
    "            self.register_buffer('class_weights', class_weights)\n",
    "            print(f\"‚úÖ Class weights: {class_weights.numpy()}\")\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò\n",
    "        if part_weights is None:\n",
    "            self.part_weights = torch.ones(6)\n",
    "        else:\n",
    "            self.part_weights = torch.tensor(part_weights, dtype=torch.float32)\n",
    "        self.register_buffer('part_weights_buf', self.part_weights)\n",
    "        print(f\"‚úÖ Part weights: {self.part_weights.numpy()}\")\n",
    "        print(f\"‚úÖ Ordinal weight: {ordinal_weight}\")\n",
    "        \n",
    "        # Ordinal loss\n",
    "        self.ordinal_loss = OrdinalRegressionLoss(num_classes)\n",
    "\n",
    "    def forward(self, pred_dict, target, use_mixup=False):\n",
    "        \"\"\"\n",
    "        pred_dict: {'logits': [B, 6, 4], 'ordinal_logits': [B, 6, 3]}\n",
    "        target: [B, 6]\n",
    "        \"\"\"\n",
    "        pred_logits = pred_dict['logits']  # [B, 6, 4]\n",
    "        ordinal_logits = pred_dict['ordinal_logits']  # [B, 6, 3]\n",
    "        \n",
    "        B, num_regions, num_classes = pred_logits.shape\n",
    "        \n",
    "        # Reshape\n",
    "        pred_logits_flat = pred_logits.view(B * num_regions, num_classes)\n",
    "        ordinal_logits_flat = ordinal_logits.view(B * num_regions, num_classes - 1)\n",
    "        target_flat = target.view(B * num_regions)\n",
    "        \n",
    "        # 1. CrossEntropyLoss (Í∏∞Î≥∏ Î∂ÑÎ•ò)\n",
    "        if self.class_weights is not None and not use_mixup:\n",
    "            ce_loss = F.cross_entropy(\n",
    "                pred_logits_flat, \n",
    "                target_flat,\n",
    "                weight=self.class_weights.to(pred_logits.device),\n",
    "                reduction='none'\n",
    "            )\n",
    "        else:\n",
    "            ce_loss = F.cross_entropy(\n",
    "                pred_logits_flat, \n",
    "                target_flat,\n",
    "                reduction='none'\n",
    "            )\n",
    "        \n",
    "        # 2. Ordinal Loss (ÏàúÏÑú ÌïôÏäµ)\n",
    "        ord_loss = self.ordinal_loss(ordinal_logits_flat, target_flat)\n",
    "        \n",
    "        # 3. Í≤∞Ìï©\n",
    "        total_loss_flat = (1 - self.ordinal_weight) * ce_loss + self.ordinal_weight * ord_loss\n",
    "        total_loss = total_loss_flat.view(B, num_regions)\n",
    "        \n",
    "        # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "        part_weights = self.part_weights_buf.to(pred_logits.device)\n",
    "        total_loss = total_loss * part_weights.unsqueeze(0)\n",
    "        \n",
    "        part_losses = total_loss.mean(dim=0)\n",
    "        \n",
    "        return total_loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Mixup (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "def mixup_data_classification(x, y, alpha=MIXUP_ALPHA):\n",
    "    \"\"\"\n",
    "    ‚úÖ NEW: Î∂ÑÎ•òÏö© Mixup\n",
    "    yÎäî one-hotÏúºÎ°ú Î≥ÄÌôò ÌõÑ mixup\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    \n",
    "    # One-hot encoding\n",
    "    y_onehot = F.one_hot(y, num_classes=NUM_CLASSES).float()  # [B, 6, 4]\n",
    "    y_onehot_shuffled = y_onehot[index]\n",
    "    \n",
    "    mixed_y = lam * y_onehot + (1 - lam) * y_onehot_shuffled  # [B, 6, 4]\n",
    "    \n",
    "    return mixed_x, mixed_y, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred_dict, y_mixed, lam):\n",
    "    \"\"\"MixupÏùÑ ÏúÑÌïú ÏÜêÏã§ Í≥ÑÏÇ∞ (Ordinal Loss ÏßÄÏõê)\"\"\"\n",
    "    pred_logits = pred_dict['logits']\n",
    "    ordinal_logits = pred_dict['ordinal_logits']\n",
    "    \n",
    "    B, num_regions, num_classes = pred_logits.shape\n",
    "    \n",
    "    # Reshape\n",
    "    pred_flat = pred_logits.view(B * num_regions, num_classes)\n",
    "    ordinal_flat = ordinal_logits.view(B * num_regions, num_classes - 1)\n",
    "    target_flat = y_mixed.view(B * num_regions, num_classes)\n",
    "    \n",
    "    # Soft target loss (CE part)\n",
    "    log_probs = F.log_softmax(pred_flat, dim=1)\n",
    "    ce_loss = -(target_flat * log_probs).sum(dim=1)\n",
    "    \n",
    "    # Ordinal partÎäî mixupÏóêÏÑú skip (hard labelÎßå ÏÇ¨Ïö©)\n",
    "    loss = ce_loss  # Mixup ÏãúÏóêÎäî ordinal loss Ï†úÏô∏\n",
    "    loss = loss.view(B, num_regions)\n",
    "    \n",
    "    # Î∂ÄÏúÑÎ≥Ñ Í∞ÄÏ§ëÏπò Ï†ÅÏö©\n",
    "    if hasattr(criterion, 'part_weights_buf'):\n",
    "        part_weights = criterion.part_weights_buf.to(pred_logits.device)\n",
    "        loss = loss * part_weights.unsqueeze(0)\n",
    "    \n",
    "    part_losses = loss.mean(dim=0)\n",
    "    return loss.mean(), part_losses\n",
    "\n",
    "# ============================================================\n",
    "# Metrics (Î∂ÑÎ•òÏö©)\n",
    "# ============================================================\n",
    "@torch.no_grad()\n",
    "def calculate_classification_metrics(pred_dict, labels):\n",
    "    \"\"\"\n",
    "    ‚úÖ Î∂ÑÎ•ò ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    pred_dict: {'logits': [B, 6, 4], 'ordinal_logits': [B, 6, 3]}\n",
    "    labels: [B, 6]\n",
    "    \"\"\"\n",
    "    pred_logits = pred_dict['logits']\n",
    "    \n",
    "    # ÏòàÏ∏° ÌÅ¥ÎûòÏä§\n",
    "    preds = pred_logits.argmax(dim=-1)  # [B, 6]\n",
    "    \n",
    "    # Exact match accuracy\n",
    "    exact_acc = (preds == labels).float().mean().item()\n",
    "    \n",
    "    # Off-by-1 accuracy (Ïù∏Ï†ë ÌÅ¥ÎûòÏä§ ÌóàÏö©)\n",
    "    off_by_1 = (torch.abs(preds - labels) <= 1).float().mean().item()\n",
    "    \n",
    "    # Per-region accuracy\n",
    "    region_acc = (preds == labels).float().mean(dim=0)  # [6]\n",
    "    \n",
    "    # MAE (Ï∞∏Í≥†Ïö©)\n",
    "    mae = torch.abs(preds.float() - labels.float()).mean().item()\n",
    "    \n",
    "    return exact_acc, off_by_1, mae, region_acc\n",
    "\n",
    "# ============================================================\n",
    "# Model - DenseNet121-MIMIC Í∏∞Î∞ò (ÏùòÎ£å ÏòÅÏÉÅ Ï†ÑÏö©)\n",
    "# ============================================================\n",
    "class DenseNet121MIMICClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    ‚úÖ DenseNet121-MIMIC Í∏∞Î∞ò Ordinal Classification Î™®Îç∏\n",
    "    - MIMIC-CXR Îç∞Ïù¥ÌÑ∞Î°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú ÏùòÎ£å ÏòÅÏÉÅ Ï†ÑÏö© Î™®Îç∏\n",
    "    - Í∞Å Î∂ÄÏúÑ(A~F)ÎßàÎã§ 4Í∞ú ÌÅ¥ÎûòÏä§(0~3) ÏòàÏ∏°\n",
    "    - Self-AttentionÏúºÎ°ú Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ\n",
    "    - Ordinal Regression Head Ï∂îÍ∞Ä (ÏàúÏÑú ÌïôÏäµ)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True, drop=0.3, num_regions=6, num_classes=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ‚úÖ MIMIC-CXRÎ°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú DenseNet121\n",
    "        if pretrained:\n",
    "            # self.backbone = xrv.models.DenseNet(weights=\"densenet121-res224-mimic_ch\")\n",
    "            self.backbone = xrv.models.DenseNet(weights=\"densenet121-res224-chex\")\n",
    "            print(\"‚úÖ Loaded DenseNet121 pretrained on CXR!\")\n",
    "        else:\n",
    "            self.backbone = xrv.models.DenseNet(weights=None)\n",
    "        \n",
    "        # DenseNet121Ïùò feature extractorÎßå ÏÇ¨Ïö©\n",
    "        self.features = self.backbone.features\n",
    "        in_feat = 1024  # DenseNet121Ïùò Ï∂úÎ†• Ï±ÑÎÑê\n",
    "        \n",
    "        # Adaptive poolingÏúºÎ°ú Í≥†Ï†ïÎêú ÌÅ¨Í∏∞ Ï∂úÎ†•\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))  # [B, 1024, 7, 7]\n",
    "        \n",
    "        # Positional embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 49, in_feat))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "        # Region queries\n",
    "        self.region_queries = nn.Parameter(torch.randn(num_regions, in_feat))\n",
    "        nn.init.xavier_uniform_(self.region_queries)\n",
    "        \n",
    "        # Cross attention (Ïù¥ÎØ∏ÏßÄ ÌäπÏßï ‚Üí Î∂ÄÏúÑÎ≥Ñ ÌäπÏßï)\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # Self-Attention (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ)\n",
    "        self.self_attention = nn.MultiheadAttention(\n",
    "            embed_dim=in_feat,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(in_feat)\n",
    "        self.norm3 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # FFN\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_feat, in_feat * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(in_feat * 2, in_feat),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        \n",
    "        self.norm4 = nn.LayerNorm(in_feat)\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(in_feat, 512),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "        \n",
    "        # ‚úÖ Classification heads (ÏùºÎ∞ò Î∂ÑÎ•ò)\n",
    "        self.classification_heads = nn.ModuleList([\n",
    "            nn.Linear(256, num_classes) for _ in range(num_regions)\n",
    "        ])\n",
    "        \n",
    "        # ‚úÖ Ordinal Regression heads (ÏàúÏÑúÌòï ÌöåÍ∑Ä)\n",
    "        self.ordinal_heads = nn.ModuleList([\n",
    "            nn.Linear(256, num_classes - 1) for _ in range(num_regions)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # ‚úÖ DenseNet121 feature extraction\n",
    "        # torchxrayvision Î™®Îç∏ÏùÄ grayscaleÏùÑ Í∏∞ÎåÄÌïòÎØÄÎ°ú RGBÏùò Í≤ΩÏö∞ ÌèâÍ∑†\n",
    "        if x.size(1) == 3:\n",
    "            # RGBÎ•º grayscaleÎ°ú Î≥ÄÌôò (weighted average)\n",
    "            x = 0.299 * x[:, 0:1] + 0.587 * x[:, 1:2] + 0.114 * x[:, 2:3]\n",
    "        \n",
    "        feat = self.features(x)  # [B, 1024, H, W]\n",
    "        feat = self.adaptive_pool(feat)  # [B, 1024, 7, 7]\n",
    "        feat = feat.flatten(2).transpose(1, 2)  # [B, 49, 1024]\n",
    "        feat = feat + self.pos_embed\n",
    "        \n",
    "        # Region queries\n",
    "        queries = self.region_queries.unsqueeze(0).expand(B, -1, -1)  # [B, 6, 1024]\n",
    "        \n",
    "        # Cross attention (Ïù¥ÎØ∏ÏßÄ ‚Üí Î∂ÄÏúÑ)\n",
    "        attn_out, _ = self.cross_attention(\n",
    "            query=queries,\n",
    "            key=feat,\n",
    "            value=feat\n",
    "        )\n",
    "        attn_out = self.norm1(attn_out + queries)  # [B, 6, 1024]\n",
    "        \n",
    "        # Self-Attention (Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ)\n",
    "        self_attn_out, attn_weights = self.self_attention(\n",
    "            query=attn_out,\n",
    "            key=attn_out,\n",
    "            value=attn_out\n",
    "        )\n",
    "        attn_out = self.norm2(attn_out + self_attn_out)  # [B, 6, 1024]\n",
    "        \n",
    "        # FFN\n",
    "        ffn_out = self.ffn(attn_out)\n",
    "        attn_out = self.norm3(attn_out + ffn_out)  # [B, 6, 1024]\n",
    "        \n",
    "        # Shared feature extraction\n",
    "        shared_features = self.shared_fc(attn_out)  # [B, 6, 256]\n",
    "        \n",
    "        # ‚úÖ Í∞Å Î∂ÄÏúÑÎ≥Ñ ÏòàÏ∏° (Classification + Ordinal)\n",
    "        classification_outputs = []\n",
    "        ordinal_outputs = []\n",
    "        \n",
    "        for i in range(len(self.classification_heads)):\n",
    "            region_feat = shared_features[:, i, :]  # [B, 256]\n",
    "            \n",
    "            # Classification logits\n",
    "            class_logits = self.classification_heads[i](region_feat)  # [B, 4]\n",
    "            classification_outputs.append(class_logits)\n",
    "            \n",
    "            # Ordinal logits (cumulative)\n",
    "            ordinal_logits = self.ordinal_heads[i](region_feat)  # [B, 3]\n",
    "            ordinal_outputs.append(ordinal_logits)\n",
    "        \n",
    "        class_out = torch.stack(classification_outputs, dim=1)  # [B, 6, 4]\n",
    "        ordinal_out = torch.stack(ordinal_outputs, dim=1)  # [B, 6, 3]\n",
    "        \n",
    "        return {\n",
    "            'logits': class_out,\n",
    "            'ordinal_logits': ordinal_out,\n",
    "            'attn_weights': attn_weights\n",
    "        }\n",
    "\n",
    "# ============================================================\n",
    "# Training Functions (Î∂ÑÎ•òÏö© ÏàòÏ†ï)\n",
    "# ============================================================\n",
    "def train_epoch(model, tr_loader, criterion, optimizer, scaler, device, \n",
    "                amp=True, use_mixup=True):\n",
    "    model.train()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(tr_loader, desc=\"Train\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)  # [B, 6]\n",
    "        \n",
    "        # Mixup Ï†ÅÏö©\n",
    "        is_mixup = use_mixup and (random.random() < 0.5)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        with autocast(enabled=amp):\n",
    "            if is_mixup:\n",
    "                imgs_mixed, labels_mixed, lam = mixup_data_classification(imgs, labels)\n",
    "                pred_dict = model(imgs_mixed)\n",
    "                loss, part_losses = mixup_criterion(criterion, pred_dict, labels_mixed, lam)\n",
    "            else:\n",
    "                pred_dict = model(imgs)\n",
    "                loss, part_losses = criterion(pred_dict, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Metrics (ÏõêÎ≥∏ labelsÎ°ú Í≥ÑÏÇ∞)\n",
    "        exact_acc, off_by_1, mae, _ = calculate_classification_metrics(pred_dict, labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\",\n",
    "            mae=f\"{run_mae/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    \n",
    "    return run_loss/n, run_acc/n, run_off1/n, run_mae/n, part_losses_avg\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader, criterion, device, split='val'):\n",
    "    model.eval()\n",
    "    run_loss = run_acc = run_off1 = run_mae = n = 0\n",
    "    part_losses_sum = torch.zeros(6)\n",
    "    region_acc_sum = torch.zeros(6)\n",
    "    \n",
    "    pbar = tqdm(val_loader, desc=f\"{split.capitalize()}\", leave=False)\n",
    "    \n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        \n",
    "        pred_dict = model(imgs)\n",
    "        loss, part_losses = criterion(pred_dict, labels)\n",
    "        \n",
    "        exact_acc, off_by_1, mae, region_acc = calculate_classification_metrics(pred_dict, labels)\n",
    "        \n",
    "        bs = imgs.size(0)\n",
    "        run_loss += loss.item() * bs\n",
    "        run_acc += exact_acc * bs\n",
    "        run_off1 += off_by_1 * bs\n",
    "        run_mae += mae * bs\n",
    "        part_losses_sum += part_losses.cpu() * bs\n",
    "        region_acc_sum += region_acc.cpu() * bs\n",
    "        n += bs\n",
    "        \n",
    "        pbar.set_postfix(\n",
    "            loss=f\"{run_loss/n:.4f}\",\n",
    "            acc=f\"{run_acc/n:.4f}\"\n",
    "        )\n",
    "    \n",
    "    avg_loss = run_loss/n\n",
    "    avg_acc = run_acc/n\n",
    "    avg_off1 = run_off1/n\n",
    "    avg_mae = run_mae/n\n",
    "    part_losses_avg = part_losses_sum / n\n",
    "    region_acc_avg = region_acc_sum / n\n",
    "    \n",
    "    print(f\"[{split}] loss:{avg_loss:.4f} acc:{avg_acc:.4f} \"\n",
    "          f\"off1:{avg_off1:.4f} mae:{avg_mae:.4f}\")\n",
    "    print(f\"  Region Acc: {region_acc_avg.numpy().round(3)}\")\n",
    "    \n",
    "    return avg_loss, avg_acc, avg_off1, avg_mae, part_losses_avg, region_acc_avg\n",
    "\n",
    "def get_lrs(optimizer):\n",
    "    return [pg['lr'] for pg in optimizer.param_groups]\n",
    "\n",
    "# ============================================================\n",
    "# Main Function\n",
    "# ============================================================\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ Brixia COVID-19 Ordinal Classification Training\")\n",
    "    print(\"   üè• DenseNet121-MIMIC (ÏùòÎ£å ÏòÅÏÉÅ Ï†ÑÏö© ÏÇ¨Ï†ÑÌïôÏäµ!)\")\n",
    "    print(\"   üí° 80% Î™©ÌëúÎ•º ÏúÑÌïú ÌïµÏã¨ Í∏∞Ïà†:\")\n",
    "    print(\"   ‚úÖ MIMIC-CXR ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ (Í∞ÄÏû• Í∞ïÎ†•!)\")\n",
    "    print(\"   ‚úÖ Ordinal Loss - ÏàúÏÑúÌòï ÌöåÍ∑Ä (0<1<2<3)\")\n",
    "    print(\"   ‚úÖ ÌÅ¥ÎûòÏä§ Î∂àÍ∑†Ìòï Ï≤òÎ¶¨ (ÏûêÎèô Í∞ÄÏ§ëÏπò)\")\n",
    "    print(\"   ‚úÖ Î∂ÄÏúÑ Í∞Ñ ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ ÌïôÏäµ (Self-Attention)\")\n",
    "    print(\"   ‚úÖ Mixed Precision Training (AMP)\")\n",
    "    print(\"   ‚úÖ Mixup Augmentation\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ\n",
    "    print(\"\\nüìÇ Loading data...\")\n",
    "    tr_df, val_df, tt_df = load_and_split_brixia(CSV_PATH)\n",
    "    \n",
    "    print(\"\\nüì¶ Creating DataLoaders...\")\n",
    "    tr_loader, val_loader, tt_loader = create_dataloaders(\n",
    "        tr_df, val_df, tt_df, img_dir=IMAGE_DIR, \n",
    "        batch_size=BATCH_SIZE, img_size=IMG_SIZE, num_workers=4\n",
    "    )\n",
    "    \n",
    "    # Train labels Ï∂îÏ∂ú\n",
    "    train_labels = torch.cat([labels for _, labels in tr_loader], dim=0)\n",
    "    \n",
    "    # ========================================\n",
    "    # ÌïôÏäµ Ï§ÄÎπÑ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìç Training Setup\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # ‚úÖ Ordinal Loss + ÌÅ¥ÎûòÏä§ Í∞ÄÏ§ëÏπò\n",
    "    criterion = AdaptiveOrdinalLoss(\n",
    "        train_labels, \n",
    "        num_classes=NUM_CLASSES,\n",
    "        use_class_weights=True,\n",
    "        part_weights=None,\n",
    "        ordinal_weight=0.5  # CE:Ordinal = 50:50\n",
    "    )\n",
    "    \n",
    "    # ‚úÖ ÏùòÎ£å ÏòÅÏÉÅ Ï†ÑÏö© ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏\n",
    "    model = DenseNet121MIMICClassification(\n",
    "        pretrained=True, \n",
    "        drop=DROP_RATIO,\n",
    "        num_regions=6,\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(DEVICE)\n",
    "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6\n",
    "    )\n",
    "    scaler = GradScaler(enabled=AMP)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_mae = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 15\n",
    "    \n",
    "    # ========================================\n",
    "    # ÌïôÏäµ Î£®ÌîÑ\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üèãÔ∏è Training Start\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for ep in range(1, EPOCHS + 1):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        tr_loss, tr_acc, tr_off1, tr_mae, tr_part_losses = train_epoch(\n",
    "            model, tr_loader, criterion, optimizer, scaler, DEVICE, AMP\n",
    "        )\n",
    "        \n",
    "        val_loss, val_acc, val_off1, val_mae, val_part_losses, val_region_acc = evaluate(\n",
    "            model, val_loader, criterion, DEVICE, split='val'\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_mae = val_mae\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': ep,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': best_acc,\n",
    "                'val_mae': best_mae,\n",
    "                'val_off1': val_off1,\n",
    "                'region_acc': val_region_acc,\n",
    "            }, BEST_PATH)\n",
    "            print(f\"‚úÖ New Best! (Acc={best_acc:.4f}, MAE={best_mae:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\n‚èπÔ∏è Early stopping at epoch {ep}\")\n",
    "            break\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"\\n[Epoch {ep:03d}/{EPOCHS}]\")\n",
    "        print(f\"  Train - loss:{tr_loss:.4f} acc:{tr_acc:.4f} off1:{tr_off1:.4f} mae:{tr_mae:.4f}\")\n",
    "        print(f\"  Val   - loss:{val_loss:.4f} acc:{val_acc:.4f} off1:{val_off1:.4f} mae:{val_mae:.4f}\")\n",
    "        print(f\"  Part losses (Val): {val_part_losses.numpy().round(3)}\")\n",
    "        print(f\"  Region Acc (Val): {val_region_acc.numpy().round(3)}\")\n",
    "        print(f\"  LR:{get_lrs(optimizer)[0]:.2e} | {elapsed:.1f}s | Patience:{patience_counter}/{max_patience}\")\n",
    "        print(\"-\" * 70)\n",
    "    \n",
    "    # ========================================\n",
    "    # Test Evaluation\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ Training Finished!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(tt_loader) > 0:\n",
    "        print(\"\\nüìä Test evaluation with best model...\")\n",
    "        checkpoint = torch.load(BEST_PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        tt_loss, tt_acc, tt_off1, tt_mae, tt_part_losses, tt_region_acc = evaluate(\n",
    "            model, tt_loader, criterion, DEVICE, split='test'\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüèÜ Test Results:\")\n",
    "        print(f\"   Accuracy: {tt_acc:.4f}\")\n",
    "        print(f\"   Off-by-1: {tt_off1:.4f}\")\n",
    "        print(f\"   MAE: {tt_mae:.4f}\")\n",
    "        print(f\"   Region Acc: {tt_region_acc.numpy().round(3)}\")\n",
    "        print(f\"   Part losses: {tt_part_losses.numpy().round(3)}\")\n",
    "    \n",
    "    print(f\"\\nüíæ Best model saved: {BEST_PATH}\")\n",
    "    print(f\"üìà Best Validation Accuracy: {best_acc:.4f}\")\n",
    "    print(f\"üìâ Best Validation MAE: {best_mae:.4f}\")\n",
    "    \n",
    "    # ÏÑ±Îä• Ìñ•ÏÉÅ Ï†úÏïà\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üí° Ï∂îÍ∞Ä ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú Ï†úÏïà:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"1. üéØ ÏïôÏÉÅÎ∏î: Îã§Î•∏ ÏãúÎìúÎ°ú 3~5Í∞ú Î™®Îç∏ ÌïôÏäµ ÌõÑ Ìà¨Ìëú\")\n",
    "    print(\"2. üî¨ ÏùòÎ£å Ï†ÑÏö© pretrained model ÏÇ¨Ïö©:\")\n",
    "    print(\"   - CheXpert, MIMIC-CXR Îì±ÏúºÎ°ú ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏\")\n",
    "    print(\"3. üìä Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä:\")\n",
    "    print(\"   - Ïô∏Î∂Ä COVID-19 Îç∞Ïù¥ÌÑ∞ÏÖã ÌôúÏö©\")\n",
    "    print(\"   - Pseudo-labelingÏúºÎ°ú unlabeled Îç∞Ïù¥ÌÑ∞ ÌôúÏö©\")\n",
    "    print(\"4. üé® Í≥†Í∏â Ï¶ùÍ∞ï:\")\n",
    "    print(\"   - CutMix, AugMix, RandAugment\")\n",
    "    print(\"   - Test-Time Augmentation (TTA)\")\n",
    "    print(\"5. üß™ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌäúÎãù:\")\n",
    "    print(\"   - ordinal_weight Ï°∞Ï†ï (0.3~0.7)\")\n",
    "    print(\"   - Learning rate, batch size Ïã§Ìóò\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\n‚úÖ Ïù¥Ï†ú gradcam_inference.pyÎ•º Ïã§ÌñâÌïòÏó¨ Í≤∞Í≥ºÎ•º ÏãúÍ∞ÅÌôîÌïòÏÑ∏Ïöî!\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "893c8310-2dc1-4844-9e80-06c9ed352ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.kakao.com/pypi/simple\n",
      "Collecting torchxrayvision\n",
      "  Downloading https://mirror.kakao.com/pypi/packages/8d/fe/0cad4be210168c00cb0bce2870f57b92d43c2f5b5e604a46d97f9d6c3957/torchxrayvision-1.4.0-py3-none-any.whl (29.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (2.3.1)\n",
      "Requirement already satisfied: torchvision>=0.5 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (0.18.1)\n",
      "Collecting scikit-image>=0.16 (from torchxrayvision)\n",
      "  Downloading https://mirror.kakao.com/pypi/packages/96/08/916e7d9ee4721031b2f625db54b11d8379bd51707afaa3e5a29aecf10bc4/scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (2.3.3)\n",
      "Requirement already satisfied: requests>=1 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (2.32.2)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchxrayvision) (10.3.0)\n",
      "Collecting imageio (from torchxrayvision)\n",
      "  Downloading https://mirror.kakao.com/pypi/packages/cb/bd/b394387b598ed84d8d0fa90611a90bee0adc2021820ad5729f7ced74a8e2/imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->torchxrayvision) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->torchxrayvision) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1->torchxrayvision) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1->torchxrayvision) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=1->torchxrayvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=1->torchxrayvision) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=1->torchxrayvision) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=1->torchxrayvision) (2024.2.2)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16->torchxrayvision) (1.15.3)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16->torchxrayvision) (3.2.1)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.16->torchxrayvision)\n",
      "  Downloading https://mirror.kakao.com/pypi/packages/5d/06/bd0a6097da704a7a7c34a94cfd771c3ea3c2f405dd214e790d22c93f6be1/tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.16->torchxrayvision) (23.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.16->torchxrayvision)\n",
      "  Downloading https://mirror.kakao.com/pypi/packages/83/60/d497a310bde3f01cb805196ac61b7ad6dc5dcf8dce66634dc34364b20b4f/lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1->torchxrayvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1->torchxrayvision) (4.15.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1->torchxrayvision) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1->torchxrayvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1->torchxrayvision) (2024.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1->torchxrayvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1->torchxrayvision) (1.3.0)\n",
      "Installing collected packages: tifffile, lazy-loader, imageio, scikit-image, torchxrayvision\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [torchxrayvision] [scikit-image]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imageio-2.37.0 lazy-loader-0.4 scikit-image-0.25.2 tifffile-2025.5.10 torchxrayvision-1.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchxrayvision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
